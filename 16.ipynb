{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b963f3",
   "metadata": {},
   "source": [
    "<h1>3 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e2f70",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 3**\n",
    "**«Решение систем линейных алгебраических уравнений методом Гаусса. Условие устойчивости вычислений. Варианты с частичным выбором ведущего элемента (по столбцу) и полным выбором по всей матрице.»**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Постановка задачи\n",
    "\n",
    "Имеется система из $n$ линейных уравнений с $n$ неизвестными:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "a_{11}x_1 + a_{12}x_2 + \\dots + a_{1n}x_n = b_1,\\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\dots + a_{2n}x_n = b_2,\\\\\n",
    "\\ \\ \\ \\vdots\\ \\ \\ \\quad\\quad\\vdots\\quad\\quad\\ddots\\quad\\vdots\\\\\n",
    "a_{n1}x_1 + a_{n2}x_2 + \\dots + a_{nn}x_n = b_n.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "В матричной форме это записывается как\n",
    "\n",
    "$$\n",
    "A\\,x = b,\\quad \n",
    "A = [a_{ij}]_{n\\times n},\\quad \n",
    "x=(x_1,\\dots,x_n)^T,\\quad\n",
    "b=(b_1,\\dots,b_n)^T.\n",
    "$$\n",
    "\n",
    "Прямое применение правила Крамера здесь неэффективно: вычисление $(n\\!+\\!1)$ детерминантов порядка $n$ требует $\\mathcal O((n+1)\\,n!\\,n)$ операций и быстро становится невыполнимым даже при умеренных $n$ .\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Суть метода Гаусса\n",
    "\n",
    "1. **Прямой ход (последовательное исключение).**\n",
    "\n",
    "   * Цель: превратить матрицу $A$ в верхнюю треугольную форму.\n",
    "   * На $k$-м шаге (для $k=1,\\dots,n-1$) мы “обнуляем” все элементы под диагональным $a_{kk}$: для каждого $i>k$\n",
    "\n",
    "     $$\n",
    "       \\text{множитель } c = \\frac{a_{ik}^{(\\,\\text{текущая}\\,)}}{a_{kk}^{(\\,\\text{текущая}\\,),}}\n",
    "       \\quad\n",
    "       \\text{замена уравнения }i:\\quad\n",
    "       a_{ij}\\;\\leftarrow\\;a_{ij}-c\\,a_{kj},\\quad\n",
    "       b_i\\;\\leftarrow\\;b_i-c\\,b_k,\n",
    "     $$\n",
    "\n",
    "     при $j=k,\\dots,n$.\n",
    "   * В результате получаем систему с треугольной матрицей коэффициентов:\n",
    "\n",
    "     $$\n",
    "     \\begin{pmatrix}\n",
    "       a_{11} & *      & \\dots & *\\\\\n",
    "       0      & a'_{22}& \\dots & *\\\\\n",
    "       \\vdots & \\vdots &\\ddots &\\vdots\\\\\n",
    "       0      & 0      & \\dots & a^{(n)}_{nn}\n",
    "     \\end{pmatrix}.\n",
    "     $$\n",
    "\n",
    "2. **Обратный ход (решение треугольной системы).**\n",
    "\n",
    "   * Снизу вверх находим\n",
    "\n",
    "     $$\n",
    "       x_n = \\frac{b_n}{a_{nn}},\\quad\n",
    "       x_k = \\frac{1}{a_{kk}}\\Bigl(b_k - \\sum_{i=k+1}^n a_{ki}\\,x_i\\Bigr),\\quad k=n-1,\\dots,1.\n",
    "     $$\n",
    "\n",
    "**Объём вычислений** при обычном методе Гаусса \\~$\\frac13n^3 + \\mathcal O(n^2)$ арифметических операций .\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Устойчивость вычислений и выбор ведущего элемента\n",
    "\n",
    "При стандартном алгоритме каждый шаг деления на $a_{kk}$ требует, чтобы этот элемент был отличен от нуля. Но даже если он ненулевой, он может быть очень мал по сравнению с другими коэффициентами, что приводит к **росту погрешностей**:\n",
    "\n",
    "$$\n",
    "\\Delta a_{ij}\\;\\mapsto\\;\\Delta a_{ij} - c\\,\\Delta a_{kj},\\quad c=\\frac{a_{ik}}{a_{kk}}.\n",
    "$$\n",
    "\n",
    "Если $|c|>1$, то погрешность “растет лавинообразно”.\n",
    "\n",
    "Чтобы этого избежать, перед исключением $x_k$ находят **главный (ведущий) элемент** — максимальный по модулю среди кандидатов на место $a_{kk}$ — и **меняют строки** так, чтобы он оказался на диагонали. Тогда\n",
    "\n",
    "$$\n",
    "|c| = \\Bigl|\\frac{a_{ik}}{a_{kk}}\\Bigr| \\le 1,\n",
    "$$\n",
    "\n",
    "и метод становится устойчивее.\n",
    "\n",
    "* **Частичный выбор**: ищем максимум в столбце $k$ (ниже или на диагонали), меняем только строки.\n",
    "* **Полный выбор**: ищем максимум в оставшейся (невырожденной) части матрицы $A_{k:n,k:n}$, меняем и строки, и столбцы. При перестановке столбцов меняется порядок переменных — его нужно запомнить и восстановить после решения.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Варианты метода\n",
    "\n",
    "1. **Без выбора ведущего элемента.**\n",
    "   Простая схема, но **неустойчива**, может “упасть” на делении на ноль или нарастании погрешностей.\n",
    "\n",
    "2. **С частичным выбором по столбцу.**\n",
    "   Надежный, находит главный элемент в каждом столбце, переставляет строки, **устойчивость** достаточно хорошая и **сложность** остаётся $\\sim\\frac13n^3$.\n",
    "\n",
    "3. **С полным выбором по матрице.**\n",
    "   Меняем и строки, и столбцы, чуть более устойчива, но дороже по сравнению: на каждом шаге ищем максимум среди $(n-k+1)^2$ элементов и ведём учёт порядка переменных.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Примеры\n",
    "\n",
    "### Пример 1. Простая система $3\\times3$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "3x + 4y + 5z = 9,\\\\\n",
    "x + 2y + 3z = 8,\\\\\n",
    "2x + 4y + 5z = 7.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "1. **Матрица и вектор:**\n",
    "\n",
    "   $$\n",
    "   A = \\begin{pmatrix}\n",
    "   3 & 4 & 5\\\\\n",
    "   1 & 2 & 3\\\\\n",
    "   2 & 4 & 5\n",
    "   \\end{pmatrix},\\quad\n",
    "   b=\\begin{pmatrix}9\\\\8\\\\7\\end{pmatrix}.\n",
    "   $$\n",
    "2. **Шаг 1 (исключаем $x$ из уравнений 2 и 3):**\n",
    "\n",
    "   $$\n",
    "   c_{21}=\\frac{1}{3},\\quad c_{31}=\\frac{2}{3};\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "     \\text{Row}_2\\;\\leftarrow\\;\\text{Row}_2 - \\tfrac13\\,\\text{Row}_1\n",
    "     \\;\\Rightarrow\\;\\bigl(0,\\;2-\\tfrac{4}{3},\\;3-\\tfrac{5}{3}\\;\\bigr|\\;8-3\\bigr)\n",
    "     =(0,\\tfrac23,\\tfrac{4}{3}\\mid5);\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "     \\text{Row}_3\\;\\leftarrow\\;\\text{Row}_3 - \\tfrac23\\,\\text{Row}_1\n",
    "     \\;\\Rightarrow\\;(0,\\;4-\\tfrac{8}{3},\\;5-\\tfrac{10}{3}\\mid7-6)\n",
    "     =(0,\\tfrac{4}{3},\\tfrac{5}{3}\\mid1).\n",
    "   $$\n",
    "\n",
    "   Получили:\n",
    "\n",
    "   $$\n",
    "   \\begin{pmatrix}\n",
    "   3 & 4 & 5 &\\mid&9\\\\\n",
    "   0 & \\tfrac23 & \\tfrac43 &\\mid&5\\\\\n",
    "   0 & \\tfrac43 & \\tfrac53 &\\mid&1\n",
    "   \\end{pmatrix}.\n",
    "   $$\n",
    "3. **Шаг 2 (исключаем $y$ из 3-го уравнения):**\n",
    "   $c_{32}=(\\tfrac43)/(\\tfrac23)=2$.\n",
    "\n",
    "   $$\n",
    "     \\text{Row}_3\\;\\leftarrow\\;\\text{Row}_3 - 2\\,\\text{Row}_2\n",
    "     \\;\\Rightarrow\\;(0,0,\\;\\tfrac{5}{3}-2\\cdot\\tfrac{4}{3}\\mid1-2\\cdot5)\n",
    "     =(0,0,-1\\mid-9).\n",
    "   $$\n",
    "\n",
    "   Система становится\n",
    "\n",
    "   $$\n",
    "   \\begin{pmatrix}\n",
    "   3 & 4 & 5 &\\mid&9\\\\\n",
    "   0 & \\tfrac23 & \\tfrac43 &\\mid&5\\\\\n",
    "   0 & 0 & -1 &\\mid&-9\n",
    "   \\end{pmatrix}.\n",
    "   $$\n",
    "4. **Обратный ход:**\n",
    "\n",
    "   $$\n",
    "     z = x_3 = \\frac{-9}{-1} = 9;\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "     y = \\frac{5 - \\tfrac43\\cdot9}{\\tfrac23}\n",
    "       = \\frac{5 - 12}{\\tfrac23}\n",
    "       = \\frac{-7}{\\tfrac23}\n",
    "       = -\\frac{7}{1}\\cdot\\frac{3}{2}\n",
    "       = -\\tfrac{21}{2} = -10.5;\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "     x = \\frac{9 - 4\\cdot(-10.5) - 5\\cdot9}{3}\n",
    "       = \\frac{9 + 42 - 45}{3}\n",
    "       = \\frac{6}{3} = 2.\n",
    "   $$\n",
    "\n",
    "   **Ответ:** $(x,y,z) = (2,\\,-10.5,\\,9)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Пример 2. Система с нулевым первым элементом (демонстрация частичного выбора)\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "0x + 2y +  z = 5,\\\\\n",
    " x +  y +  z = 6,\\\\\n",
    "2x + 3y + 4z =11.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Здесь на первом шаге $a_{11}=0$, поэтому **частично** меняем первую строку на ту, где в первом столбце стоит ненулевое число (максимальное по модулю из строк 1–3):\n",
    "\n",
    "* Смотрим столбец 1: $\\{0,1,2\\}$ → выбираем $2$ (строка 3).\n",
    "* Меняем строки 1↔️3.\n",
    "\n",
    "Получаем эквивалентную систему\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "2x + 3y + 4z =11,\\\\\n",
    " x +  y +  z = 6,\\\\\n",
    "0x + 2y +  z = 5.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Дальше действуем, как в обычном методе:\n",
    "\n",
    "1. **Исключаем $x$** из строк 2 и 3:\n",
    "\n",
    "   $$\n",
    "     c_{21}=\\tfrac12,\\quad\n",
    "     \\text{Row}_2 \\leftarrow \\text{Row}_2 - \\tfrac12\\,\\text{Row}_1\n",
    "     \\;\\Rightarrow\\;(0,\\;1-\\tfrac{3}{2},\\;1-\\tfrac{4}{2}\\mid6-\\tfrac{11}{2})\n",
    "     =(0,-\\tfrac12,-1\\mid\\tfrac{1}{2});\n",
    "   $$\n",
    "\n",
    "   строку 3 менять не нужно (там $x$ уже отсутствует).\n",
    "2. **Исключаем $y$** из строки 3:\n",
    "\n",
    "   $$\n",
    "     c_{32}=\\frac{2}{-1/2}=-4,\\quad\n",
    "     \\text{Row}_3\\leftarrow\\text{Row}_3 -(-4)\\,\\text{Row}_2\n",
    "     =(0,0,\\;1-(-4)\\cdot(-1)\\mid5-(-4)\\cdot\\tfrac12)\n",
    "     =(0,0,-3\\mid7).\n",
    "   $$\n",
    "3. **Обратный ход:**\n",
    "\n",
    "   $$\n",
    "     z = \\frac{7}{-3} = -\\tfrac73,\\quad\n",
    "     y = \\frac{\\tfrac12 - (-1)\\cdot(-\\tfrac73)}{-\\tfrac12}\n",
    "       = \\frac{\\tfrac12 - \\tfrac73}{-\\tfrac12}\n",
    "       = \\frac{-\\tfrac{11}{6}}{-\\tfrac12}\n",
    "       = \\tfrac{11}{6}\\cdot2 = \\tfrac{11}{3},\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "     x = \\frac{11 - 3\\cdot\\tfrac{11}{3} - 4\\cdot(-\\tfrac73)}{2}\n",
    "       = \\frac{11 -11 + \\tfrac{28}{3}}{2}\n",
    "       = \\frac{\\tfrac{28}{3}}{2} = \\tfrac{14}{3}.\n",
    "   $$\n",
    "\n",
    "   **Ответ:** $\\bigl(x,y,z\\bigr)=\\bigl(\\tfrac{14}{3},\\,\\tfrac{11}{3},\\,-\\tfrac73\\bigr)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Комментарии к полному выбору\n",
    "\n",
    "Если же на каждом шаге искать **максимум не только в столбце**, но и во **всей** необработанной части матрицы, то придется переставлять **и столбцы**, и **строки**. После решения нужно будет “распутать” полученный порядок переменных. Алгоритм становится более затратным (по сравнению с выбором по столбцу), и на практике, как правило, достаточно **частичного** выбора ведущего элемента.\n",
    "\n",
    "---\n",
    "\n",
    "### Итоговая схема (частичный выбор)\n",
    "\n",
    "1. Для $k=1..n-1$:\n",
    "\n",
    "   * Найти в столбце $k$ (строки $k..n$) максимальный по модулю элемент.\n",
    "   * Поменять строки так, чтобы он оказался на $(k,k)$.\n",
    "   * Выполнить исключение $x_k$ из строк $k+1..n$.\n",
    "2. Обратным ходом найти $x_n,x_{n-1},\\dots,x_1$.\n",
    "\n",
    "Метод обеспечивает **устойчивую** и **эффективную** реализацию с $\\mathcal O(n^3)$ операций.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae0b9e",
   "metadata": {},
   "source": [
    "<h1>3 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7859611",
   "metadata": {},
   "source": [
    "**Шпаргалка по билету № 3**\n",
    "**«Метод Гаусса. Устойчивость. Частичный и полный выбор ведущего элемента»**\n",
    "\n",
    "---\n",
    "\n",
    "1. **Суть метода**\n",
    "\n",
    "   * Прямой ход: приводим $A$ к верхнетреугольному виду, обнуляя ниже диагонали\n",
    "   * Обратный ход: решаем полученную треугольную систему снизу вверх\n",
    "\n",
    "2. **Алгоритм (частичный выбор)**\n",
    "\n",
    "   1. Для $k=1,\\dots,n-1$:\n",
    "\n",
    "      * в столбце $k$ (строки $k..n$) ищем элемент max |aᵢₖ|\n",
    "      * меняем строку $k$ и ту, где найден максимум\n",
    "      * для $i=k+1..n$:\n",
    "        $\\displaystyle c = \\frac{a_{i\\,k}}{a_{k\\,k}},\\quad\\text{Row}_i \\leftarrow \\text{Row}_i - c\\,\\text{Row}_k$\n",
    "   2. Для $k=n..1$:\n",
    "      $\\displaystyle x_k = \\frac{1}{a_{k\\,k}}\\Bigl(b_k - \\sum_{j=k+1}^n a_{k\\,j}x_j\\Bigr)$\n",
    "\n",
    "3. **Устойчивость**\n",
    "\n",
    "   * Без выбора ведущего: возможны деления на малые «псевдониули» → накопление погрешностей\n",
    "   * Частичный выбор (по столбцу): гарантирует $|c|\\le1$, достаточная устойчивость\n",
    "   * Полный выбор (по всей матрице): ещё более устойчивает, но сложнее (перестановка столбцов → учёт порядка переменных)\n",
    "\n",
    "4. **Сложность**\n",
    "   $\\displaystyle \\tfrac13n^3 + O(n^2)$ операций\n",
    "\n",
    "5. **Варианты**\n",
    "\n",
    "   * Без выбора: простая, но неустойчивая\n",
    "   * Частичный выбор: оптимальный баланс надёжности и скорости\n",
    "   * Полный выбор: максимальная устойчивость, больше затрат на поиск и перестановки\n",
    "\n",
    "---\n",
    "\n",
    "*Метод Гаусса с частичным выбором ведущего элемента – стандартная, эффективная и устойчивая схема решения СЛАУ.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e471a",
   "metadata": {},
   "source": [
    "<h1>4 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a479b4",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 4**\n",
    "**«Решение системы линейных алгебраических уравнений методом Гаусса. Приведение к треугольной форме с единичной диагональю и к диагональной форме (метод Гаусса–Жордана). Оценка количества арифметических операций, затрачиваемых на прямой и обратный ход. Матричная формулировка метода Гаусса, его связь с LU-факторизацией матрицы коэффициентов. Вычисление обратной матрицы $A^{-1}$».**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Приведение к треугольной форме с единичной диагональю\n",
    "\n",
    "1. **Идея «деления ведущего уравнения**\n",
    "   На $k$-м шаге после выбора (и, при необходимости, перестановки) ведущего элемента $a_{kk}$ делим $k$-е уравнение на $a_{kk}$, чтобы получить $a'_{kk}=1$. Тогда\n",
    "\n",
    "   $$\n",
    "     a'_{k,i} = \\frac{a_{k,i}}{a_{kk}},\\quad \n",
    "     b'_k = \\frac{b_k}{a_{kk}},\\quad i=k+1,\\dots,n.\n",
    "   $$\n",
    "\n",
    "2. **Исключение**\n",
    "   Для каждого $i>k$ прибавляем к $i$-му уравнению «новое» $k$-е, умноженное на $-a_{i,k}$.\n",
    "   В результате в получившейся верхнетреугольной матрице на диагонали окажутся единицы, а поддиагональные элементы обнулятся .\n",
    "\n",
    "3. **Обратный ход**\n",
    "   Поскольку на диагонали — единицы, обратный ход сводится к\n",
    "\n",
    "   $$\n",
    "     x_n = \\tilde b_n,\\quad\n",
    "     x_k = \\tilde b_k - \\sum_{i=k+1}^n \\tilde a_{k,i}\\,x_i,\\quad k=n-1,\\dots,1.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Метод Гаусса–Жордана (полное приведение к диагональной форме)\n",
    "\n",
    "* **Прямой ход**\n",
    "  Каждое неизвестное «исключается» не только из уравнений ниже, но и выше: после обработки $k$-го столбца все элементы вне диагонали становятся нулями.\n",
    "* **Обратный ход**\n",
    "  В чистом варианте отсутствует — после прямого хода система сразу имеет вид\n",
    "  $\\mathrm{diag}(d_1,\\dots,d_n)\\,x = \\tilde b$,\n",
    "  и остаётся лишь разделить каждое уравнение на $d_k$.\n",
    "* **Сравнение объёмов**\n",
    "  Прямой ход Гаусса–Жордана требует\n",
    "\n",
    "  $$\n",
    "    (n-1)^2\\text{ делений,}\\quad\n",
    "    \\tfrac{n(n^2-3)+2}2\\text{ умножений и столько же сложений},\n",
    "  $$\n",
    "\n",
    "  то есть порядка $n^3$ при отсутствии обратного хода. Обычный метод Гаусса тратит на прямой ход $\\sim n^3/3$ операций, а обратный — $\\sim n^2/2$ операций . При $n=100$ общее число операций соответственно $1{,}01\\cdot10^6$ и $0{,}682\\cdot10^6$, поэтому метод Жордана применяется редко.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Оценка числа арифметических операций\n",
    "\n",
    "1. **Прямой ход (Гаусс)**\n",
    "   На $k$-м шаге выполняется\n",
    "\n",
    "   $$\n",
    "     n-k\\text{ делений},\\quad\n",
    "     (n-k)(n-k+1)\\text{ умножений и сложений}.\n",
    "   $$\n",
    "\n",
    "   Всего:\n",
    "\n",
    "   $$\n",
    "     \\frac{n(n-1)}2\\text{ делений},\\quad\n",
    "     \\frac{n(n^2-1)}3\\text{ умножений и }+\\text{сложений}.\n",
    "   $$\n",
    "\n",
    "2. **Обратный ход**\n",
    "\n",
    "   $$\n",
    "     n\\text{ делений},\\quad\n",
    "     \\frac{n(n-1)}2\\text{ умножений и вычитаний}.\n",
    "   $$\n",
    "\n",
    "3. **Итог**\n",
    "\n",
    "   $$\n",
    "     \\mathcal O(n^3)\\text{ операций (ведущая константа }\\tfrac13).\n",
    "   $$\n",
    "\n",
    "   Обратный ход даёт лишь $\\mathcal O(n^2)$ операций и с ростом $n$ становится незначительным .\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Матричная формулировка и связь с LU-факторизацией\n",
    "\n",
    "1. **Матрицы преобразований**\n",
    "   Каждый шаг прямого хода — умножение слева на нижнетреугольную «элиминаторную» матрицу $M_k$. После $n-1$ шагов\n",
    "\n",
    "   $$\n",
    "     U = M_{n-1}\\cdots M_1\\,A,\n",
    "   $$\n",
    "\n",
    "   где $U$ — верхнетреугольная матрица. Значит\n",
    "\n",
    "   $$\n",
    "     A = (M_{n-1}\\cdots M_1)^{-1}\\,U = L\\,U,\n",
    "   $$\n",
    "\n",
    "   где $L = M_1^{-1}M_2^{-1}\\cdots M_{n-1}^{-1}$ — нижнетреугольная матрица с единичной диагональю. Это и есть **LU-разложение** матрицы $A$ .\n",
    "\n",
    "2. **Решение через LU**\n",
    "   Запишем $LU\\,x=b$. Вводим вспомогательный вектор $z=U\\,x$. Тогда\n",
    "\n",
    "   $$\n",
    "     L\\,z = b,\\quad U\\,x = z,\n",
    "   $$\n",
    "\n",
    "   что даёт две треугольные системы, решаемые быстрыми «вперёд» и «назад» алгоритмами.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Вычисление обратной матрицы $A^{-1}$\n",
    "\n",
    "1. По определению\n",
    "   $\\displaystyle A\\,A^{-1}=E$.\n",
    "2. Обратную можно найти, решив $n$ систем\n",
    "\n",
    "   $$\n",
    "     A\\,z_k = e_k,\\quad k=1,\\dots,n,\n",
    "   $$\n",
    "\n",
    "   где $e_k$ — $k$-й базисный вектор. Столбцы $z_k$ образуют $A^{-1}$.\n",
    "3. При наличии LU-разложения вместо каждого прямого хода решают две треугольные системы; благодаря единожды выполненному факторизированию это эффективно .\n",
    "\n",
    "---\n",
    "\n",
    "## Примеры\n",
    "\n",
    "### Пример 1. 2×2, треугольная форма с единичной диагональю\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "4 & 3\\\\\n",
    "2 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}x\\\\y\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}11\\\\5\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "1. Делим 1-е уравнение на 4:\n",
    "   $\\;x + \\tfrac34y = \\tfrac{11}4$.\n",
    "2. Исключаем $x$ из 2-го:\n",
    "   $\\;(2x+y) -2(x+\\tfrac34y) = 5 -2\\cdot\\tfrac{11}4\\ \\Longrightarrow\\ -\\tfrac12y=-\\tfrac{3}2$;\n",
    "   $y=3$.\n",
    "3. $x = \\tfrac{11}4 - \\tfrac34\\cdot3 = -\\tfrac1{4}$.\n",
    "\n",
    "### Пример 2. 3×3, Gauss–Jordan, вычисление $A^{-1}$\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "2&1&0\\\\\n",
    "1&2&1\\\\\n",
    "0&1&2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Составляем расширенную матрицу $[A\\mid E]$ и приводим её методом Жордана к $[E\\mid A^{-1}]$. В итоге получаем\n",
    "\n",
    "$$\n",
    "A^{-1}=\\frac1{4}\n",
    "\\begin{pmatrix}\n",
    "3&-2&1\\\\\n",
    "-2&4&-2\\\\\n",
    "1&-2&3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "(Детали алгебры — деления строк на ведущие элементы и обнуления остальных — опускаем в силу ограничений шпаргалки.)\n",
    "\n",
    "---\n",
    "\n",
    "**Итог:**\n",
    "\n",
    "* Метод Гаусса с единичной диагональю избавляет от операций деления в обратном ходе.\n",
    "* Гаусс–Жордан сразу даёт диагональную систему, но дороже по операциям.\n",
    "* LU-разложение связывает обе задачи: решение СЛАУ и обращение матриц через треугольные множители.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562616fb",
   "metadata": {},
   "source": [
    "<h1>4 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d6a68",
   "metadata": {},
   "source": [
    "**Шпаргалка по билету № 4**\n",
    "**«Метод Гаусса, Гаусса–Жордана, LU-факторизация, обратная матрица»**\n",
    "\n",
    "---\n",
    "\n",
    "1. **Прямой ход Гаусса (единичная диагональ)**\n",
    "\n",
    "   * На шаге k:\n",
    "\n",
    "     1. При необходимости выбрать ведущий элемент (частичный/полный выбор) и переставить строки/столбцы.\n",
    "     2. Разделить k-е уравнение на $a_{kk}$ → на диагонали единица.\n",
    "     3. Для $i=k+1..n$: вычесть $a_{ik}\\times$(новое k-е уравнение) → обнулить элементы под диагональю.\n",
    "\n",
    "2. **Обратный ход**\n",
    "\n",
    "   $$\n",
    "   x_n = \\tilde b_n,\\quad\n",
    "   x_k = \\tilde b_k - \\sum_{j=k+1}^n \\tilde a_{kj}\\,x_j.\n",
    "   $$\n",
    "\n",
    "3. **Метод Гаусса–Жордана**\n",
    "\n",
    "   * Расширенный прямой ход: для каждого k обнуляем все вне диагонали (и выше, и ниже).\n",
    "   * После прямого хода сразу диагональная матрица → делим на диагональные элементы.\n",
    "   * Объём ≈ $n^3$ операций (без отдельного обратного хода).\n",
    "\n",
    "4. **Оценка затрат**\n",
    "\n",
    "   * Гаусс-прямой: $\\sim\\frac13n^3$ оп.; обратный: $\\sim\\frac12n^2$ оп.\n",
    "   * Жордан-прямой: $\\sim n^3$ оп.\n",
    "\n",
    "5. **LU-факторизация**\n",
    "\n",
    "   $$\n",
    "     A = L\\,U,\\quad\n",
    "     L\\text{—нижнетреугольная (единицы на диаг.)},\\ \n",
    "     U\\text{—верхнетреугольная}.\n",
    "   $$\n",
    "\n",
    "   Решение через две треугольные системы $Lz=b,\\;Ux=z$.\n",
    "\n",
    "6. **Обращение матрицы**\n",
    "\n",
    "   * Решаем $A\\,X=I$ по столбцам: $A z_k = e_k$.\n",
    "   * При наличии LU → два треугольных решения для каждого $e_k$.\n",
    "   * Результат — $A^{-1}=[z_1,\\dots,z_n]$.\n",
    "\n",
    "---\n",
    "\n",
    "*Всё в рамках $\\mathcal O(n^3)$ вычислений; стандартный подход — Гаусс с единичной диагональю + LU для многократных решений и обращения.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab51fac",
   "metadata": {},
   "source": [
    "<h1>5 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec1cd",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 5**\n",
    "\n",
    "---\n",
    "\n",
    "### Тема\n",
    "\n",
    "**Решение нелинейного уравнения методом деления отрезка (бисекции). Условия применимости метода и скорость сходимости к решению.**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Введение в метод\n",
    "\n",
    "1. **Задача.** Найти корень уравнения\n",
    "\n",
    "   $$\n",
    "   F(x) = 0\n",
    "   $$\n",
    "\n",
    "   в заранее заданном отрезке $[a,b]$.\n",
    "\n",
    "2. **Основная идея.** Если на концах отрезка функция принимает значения разных знаков,\n",
    "\n",
    "   $$\n",
    "   F(a)\\cdot F(b)<0,\n",
    "   $$\n",
    "\n",
    "   то по теореме Больцано–Коши на отрезке существует хотя бы один корень. Метод бисекции последовательно «разрезает» этот отрезок пополам, локализуя корень всё точнее.\n",
    "\n",
    "3. **Классификация.** Это итерационный метод: начиная с отрезка $[a,b]$, на каждой итерации мы получаем новый, вдвое более короткий отрезок, содержащий корень.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Условия применимости\n",
    "\n",
    "1. **Непрерывность.** $F$ должна быть непрерывна на $[a,b]$.\n",
    "2. **Противоположные знаки на концах.** $F(a)\\cdot F(b)<0$.\n",
    "3. **Один корень на отрезке.** Желательно, чтобы функция меняла знак только один раз («простая» изоляция корня).\n",
    "4. **Запас прочности по вычислительной арифметике.**\n",
    "\n",
    "   * При реализации на компьютере лучше проверять знак произведения через «знаковую функцию»\n",
    "\n",
    "     $$\n",
    "       \\operatorname{sign}(y)=\\frac{y}{|y|},\n",
    "     $$\n",
    "\n",
    "     чтобы избежать проблем, когда из-за переполнения или «машинного нуля» $F(a)$ или $F(b)$ становятся буквально равны нулю в машинном представлении.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Алгоритм метода деления отрезка\n",
    "\n",
    "1. **Начало.** Заданы $a$, $b$ с $F(a)\\cdot F(b)<0$, а также критерии точности:\n",
    "\n",
    "   * по аргументу: длина отрезка $b-a\\le\\varepsilon$;\n",
    "   * (или) по значению функции: $|F(c)|\\le\\delta$.\n",
    "\n",
    "2. **Шаг итерации.**\n",
    "\n",
    "   1. Вычислить середину\n",
    "\n",
    "      $$\n",
    "        c = a + \\frac{b-a}{2}.\n",
    "      $$\n",
    "\n",
    "      Такой способ надёжнее, чем $(a+b)/2$, — он не «вываливается» за $[a,b]$ из-за округлений.\n",
    "   2. Посчитать $F(c)$.\n",
    "   3. Если выполнено хотя бы одно условие останова\n",
    "\n",
    "      $$\n",
    "        |F(c)| \\le \\delta\n",
    "        \\quad\\text{или}\\quad\n",
    "        \\frac{b-a}2 \\le \\varepsilon,\n",
    "      $$\n",
    "\n",
    "      то принять $c$ за приближённый корень и выйти.\n",
    "   4. Иначе сравнить знаки $F(a)$ и $F(c)$:\n",
    "\n",
    "      * если $F(a)\\cdot F(c)<0$, то корень в $[a,c]$ → положить $b=c$;\n",
    "      * иначе корень в $[c,b]$ → положить $a=c$.\n",
    "   5. Перейти к следующей итерации.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Скорость сходимости\n",
    "\n",
    "* После $n$ итераций длина текущего отрезка не превосходит\n",
    "\n",
    "  $$\n",
    "    L_n = \\frac{b-a}{2^n}.\n",
    "  $$\n",
    "* Следовательно, погрешность по аргументу (максимальное расстояние до истинного корня)\n",
    "\n",
    "  $$\n",
    "    \\delta_n \\le \\tfrac12 L_n = \\frac{b-a}{2^{n+1}}.\n",
    "  $$\n",
    "* Чтобы сократить начальную ошибку в $T$ раз, нужно примерно\n",
    "\n",
    "  $$\n",
    "    n \\approx \\log_2 T\n",
    "  $$\n",
    "\n",
    "  итераций.\n",
    "* Это **линейная** сходимость (ошибка убывает как геометрическая прогрессия с коэффициентом $1/2$).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Простой язык без «воды»\n",
    "\n",
    "> Представьте, что вы ищете точку, где кривая $y=F(x)$ пересекает ось $OX$.\n",
    "> Если на концах отрезка график «над» осью, а «под» осью — корень между ними.\n",
    "> Каждый раз берём середину, смотрим, в какой половине знак меняется,\n",
    "> и продолжаем поиск там. Шаг за шагом отрезок становится всё короче,\n",
    "> и вы «стягиваете» корень всё плотнее.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Примеры\n",
    "\n",
    "### Пример 1.\n",
    "\n",
    "**Уравнение:** $x^3 - x - 2 = 0$.\n",
    "**Изоляция корня:** на отрезке $[1,2]$\n",
    "\n",
    "* $F(1)=1-1-2=-2$;\n",
    "* $F(2)=8-2-2=4$;\n",
    "* знак меняется → есть корень.\n",
    "\n",
    "| Итерация | $[a,b]$           | $c$    | $F(c)$    | Новый отрезок     |\n",
    "| :------: | :---------------- | :----- | :-------- | :---------------- |\n",
    "|     1    | $1.000, 2.000]   | 1.500  | –0.125    | $1.500, 2.000]   |\n",
    "|     2    | $1.500, 2.000]   | 1.750  | 1.609375  | $1.500, 1.750]   |\n",
    "|     3    | $1.500, 1.750]   | 1.625  | –0.984375 | $1.625, 1.750]   |\n",
    "|     …    | …                 | …      | …         | …                 |\n",
    "|     5    | $1.6719, 1.6875] | 1.6797 | 0.0435    | $1.6719, 1.6797] |\n",
    "|     6    | $1.6719, 1.6797] | 1.6758 | –0.0208   | $1.6758, 1.6797] |\n",
    "\n",
    "Через 6 шагов длина отрезка ≈ 0.0039, корень ≈ 1.6784.\n",
    "\n",
    "---\n",
    "\n",
    "### Пример 2.\n",
    "\n",
    "**Уравнение:** $\\cos x - x = 0$.\n",
    "**Изоляция корня:** на $[0,\\,1]$\n",
    "\n",
    "* $F(0)=1$; $F(1)=\\cos1-1\\approx -0.46$.\n",
    "\n",
    "| Итерация | $[a,b]$          | $c$    | $F(c)$  | Новый отрезок     |\n",
    "| :------: | :--------------- | :----- | :------ | :---------------- |\n",
    "|     1    | $0.000, 1.000]  | 0.500  | 0.3776  | $0.500, 1.000]   |\n",
    "|     2    | $0.500, 1.000]  | 0.750  | –0.0183 | $0.500, 0.750]   |\n",
    "|     3    | $0.500, 0.750]  | 0.625  | 0.1790  | $0.625, 0.750]   |\n",
    "|     4    | $0.625, 0.750]  | 0.6875 | 0.0839  | $0.6875, 0.750]  |\n",
    "|     5    | $0.6875, 0.750] | 0.7188 | 0.0332  | $0.7188, 0.750]  |\n",
    "|     6    | $0.7188, 0.750] | 0.7344 | 0.0070  | $0.7344, 0.750]  |\n",
    "|     7    | $0.7344, 0.750] | 0.7422 | –0.0057 | $0.7344, 0.7422] |\n",
    "\n",
    "После 7 шагов корень локализован в отрезке длиной ≈ 0.0078, значение $x\\approx0.739$.\n",
    "\n",
    "---\n",
    "\n",
    "### Итог\n",
    "\n",
    "* **Надёжность.** Гарантированное сокращение погрешности вдвое при каждой итерации.\n",
    "* **Простота.** Не требует знания производных и сложных преобразований.\n",
    "* **Медленность.** Линейная сходимость: чтобы добавить одну точку «двойной» точности (уменьшить ошибку в 10 раз), нужно ∼ 3.3 итерации.\n",
    "\n",
    "Метод бисекции идеален для быстрого «грубо́го» нахождения корня и служит хорошей отправной точкой прежде чем переходить к более быстрым (но требовательным к гладкости) методам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b69592",
   "metadata": {},
   "source": [
    "<h1>5 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87411dd",
   "metadata": {},
   "source": [
    "**Билет № 5. Метод бисекции (деления отрезка)**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Постановка задачи\n",
    "\n",
    "Найти приближённый корень $x^*$ уравнения\n",
    "\n",
    "$$\n",
    "F(x) = 0\n",
    "$$\n",
    "\n",
    "на отрезке $[a,b]$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Условия применимости\n",
    "\n",
    "1. **Непрерывность**: $F$ непрерывна на $[a,b]$.\n",
    "2. **Изоляция корня**: $F(a)\\cdot F(b)<0$ (функция меняет знак ровно один раз).\n",
    "3. **Один корень**: желательно, чтобы внутри $[a,b]$ было только одно пересечение с осью $OX$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Алгоритм метода\n",
    "\n",
    "1. **Инициализация**:\n",
    "\n",
    "   * Заданы $a$, $b$, критерии точности по аргументу $\\varepsilon$ и/или по функции $\\delta$.\n",
    "2. **Основной шаг** (итерация):\n",
    "\n",
    "   1. Вычислить середину\n",
    "\n",
    "      $$\n",
    "        c = a + \\frac{b-a}{2}.\n",
    "      $$\n",
    "   2. Оценить $F(c)$.\n",
    "   3. **Проверка завершения**:\n",
    "\n",
    "      * Если $\\lvert F(c)\\rvert \\le \\delta$ **или** $\\frac{b-a}{2} \\le \\varepsilon$, то принять $c$ за ответ и выйти.\n",
    "   4. **Локализация**:\n",
    "\n",
    "      * Если $F(a)\\,F(c)<0$, корень в $[a,c]$ → $b := c$.\n",
    "      * Иначе корень в $[c,b]$ → $a := c$.\n",
    "3. **Повторять**, пока не выполнится одно из условий остановки.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Скорость сходимости\n",
    "\n",
    "* **Линейная**: каждый шаг уменьшает длину отрезка вдвое.\n",
    "* После $n$ итераций длина отрезка\n",
    "\n",
    "  $$\n",
    "    \\frac{b-a}{2^n},\n",
    "  $$\n",
    "\n",
    "  а погрешность по корню\n",
    "\n",
    "  $$\n",
    "    \\le \\frac{b-a}{2^{n+1}}.\n",
    "  $$\n",
    "* Для снижения ошибки в $T$ раз нужно примерно $\\log_2 T$ шагов.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Достоинства и недостатки\n",
    "\n",
    "* **Плюсы**:\n",
    "\n",
    "  * Гарантированная сходимость при выполнении условий.\n",
    "  * Простота реализации (не требуются производные).\n",
    "* **Минусы**:\n",
    "\n",
    "  * Низкая скорость (много итераций для высокой точности).\n",
    "  * Не годится, если корень кратный или функция не меняет знак.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Краткая формулировка «для шпаргалки»\n",
    "\n",
    "> **Метод бисекции**:\n",
    ">\n",
    "> 1. Убеждаемся в наличии корня на $[a,b]$ по знакам $F(a)$ и $F(b)$.\n",
    "> 2. Берём середину $c$.\n",
    "> 3. Выбираем ту половину, где $F$ меняет знак, — там и ищем дальше.\n",
    "> 4. Повторяем, пока отрезок не станет достаточно малым.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759d170",
   "metadata": {},
   "source": [
    "<h1>6 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ccc75",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 6**\n",
    "\n",
    "---\n",
    "\n",
    "### Тема\n",
    "\n",
    "**Решение нелинейных уравнений методом простой итерации.**\n",
    "– Условия сходимости: сжимающие отображения и неподвижная точка.\n",
    "– Варианты выбора итерационной функции.\n",
    "– Решение систем уравнений.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Идея метода простой итерации\n",
    "\n",
    "1. **Исходная задача.** У нас есть уравнение\n",
    "\n",
    "   $$\n",
    "     F(x) = 0.\n",
    "   $$\n",
    "2. **Переход к форме итерации.** Переписываем его в эквивалентном виде\n",
    "\n",
    "   $$\n",
    "     x = \\varphi(x),\n",
    "   $$\n",
    "\n",
    "   так что все решения исходного и нового уравнения совпадают.\n",
    "3. **Алгоритм.**\n",
    "\n",
    "   1. Берём начальное приближение $x_0$.\n",
    "   2. Вычисляем\n",
    "\n",
    "      $$\n",
    "        x_{n+1} = \\varphi(x_n),\\quad n=0,1,2,\\dots\n",
    "      $$\n",
    "   3. Останавливаемся, когда $\\bigl|x_{n+1}-x_n\\bigr|\\le\\varepsilon$.\n",
    "4. Точка $x^*$, удовлетворяющая $x^*=\\varphi(x^*)$, называется **неподвижной** – это и есть наш корень.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Условия сходимости: сжимающие отображения\n",
    "\n",
    "Чтобы последовательность $\\{x_n\\}$ сходилась к $x^*$, достаточно, чтобы $\\varphi$ задавало **сжимающее отображение** на некотором отрезке $[a,b]$:\n",
    "\n",
    "$$\n",
    "\\bigl|\\varphi(x)-\\varphi(y)\\bigr|\\le q\\,|x-y|,\\quad q<1,\\;\\forall x,y\\in[a,b].\n",
    "$$\n",
    "\n",
    "По теореме о фиксированной точке:\n",
    "\n",
    "* Тогда единственная неподвижная точка $x^*\\in[a,b]$ существует и\n",
    "* Для любого $x_0\\in[a,b]$ итерации сходятся к $x^*$.\n",
    "\n",
    "**Практический критерий (через производную).**\n",
    "Если $\\varphi$ непрерывно дифференцируема на $[a,b]$ и\n",
    "\n",
    "$$\n",
    "\\max_{x\\in[a,b]}|\\varphi'(x)| = q < 1,\n",
    "$$\n",
    "\n",
    "то $\\varphi$ — сжимающее отображение, и метод сходится линейно.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Как выбрать итерационную функцию\n",
    "\n",
    "Из уравнения $F(x)=0$ можно получить семейство эквивалентных форм:\n",
    "\n",
    "$$\n",
    "x = x + g(x)\\,F(x),\n",
    "$$\n",
    "\n",
    "где $g(x)$ — произвольная функция, не обращающаяся в нуль на отрезке поиска. Тогда\n",
    "\n",
    "$$\n",
    "\\varphi(x) = x + g(x)\\,F(x).\n",
    "$$\n",
    "\n",
    "* **Метод релаксации**: $g(x)\\equiv C$ (константа).\n",
    "  Тогда\n",
    "  $\\varphi'(x)=1+C\\,F'(x)$, и условие сходимости\n",
    "  $-2< C\\,F'(x^*)<0$.\n",
    "* **Ньютоновская трактовка**: при $g(x)=-1/F'(x)$ получаем метод Ньютона (квадратичная сходимость), но это уже отдельная тема.\n",
    "\n",
    "Цель — подобрать $g(x)$ так, чтобы $|\\varphi'(x)|$ было как можно меньше на отрезке.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Решение систем уравнений\n",
    "\n",
    "Для системы\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "F_1(x,y,\\dots)=0,\\\\\n",
    "F_2(x,y,\\dots)=0,\\\\\n",
    "\\quad\\vdots\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "строят эквивалентную систему неподвижных точек\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x = \\varphi_1(x,y,\\dots),\\\\\n",
    "y = \\varphi_2(x,y,\\dots),\\\\\n",
    "\\qquad\\vdots\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "и итерационно вычисляют вектор\n",
    "$\\mathbf{x}_{n+1} = \\boldsymbol\\varphi(\\mathbf{x}_n)$.\n",
    "Сходимость обеспечивается тем же «сжатием», но уже в $n$-мерном пространстве (норма Якобиана $\\|\\partial\\varphi_i/\\partial x_j\\|<1$) .\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Простое объяснение\n",
    "\n",
    "> **Метод простой итерации** — это когда вы угадываете $x$, подставляете в «правую часть» вида $x=\\varphi(x)$, получаете новое «угадывание» и повторяете.\n",
    "> Чтобы эти угадывания «стягивались» к одному значению (корню), нужно, чтобы операция $x\\mapsto\\varphi(x)$ слегка «сжимала» все числа к настоящему корню (т. е. не растягивала их).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Примеры\n",
    "\n",
    "### Пример 1.\n",
    "\n",
    "**Уравнение:** $x^2-2=0$.\n",
    "**Цель:** найти $\\sqrt2\\approx1.4142$.\n",
    "**Итерация (метод средних):**\n",
    "\n",
    "$$\n",
    "\\varphi(x)=\\tfrac12\\Bigl(x+\\tfrac{2}{x}\\Bigr),\n",
    "$$\n",
    "\n",
    "тогда\n",
    "\n",
    "$$\n",
    "x_{n+1}=\\tfrac12\\Bigl(x_n+\\tfrac{2}{x_n}\\Bigr).\n",
    "$$\n",
    "\n",
    "Здесь $\\varphi'(x)=\\tfrac12\\Bigl(1 - \\tfrac{2}{x^2}\\Bigr)$,\n",
    "и около $x^*$ оно мало по модулю, что даёт быструю (квадратично близкую) сходимость .\n",
    "\n",
    "| $n$ | $x_n$  |\n",
    "| :-: | :----- |\n",
    "|  0  | 1.0    |\n",
    "|  1  | 1.5    |\n",
    "|  2  | 1.4167 |\n",
    "|  3  | 1.4142 |\n",
    "\n",
    "---\n",
    "\n",
    "### Пример 2.\n",
    "\n",
    "**Уравнение:** $\\cos x - x = 0$.\n",
    "**Итерация:**\n",
    "\n",
    "$$\n",
    "\\varphi(x)=\\cos x.\n",
    "$$\n",
    "\n",
    "Так как $|\\varphi'(x)|=|\\sin x|\\le1$, но на подходящем отрезке, например $[0,1]$, $\\max|\\sin x|\\approx0.84<1$, метод сходится линейно.\n",
    "\n",
    "| $n$ | $x_n$  |\n",
    "| :-: | :----- |\n",
    "|  0  | 0.5    |\n",
    "|  1  | 0.8776 |\n",
    "|  2  | 0.6390 |\n",
    "|  3  | 0.8027 |\n",
    "|  …  | …      |\n",
    "|  7  | 0.7391 |\n",
    "\n",
    "---\n",
    "\n",
    "### Пример 3. (Система из двух уравнений)\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "2x - y = 3,\\\\\n",
    "3y - x^2 = 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Перепишем в виде неподвижной точки:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x = \\tfrac{3+y}{2},\\\\\n",
    "y = \\tfrac{1 + x^2}{3}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Итерации:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_{n+1} = \\tfrac{3 + y_n}{2},\\\\\n",
    "y_{n+1} = \\tfrac{1 + x_n^2}{3}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "При начальном $(x_0,y_0)=(1,1)$ получаем быстрое (линейное) стягивание к единственному решению.\n",
    "\n",
    "---\n",
    "\n",
    "**Вывод:**\n",
    "Метод простой итерации универсален и прозрачен, но его надёжность и скорость зависят от выбора $\\varphi(x)$ — именно этому посвящены критерии сжимающих отображений и анализ производной итерационной функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03201b22",
   "metadata": {},
   "source": [
    "<h1>6 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f318bf",
   "metadata": {},
   "source": [
    "**Билет № 6. Метод простой итерации: шпаргалка**\n",
    "\n",
    "---\n",
    "\n",
    "1. **Переписывание уравнения**\n",
    "\n",
    "   $$\n",
    "   F(x)=0\\quad\\Longrightarrow\\quad x=\\varphi(x)\\quad(\\text{эквивалентно}).\n",
    "   $$\n",
    "\n",
    "2. **Итерационный процесс**\n",
    "\n",
    "   $$\n",
    "   x_{n+1}=\\varphi(x_n),\\quad n=0,1,2,\\dots\n",
    "   $$\n",
    "\n",
    "   Стоп-критерий: $|x_{n+1}-x_n|\\le\\varepsilon$.\n",
    "\n",
    "3. **Неподвижная точка**\n",
    "   Корень $x^*$ — это такая точка, что\n",
    "   $\\;x^*=\\varphi(x^*)$.\n",
    "\n",
    "4. **Условие сходимости (сжимающее отображение)**\n",
    "   На отрезке $[a,b]$:\n",
    "\n",
    "   $$\n",
    "   |\\varphi(x)-\\varphi(y)|\\le q\\;|x-y|,\\quad q<1.\n",
    "   $$\n",
    "\n",
    "   Достаточно: $\\max_{[a,b]}|\\varphi'(x)|=q<1$.\n",
    "\n",
    "5. **Выбор $\\varphi(x)$**\n",
    "\n",
    "   * **Релаксация**: $\\varphi(x)=x+C\\,F(x)$,\n",
    "     условие: $-2< C\\,F'(x^*)<0$.\n",
    "   * **Квадратное ускорение**: $\\varphi(x)=x-\\tfrac{F(x)}{F'(x)}$ (метод Ньютона).\n",
    "   * **Другие**: любой $g(x)\\neq0$ в $\\varphi(x)=x+g(x)\\,F(x)$, чтобы $|\\varphi'|$ был мал.\n",
    "\n",
    "6. **Системы уравнений**\n",
    "   Для $\\mathbf{F}(\\mathbf{x})=\\mathbf{0}$ строим\n",
    "   $\\;\\mathbf{x}=\\boldsymbol\\varphi(\\mathbf{x})$,\n",
    "   итерации $\\mathbf{x}_{n+1}=\\boldsymbol\\varphi(\\mathbf{x}_n)$.\n",
    "   Критерий: норма Якобиана $\\|\\partial\\varphi_i/\\partial x_j\\|<1$.\n",
    "\n",
    "---\n",
    "\n",
    "> **Ключ**: каждую итерацию «подставляй» в $\\varphi$ и проверяй, что функция «стягивает» значения (малый $|\\varphi'|$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68fcb6",
   "metadata": {},
   "source": [
    "<h1>7 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a743631",
   "metadata": {},
   "source": [
    "Вот развёрнутый ответ по пунктам “билета” и два примера, иллюстрирующих метод Ньютона.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Метод Ньютона для одного уравнения\n",
    "\n",
    "Пусть нужно найти корень уравнения\n",
    "\n",
    "$$\n",
    "F(x) = 0.\n",
    "$$\n",
    "\n",
    "1. **Идея.**\n",
    "   В окрестности приближения $x_n$ функция $F(x)$ заменяется её касательной:\n",
    "\n",
    "   $$\n",
    "   F(x)\\approx F(x_n) + F'(x_n)(x - x_n).\n",
    "   $$\n",
    "\n",
    "   Решая линейное уравнение\n",
    "   $\\;F(x_n) + F'(x_n)(x - x_n)=0$,\n",
    "   получаем новое приближение\n",
    "\n",
    "   $$\n",
    "   x_{n+1} = x_n - \\frac{F(x_n)}{F'(x_n)}.\n",
    "   $$\n",
    "\n",
    "2. **Условия применимости.**\n",
    "\n",
    "   * $F$ должна быть дифференцируема около корня.\n",
    "   * В окрестности решения $F'(x)\\neq0$ (иначе касательная горизонтальна и не пересекает ось абсцисс).\n",
    "\n",
    "3. **Выбор начального приближения.**\n",
    "   Чем ближе $x_0$ к истинному корню, тем быстрее и надёжнее сходится метод.\n",
    "   Чтобы гарантировать сходимость, часто выбирают $x_0$ так, чтобы на отрезке $[a,b]$, где лежит корень:\n",
    "\n",
    "   * $F$, $F'$ и $F''$ сохраняют знак (монотонность и «выпуклость» не меняются).\n",
    "   * Начальное $x_0$ берут в том конце отрезка, где одновременно одинаковы знаки $F(x)$ и $F''(x)$.\n",
    "\n",
    "4. **Скорость сходимости.**\n",
    "   При условии «хорошего» приближения (когда высшие члены Тейлора малы) ошибка $\\delta_n = x_n - x^*$ убывает по квадратичному закону:\n",
    "\n",
    "   $$\n",
    "   \\delta_{n+1} \\approx C\\,\\delta_n^2,\n",
    "   $$\n",
    "\n",
    "   что означает **очень быструю** (квадратичную) сходимость.\n",
    "\n",
    "5. **Метод Ньютона как частный случай простой итерации.**\n",
    "   Общая простая итерация:\n",
    "\n",
    "   $$\n",
    "   x = \\varphi(x),\\quad x_{n+1}=\\varphi(x_n).\n",
    "   $$\n",
    "\n",
    "   Если выбрать\n",
    "   $\\;\\varphi(x)=x - \\tfrac{F(x)}{F'(x)}$,\n",
    "   то именно получим метод Ньютона. При этом $\\varphi'(x^*)=0$, что обеспечивает максимальную скорость сходимости.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Обобщение на систему уравнений\n",
    "\n",
    "Рассмотрим систему\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "F_1(x_1,\\dots,x_N)=0,\\\\\n",
    "\\quad\\vdots\\\\\n",
    "F_N(x_1,\\dots,x_N)=0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Шаги метода:\n",
    "\n",
    "1. **Начальное приближение** $\\mathbf x^{(0)}=(x_1^{(0)},\\dots,x_N^{(0)})$.\n",
    "2. **Линейное приближение.** Выписываем первый порядок Тейлора для всех $F_i$:\n",
    "\n",
    "   $$\n",
    "   F_i(\\mathbf x^{(n)}) + \\sum_{j=1}^N \\frac{\\partial F_i}{\\partial x_j}(\\mathbf x^{(n)})\\;\\Delta x_j = 0.\n",
    "   $$\n",
    "3. **Матрица Якоби** $J_{ij}=\\frac{\\partial F_i}{\\partial x_j}$.\n",
    "   Решаем линейную систему\n",
    "\n",
    "   $$\n",
    "   J(\\mathbf x^{(n)})\\;\\Delta \\mathbf x = -\\,\\mathbf F(\\mathbf x^{(n)}).\n",
    "   $$\n",
    "4. **Обновление** $\\mathbf x^{(n+1)}=\\mathbf x^{(n)}+\\Delta\\mathbf x$.\n",
    "5. **Критерий сходимости**: норма вектора поправок $\\|\\Delta\\mathbf x\\|$ меньше заданного $\\varepsilon$.\n",
    "6. **Повторять**, пока $\\|\\Delta\\mathbf x\\|>\\varepsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Примеры\n",
    "\n",
    "### Пример 1. Одномерное уравнение\n",
    "\n",
    "$$\n",
    "F(x)=x^2-2=0\n",
    "$$\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "1. $F'(x)=2x$.\n",
    "2. Итерационная формула:\n",
    "\n",
    "   $$\n",
    "   x_{n+1} = x_n - \\frac{x_n^2-2}{2x_n} \n",
    "           = \\frac12\\Bigl(x_n + \\frac{2}{x_n}\\Bigr).\n",
    "   $$\n",
    "3. Возьмём $x_0=1$.\n",
    "\n",
    "   * $n=0$: $x_1 = \\tfrac12(1 + 2/1)=1.5$.\n",
    "   * $n=1$: $x_2 = \\tfrac12(1.5 + 2/1.5)\\approx1.4167$.\n",
    "   * $n=2$: $x_3\\approx1.4142$.\n",
    "     Уже на третьем шаге достигаем точности порядка $10^{-4}$.\n",
    "\n",
    "### Пример 2. Система из двух уравнений\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "F_1(x,y) = x^2 + y^2 - 5 = 0,\\\\\n",
    "F_2(x,y) = x\\,y - 1 = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "1. **Якобиан**\n",
    "\n",
    "   $$\n",
    "   J=\\begin{pmatrix}\n",
    "   2x & 2y\\\\\n",
    "   y  & x\n",
    "   \\end{pmatrix}.\n",
    "   $$\n",
    "2. Начальное приближение $(x_0,y_0)=(2,1)$.\n",
    "3. Вычисляем\n",
    "   $\\;\\mathbf F(2,1)=\\bigl(2^2+1^2-5,\\;2\\cdot1-1\\bigr)=(0,1)$.\n",
    "   Якобиан в точке $(2,1)$:\n",
    "   $\\;J=\\begin{pmatrix}4&2\\\\1&2\\end{pmatrix}.$\n",
    "4. Решаем линейную систему\n",
    "\n",
    "   $$\n",
    "   \\begin{pmatrix}4&2\\\\1&2\\end{pmatrix}\n",
    "   \\begin{pmatrix}\\Delta x\\\\\\Delta y\\end{pmatrix}\n",
    "   =-\\begin{pmatrix}0\\\\1\\end{pmatrix}.\n",
    "   $$\n",
    "\n",
    "   Получаем, например, $(\\Delta x,\\Delta y)=(-\\tfrac14,\\;\\tfrac18)$.\n",
    "5. Новая точка\n",
    "   $(x_1,y_1)=(2-0.25,\\;1+0.125)=(1.75,\\;1.125)$.\n",
    "6. Повторяя, мы быстро стягиваемся к решению примерно $(\\sqrt5,\\;1/\\sqrt5)\\approx(2.236,0.447)$.\n",
    "\n",
    "---\n",
    "\n",
    "Таким образом, метод Ньютона в одномерном случае даёт очень быструю (квадратичную) сходимость при хорошем выборе $x_0$, а в многомерном сводится к решению линейной системы на каждом шаге с использованием матрицы Якоби.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b513370",
   "metadata": {},
   "source": [
    "<h1>7 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf5569",
   "metadata": {},
   "source": [
    "**Билет 7.**\n",
    "Решение нелинейного уравнения методом Ньютона. Условия применимости. Выбор начального приближения. Достаточное условие сходимости. Скорость сходимости. Метод Ньютона как частный случай простой итерации. Обобщение метода Ньютона на случай системы нелинейных уравнений.\n",
    "\n",
    "---\n",
    "\n",
    "### Краткая «шпаргалка»\n",
    "\n",
    "1. **Формула итерации (одномерный случай):**\n",
    "\n",
    "   $$\n",
    "   x_{n+1} = x_n - \\frac{F(x_n)}{F'(x_n)}.\n",
    "   $$\n",
    "\n",
    "2. **Условия применимости:**\n",
    "\n",
    "   * $F$ непрерывно дифференцируема в окрестности корня.\n",
    "   * $F'(x)\\neq0$ на этом отрезке.\n",
    "\n",
    "3. **Выбор начального приближения $x_0$:**\n",
    "\n",
    "   * Ближе к действительному корню → быстрее сходимость.\n",
    "   * Часто берут край отрезка $[a,b]$, где $F(a)\\,F''(a)>0$ (или то же в $b$).\n",
    "\n",
    "4. **Сходимость и её скорость:**\n",
    "\n",
    "   * Квадратичная: ошибка $\\delta_{n+1}\\approx C\\,\\delta_n^2$.\n",
    "   * Гарантируется при «хорошей» гладкости $F$ и малой начальной ошибке.\n",
    "\n",
    "5. **Как частный случай простой итерации:**\n",
    "   $\\varphi(x)=x - F(x)/F'(x)$, причём $\\varphi'(x^*)=0$.\n",
    "\n",
    "6. **Обобщение на систему $ \\mathbf F(\\mathbf x)=\\mathbf0$:**\n",
    "\n",
    "   * Якобиан $J_{ij}=\\partial F_i/\\partial x_j$.\n",
    "   * На каждом шаге решаем\n",
    "\n",
    "     $$\n",
    "       J(\\mathbf x^{(n)})\\;\\Delta\\mathbf x = -\\,\\mathbf F(\\mathbf x^{(n)}),\n",
    "     $$\n",
    "\n",
    "     после чего $\\mathbf x^{(n+1)}=\\mathbf x^{(n)}+\\Delta\\mathbf x$.\n",
    "   * Критерий остановки: $\\|\\Delta\\mathbf x\\|<\\varepsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "**Конец шпаргалки.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d9b5f",
   "metadata": {},
   "source": [
    "<h1>8 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a45374",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 8. Интерполяция таблично заданной функции**\n",
    "\n",
    "1. **Постановка задачи интерполяции.**\n",
    "   • Даны узлы интерполяции $(x_i, y_i)$, где $y_i = f(x_i)$ и все $x_i$ различны и упорядочены.\n",
    "   • Требуется построить функцию-оценку $F(x)$, такую что $F(x_i)=y_i$ для всех узлов, а для $x$ между узлами считать $F(x)\\approx f(x)$.&#x20;\n",
    "\n",
    "2. **Единственность интерполяционного многочлена.**\n",
    "   • Существует ровно один многочлен $P_n(x)$ степени не выше $n$, который проходит через все $n+1$ узлов.\n",
    "   • Доказательство опирается на невырожденность системы с матрицей Вандермонда, определитель которой отличен от нуля при разных $x_i$.&#x20;\n",
    "\n",
    "3. **Форма Лагранжа.**\n",
    "   • Многочлен $P_n(x)$ удобно записать через базисные многочлены\n",
    "\n",
    "   $$\n",
    "     L_k(x)=\\prod_{j\\neq k}\\frac{x - x_j}{x_k - x_j}, \n",
    "     \\quad P_n(x)=\\sum_{k=0}^n y_k\\,L_k(x).\n",
    "   $$\n",
    "\n",
    "   • В узле $x_i$ все $L_k$ обращаются в ноль, кроме $L_i(x_i)=1$, что обеспечивает автоматическое выполнение условий интерполяции.&#x20;\n",
    "\n",
    "4. **Выражение для ошибки интерполяции (остаточный член).**\n",
    "   • Если $f$ имеет непрерывную $(n+1)$-ю производную на отрезке, то при любом $x$ найдётся $\\xi$ «между» узлами, для которой\n",
    "\n",
    "   $$\n",
    "     r_n(x)=f(x)-P_n(x)\n",
    "     =\\frac{f^{(n+1)}(\\xi)}{(n+1)!}\\,\\prod_{i=0}^n(x-x_i).\n",
    "   $$\n",
    "\n",
    "   • Это выражение даёт представление о том, как растёт погрешность в зависимости от расположения узлов и гладкости $f$.&#x20;\n",
    "\n",
    "---\n",
    "\n",
    "**Примеры решения.**\n",
    "\n",
    "1. **Линейная интерполяция ($n=1$) для $f(x)=x^2$.**\n",
    "   Узлы: $(1,1)$, $(2,4)$.\n",
    "\n",
    "   $$\n",
    "     P_1(x)=1\\cdot\\frac{x-2}{1-2}+4\\cdot\\frac{x-1}{2-1}\n",
    "            =- (x-2)+4(x-1)=2x-2.\n",
    "   $$\n",
    "\n",
    "   Ошибка в точке $x=1.5$:\n",
    "\n",
    "   $$\n",
    "     r_1(1.5)=1.5^2 - (2\\cdot1.5-2)=2.25-1=1.25,\n",
    "     \\quad \\omega_2(1.5)=(1.5-1)(1.5-2)= -0.25.\n",
    "   $$\n",
    "\n",
    "2. **Кубическая интерполяция ($n=3$) для $f(x)=\\sin x$.**\n",
    "   Узлы: $x_0=0,\\;x_1=\\tfrac\\pi6,\\;x_2=\\tfrac\\pi3,\\;x_3=\\tfrac\\pi2$.\n",
    "   По формуле Лагранжа строим $P_3(x)$, а затем оцениваем, например, $P_3(\\pi/4)\\approx \\sin(\\pi/4)=0.7071$.\n",
    "   Остаточный член оценим через $\\max|f^{(4)}(\\xi)|=|\\sin\\xi|\\le1$ и $\\omega_4(\\pi/4)=\\prod_{i=0}^3(\\pi/4-x_i)$.\n",
    "\n",
    "Какой пример интерполяции вам показался более понятным и интересным?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da2fc3",
   "metadata": {},
   "source": [
    "<h1>8 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378312c2",
   "metadata": {},
   "source": [
    "**Шпаргалка по билет № 8 «Интерполяция таблично заданной функции»**\n",
    "\n",
    "1. **Цель:**\n",
    "   – Построить многочлен $P_n(x)$ степени ≤ $n$, проходящий через данные узлы $(x_i,y_i)$.\n",
    "\n",
    "2. **Единственность:**\n",
    "   – Для $n+1$ разных точек такой многочлен единственный (матрица Вандермонда невырождена).\n",
    "\n",
    "3. **Форма Лагранжа:**\n",
    "\n",
    "   $$\n",
    "     P_n(x)=\\sum_{k=0}^n y_k\\;L_k(x), \n",
    "     \\quad\n",
    "     L_k(x)=\\prod_{j\\neq k}\\frac{x - x_j}{x_k - x_j}.\n",
    "   $$\n",
    "\n",
    "4. **Условия:**\n",
    "   – $P_n(x_i)=y_i$ благодаря свойству $L_k(x_i)=\\delta_{ik}$.\n",
    "\n",
    "5. **Остаточный член (ошибка):**\n",
    "\n",
    "   $$\n",
    "     r_n(x)=f(x)-P_n(x)\n",
    "     =\\frac{f^{(n+1)}(\\xi)}{(n+1)!}\\;\\prod_{i=0}^n(x - x_i),\n",
    "     \\quad \\xi\\in[\\min x_i,\\max x_i].\n",
    "   $$\n",
    "\n",
    "6. **Краткий алгоритм ответа на экзамене:**\n",
    "\n",
    "   1. Определить узлы и степень $n$.\n",
    "   2. Сказать про единственность (Вандермонд).\n",
    "   3. Записать формулу Лагранжа.\n",
    "   4. Объяснить $L_k(x_i)$.\n",
    "   5. Упомянуть формулу ошибки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293c00e",
   "metadata": {},
   "source": [
    "<h1>9 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5cea2",
   "metadata": {},
   "source": [
    "**Экзаменационный билет № 9. Интерполяция таблично заданной функции. Сходимость интерполяции. Теоремы Фабера и Марцинкевича. Свойства многочленов Чебышёва.**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Постановка задачи и виды сходимости\n",
    "\n",
    "* **Интерполяция** — это построение многочлена $P_n(x)$ степени ≤ $n$, который в узлах $\\{x_i\\}_{i=0}^n$ точно воспроизводит заданные значения $f(x_i)$.\n",
    "* **Сходимость процесса** означает, что при $n\\to\\infty$ (и соответствующем выборе узлов) погрешность\n",
    "\n",
    "  $$\n",
    "    r_n(x)=f(x)-P_n(x)\n",
    "  $$\n",
    "\n",
    "  стремится к нулю.\n",
    "* Различают **точечную сходимость** (для каждого фиксированного $x$) и **равномерную сходимость** на отрезке $[a,b]$, когда\n",
    "  $\\max_{x\\in[a,b]}|r_n(x)|\\to0$ при $n\\to\\infty$. На сходимость влияет не только число узлов, но и их распределение по отрезку .\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Теорема Фабера\n",
    "\n",
    "* **Утверждение:** для **любой** заранее выбранной схемы расположения узлов на $[a,b]$ найдётся непрерывная функция $f$, для которой интерполяционные многочлены **не** сходятся равномерно к $f$ на всём отрезке.\n",
    "* **Вывод:** не существует единой «универсальной» сетки, подходящей для **всех** функций .\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Теорема Марцинкевича\n",
    "\n",
    "* **Утверждение:** для **каждой конкретной** функции $f$ можно подобрать такую последовательность сеток, что интерполяционный процесс будет сходиться равномерно на всём отрезке.\n",
    "* **Ключевой пример:** для функции Рунге $f(x)=1/(1+25x^2)$ равномерная сходимость достигается при выборе узлов в виде **корней многочленов Чебышёва** на $[-1,1]$ .\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Многочлены Чебышёва $T_n(x)$ и их свойства\n",
    "\n",
    "1. **Рекуррентное определение:**\n",
    "\n",
    "   $$\n",
    "     T_0(x)=1,\\quad T_1(x)=x,\\quad\n",
    "     T_{n+1}(x)=2x\\,T_n(x)-T_{n-1}(x).\n",
    "   $$\n",
    "\n",
    "2. **Тригонометрическое представление:**\n",
    "\n",
    "   $$\n",
    "     T_n(x)=\\cos\\bigl(n\\arccos x\\bigr),\n",
    "     \\qquad x\\in[-1,1].\n",
    "   $$\n",
    "\n",
    "3. **Корни многочленов:**\n",
    "\n",
    "   $$\n",
    "     x_k=\\cos\\!\\Bigl(\\tfrac{2k+1}{2n}\\pi\\Bigr),\\quad\n",
    "     k=0,1,\\dots,n-1.\n",
    "   $$\n",
    "\n",
    "   Эти узлы равномерно «сжимают» интервалы к краям, минимизируя максимальную величину полинома\n",
    "   $\\omega_{n+1}(x)=\\prod_{i=0}^n(x-x_i)$ (принцип минимакса) .\n",
    "\n",
    "4. **Оптимальность для интерполяции:**\n",
    "   — Распределение корней $T_n$ обеспечивает наименьшую максимальную погрешность при интерполяции на $[-1,1]$.\n",
    "   — Для произвольного отрезка $[a,b]$ достаточно линейно преобразовать корни $[−1,1]$ в точки $[a,b]$ .\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Примеры\n",
    "\n",
    "1. **Разбегание на равномерных узлах (функция Рунге):**\n",
    "\n",
    "   $$\n",
    "     f(x)=\\frac1{1+25x^2},\\quad x\\in[-1,1].\n",
    "   $$\n",
    "\n",
    "   При ровных узлах интерполяционные многочлены быстро «гуляют» у краёв отрезка (эффект Рунге): погрешность растёт при $n\\to\\infty$ .\n",
    "\n",
    "2. **Сходимость на узлах Чебышёва (та же функция):**\n",
    "   Выбираем узлы $x_k=\\cos\\!\\bigl((2k+1)\\pi/2n\\bigr)$. Интерполяция полиномами тех же степеней стабильно приближает $f(x)$ и $ \\max|r_n(x)|\\to0$ равномерно на всём $[-1,1]$ .\n",
    "\n",
    "---\n",
    "\n",
    "Таким образом, **ключ к успешной интерполяции** — не просто увеличить число узлов, а грамотно их разместить: теорема Фабера предупреждает об опасностях равномерных сеток, а сетка Чебышёва, благодаря свойствам $T_n(x)$, реализует идею Марцинкевича, обеспечивая равномерную сходимость.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a53924",
   "metadata": {},
   "source": [
    "<h1>9 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec3521",
   "metadata": {},
   "source": [
    "**Расширенная шпаргалка по билет № 9 «Интерполяция. Сходимость. Фабер. Марцинкевич. Чебышёвы»**\n",
    "\n",
    "1. **Цель и единственность:**\n",
    "   – Нужно найти многочлен $P_n(x)$ степени ≤ $n$, который в узлах $\\{x_i\\}$ точно даёт значения $f(x_i)$.\n",
    "   – Единственность гарантируется невырожденностью матрицы Вандермонда.\n",
    "\n",
    "2. **Виды сходимости:**\n",
    "   – *Точечная*: при любом фиксированном $x$, $P_n(x)\\to f(x)$ по мере роста $n$.\n",
    "   – *Равномерная*: $\\max_{x\\in[a,b]}|P_n(x)-f(x)|\\to0$; зависит от выбора узлов.\n",
    "\n",
    "3. **Теорема Фабера:**\n",
    "   – Для любой заранее заданной схемы узлов найдётся хотя бы одна «плохая» функция $f$, для которой равномерная сходимость не выполняется.\n",
    "   – Это означает: нельзя одним универсальным способом размещать узлы для всех функций.\n",
    "\n",
    "4. **Теорема Марцинкевича:**\n",
    "   – Для каждой конкретной функции $f\\in C[a,b]$ можно подобрать свою, «адаптивную» сетку узлов, обеспечивающую равномерную сходимость.\n",
    "\n",
    "5. **Роль многочленов Чебышёва:**\n",
    "   – Узлы по формулам\n",
    "   $\\displaystyle x_k=\\cos\\!\\bigl(\\tfrac{2k+1}{2n+2}\\pi\\bigr)$ на $[-1,1]$\n",
    "   минимизируют максимум модуля $\\omega_{n+1}(x)=\\prod(x-x_i)$.\n",
    "   – При такой сетке погрешность интерполяции растёт наиболее медленно (минимакс-принцип).\n",
    "\n",
    "6. **Краткий алгоритм ответа:**\n",
    "\n",
    "   1. Обозначить задачу (найти $P_n$, единственность).\n",
    "   2. Описать точечную и равномерную сходимость.\n",
    "   3. Сформулировать обе теоремы (Фабера «против», Марцинкевича «за»).\n",
    "   4. Показать формулу узлов Чебышёва и объяснить их оптимальность.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dafce1",
   "metadata": {},
   "source": [
    "<h1>10 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65f5d4",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Кубические сплайны\n",
    "\n",
    "#### 1. **Недостатки интерполяции многочленами**\n",
    "Интерполяция многочленами (например, полиномом Лагранжа) имеет два ключевых недостатка:\n",
    "- **Жёсткая связь степени и количества узлов**:  \n",
    "  Для $n+1$ точек требуется многочлен степени $n$. При больших $n$ полиномы становятся неустойчивыми: возникают сильные колебания (эффект Рунге), особенно на краях интервала.  \n",
    "  *Пример*: Для 10 точек степень полинома — 9. Такой полином может давать дикие осцилляции между узлами, даже если исходная функция гладкая.\n",
    "\n",
    "- **Нелокальность**:  \n",
    "  Изменение значения в одном узле влияет на поведение полинома *на всём интервале*. Это неудобно, если данные имеют локальные особенности.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Понятие сплайна**\n",
    "**Сплайн** — кусочно-полиномиальная функция, \"склеенная\" из отрезков многочленов низкой степени.  \n",
    "- **Ключевые свойства**:  \n",
    "  - Непрерывность самой функции и её производных (до определённого порядка) в узлах.  \n",
    "  - Степень сплайна *не зависит* от количества узлов (можно строить сплайн 3-й степени для 100 точек).  \n",
    "- **Почему \"сплайн\"?**  \n",
    "  Термин происходит от гибкой чертёжной рейки (англ. *spline*), которая под действием упругости принимает форму с минимальной кривизной, проходя через заданные точки. Математически это соответствует *кубическому сплайну*.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Построение кубического сплайна**\n",
    "Кубический сплайн $S(x)$ состоит из сегментов-кубиков на каждом интервале $[x_i, x_{i+1}]$:  \n",
    "$\n",
    "s_i(x) = A_i + B_i x + C_i x^2 + D_i x^3.\n",
    "$\n",
    "\n",
    "**Условия для определения коэффициентов**:  \n",
    "1. **Интерполяция**: $s_i(x_i) = f_i$, $s_i(x_{i+1}) = f_{i+1}$.  \n",
    "2. **Непрерывность производных**:  \n",
    "   - $s'_{i-1}(x_i) = s'_i(x_i)$ (гладкость \"без изломов\"),  \n",
    "   - $s''_{i-1}(x_i) = s''_i(x_i)$ (плавность кривизны).  \n",
    "\n",
    "**Проблема**: Условия дают $4n-2$ уравнений, но коэффициентов $4n$ → нужно ещё **2 граничных условия**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Типы граничных условий**\n",
    "- **Естественный сплайн**: $s''(x_0) = s''(x_n) = 0$ (рейка \"свободна\" на концах).  \n",
    "- **Not-a-knot**: $s_0'''(x_1) = s_1'''(x_1)$, $s_{n-2}'''(x_{n-1}) = s_{n-1}'''(x_{n-1})$ (первые и последние сегменты \"сливаются\").  \n",
    "- **Clamped**: $s'(x_0) = s'(x_n) = 0$ (закреплённые концы).  \n",
    "- **Периодический**: $s'(x_0) = s'(x_n)$, $s''(x_0) = s''(x_n)$.  \n",
    "\n",
    "> **Важно!** Естественный сплайн не всегда даёт лучшее приближение — его кривизна может быть слишком большой у границ.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Практическая реализация**\n",
    "Чтобы упростить расчёты, сплайн удобно записать в форме:  \n",
    "$\n",
    "s_i(x) = f_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3.\n",
    "$  \n",
    "**Смысл коэффициентов**:  \n",
    "- $b_i = s_i'(x_i)$ (первая производная),  \n",
    "- $c_i = \\frac{s_i''(x_i)}{2}$ (половина второй производной),  \n",
    "- $d_i = \\frac{s_i'''(x_i)}{6}$ (шестая часть третьей производной).  \n",
    "\n",
    "**Алгоритм**:  \n",
    "1. Решить систему уравнений для $\\sigma_i = \\frac{s_i''(x_i)}{6}$ (трёхдиагональная матрица).  \n",
    "2. Вычислить $b_i, c_i, d_i$ через $\\sigma_i$.  \n",
    "\n",
    "---\n",
    "\n",
    "### Примеры\n",
    "#### Пример 1: Простой случай (3 точки)\n",
    "**Дано**:  \n",
    "Узлы: $x = [0, 1, 2]$, $f = [1, 3, 2]$.  \n",
    "Граничные условия: *естественный сплайн* ($s''(0) = s''(2) = 0$).  \n",
    "\n",
    "**Решение**:  \n",
    "1. Сегмент $s_0(x)$ на $[0,1]$: $s_0(x) = 1 + b_0 x + c_0 x^2 + d_0 x^3$.  \n",
    "2. Сегмент $s_1(x)$ на $[1,2]$: $s_1(x) = 3 + b_1(x-1) + c_1(x-1)^2 + d_1(x-1)^3$.  \n",
    "3. Условия:  \n",
    "   - $s_0(1) = 3$ → $1 + b_0 + c_0 + d_0 = 3$,  \n",
    "   - $s_1(2) = 2$ → $3 + b_1 + c_1 + d_1 = 2$,  \n",
    "   - Непрерывность производных в $x=1$:  \n",
    "     $\n",
    "     s_0'(1) = s_1'(1) \\Rightarrow b_0 + 2c_0 + 3d_0 = b_1,  \n",
    "     $  \n",
    "     $\n",
    "     s_0''(1) = s_1''(1) \\Rightarrow 2c_0 + 6d_0 = 2c_1.  \n",
    "     $  \n",
    "   - Граничные: $s_0''(0) = 0 \\Rightarrow 2c_0 = 0$, $s_1''(2) = 0 \\Rightarrow 2c_1 + 6d_1 = 0$.  \n",
    "\n",
    "**Результат**:  \n",
    "$c_0 = 0$, $c_1 = -3$, $d_0 = 2$, $d_1 = 1$, $b_0 = 3$, $b_1 = -4$.  \n",
    "$\n",
    "s_0(x) = 1 + 3x + 2x^3, \\quad s_1(x) = 3 -4(x-1) -3(x-1)^2 + (x-1)^3.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Пример 2: Сравнение с полиномом Лагранжа\n",
    "**Дано**:  \n",
    "Точки: $(0,0), (1,1), (2,0), (3,1)$.  \n",
    "\n",
    "- **Полином Лагранжа** (степень 3):  \n",
    "  $\n",
    "  P_3(x) = -\\frac{x^3}{2} + \\frac{5x^2}{2} - 3x.\n",
    "  $  \n",
    "  На интервале $[0,3]$ он осциллирует (например, в $x=1.5$: $P_3(1.5) = -0.375$, хотя данные идут от 0 до 1).  \n",
    "\n",
    "- **Кубический сплайн** (границы *not-a-knot*):  \n",
    "  Сегменты гладко соединяются, кривая следует за данными без выбросов.  \n",
    "  *Результат для $x=1.5$*: $s \\approx 0.8$ (логично между 0 и 1).  \n",
    "\n",
    "---\n",
    "\n",
    "### Итог\n",
    "**Преимущества сплайнов**:  \n",
    "1. **Устойчивость**: Нет осцилляций при большом числе точек.  \n",
    "2. **Локальность**: Изменение данных в одном узле влияет только на соседние сегменты.  \n",
    "3. **Гладкость**: Непрерывность второй производной обеспечивает плавность кривой.  \n",
    "4. **Гибкость**: Разные граничные условия адаптируют сплайн под физику задачи.  \n",
    "\n",
    "**Применение**:  \n",
    "Чертёжные работы, моделирование траекторий, компьютерная графика, обработка экспериментальных данных (например, термодинамика: расчёт теплоты по табличным значениям теплоёмкости)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fe0fd",
   "metadata": {},
   "source": [
    "<h1>10 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de348c",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет: Кубические сплайны  \n",
    "**(Краткая шпаргалка)**  \n",
    "\n",
    "#### 1. **Недостатки интерполяции многочленами**  \n",
    "- **Высокая степень полинома**: Для $n+1$ точек → полином степени $n$.  \n",
    "- **Осцилляции**: Сильные колебания между узлами (эффект Рунге).  \n",
    "- **Глобальное влияние**: Изменение одной точки меняет весь полином.  \n",
    "\n",
    "#### 2. **Сплайн — определение**  \n",
    "- Кусочно-полиномиальная функция степени $k$ (чаще $k=3$).  \n",
    "- **Свойства**:  \n",
    "  - Непрерывность $S(x)$, $S'(x)$, $S''(x)$ в узлах.  \n",
    "  - Степень не зависит от числа узлов.  \n",
    "\n",
    "#### 3. **Кубический сплайн**  \n",
    "- **Сегменты**: На $[x_i, x_{i+1}]$:  \n",
    "  $\n",
    "  s_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3.\n",
    "  $  \n",
    "- **Условия**:  \n",
    "  - Интерполяция: $s_i(x_i) = f_i$, $s_i(x_{i+1}) = f_{i+1}$.  \n",
    "  - Склейка: В узлах $x_i$ ($i=1,...,n-1$):  \n",
    "    $\n",
    "    s_{i-1}'(x_i) = s_i'(x_i), \\quad s_{i-1}''(x_i) = s_i''(x_i).\n",
    "    $  \n",
    "- **Граничные условия** (2 доп. уравнения):  \n",
    "  - **Естественный**: $s''(x_0) = s''(x_n) = 0$.  \n",
    "  - **Not-a-knot**: $s_0'''(x_1) = s_1'''(x_1)$, $s_{n-2}'''(x_{n-1}) = s_{n-1}'''(x_n)$.  \n",
    "  - **Clamped**: $s'(x_0) = A$, $s'(x_n) = B$ (часто $A=B=0$).  \n",
    "\n",
    "#### 4. **Алгоритм построения**  \n",
    "1. Решить СЛАУ для $\\sigma_i = \\frac{s_i''(x_i)}{6}$ (трёхдиагональная матрица).  \n",
    "2. Вычислить коэффициенты:  \n",
    "   $\n",
    "   b_i = \\Delta_i - h_i(\\sigma_{i+1} + 2\\sigma_i), \\quad c_i = 3\\sigma_i, \\quad d_i = \\frac{\\sigma_{i+1} - \\sigma_i}{h_i}.\n",
    "   $  \n",
    "\n",
    "#### 5. **Преимущества перед полиномами**  \n",
    "- **Устойчивость**: Нет осцилляций.  \n",
    "- **Локальность**: Изменение точки влияет только на соседние сегменты.  \n",
    "- **Гладкость**: $C^2$-непрерывность (плавность кривой).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51932d47",
   "metadata": {},
   "source": [
    "<h1>11 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4499107",
   "metadata": {},
   "source": [
    "### Экзаменационный билет:  \n",
    "**Суммарная погрешность численного дифференцирования и её составляющие: ошибка дискретизации (усечения) и ошибка округления. Оптимальный выбор шага.**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Суть проблемы**  \n",
    "При численном дифференцировании мы заменяем точную производную $ f'(x) $ приближённой формулой, например:  \n",
    "$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x)}{h} \\quad \\text{(простейшая формула)}.\n",
    "$  \n",
    "Здесь возникает **два типа погрешностей**:  \n",
    "- **Ошибка усечения (дискретизации)** $\\delta_t$ — из-за отбрасывания старших членов ряда Тейлора.  \n",
    "- **Ошибка округления** $\\delta_r$ — из-за ограниченной точности вычислений значений функции.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Составляющие погрешности**  \n",
    "#### (а) Ошибка усечения ($\\delta_t$)  \n",
    "- **Причина**: Замена бесконечно малого приращения аргумента на конечный шаг $ h $.  \n",
    "- **Формула** (для простейшего метода):  \n",
    "$\n",
    "\\delta_t = \\frac{h}{2} f''(x) + \\frac{h^2}{6} f'''(x) + \\cdots\n",
    "$  \n",
    "- **Свойства**:  \n",
    "  - $\\delta_t \\propto h$ (для малых $ h $).  \n",
    "  - Уменьшается при уменьшении $ h $.  \n",
    "\n",
    "#### (б) Ошибка округления ($\\delta_r$)  \n",
    "- **Причина**: При малых $ h $ значения $ f(x) $ и $ f(x+h) $ близки. Их разность вычисляется с большой относительной ошибкой.  \n",
    "- **Формула**:  \n",
    "$\n",
    "\\delta_r \\approx \\frac{2 \\varepsilon |f(x)|}{h},\n",
    "$  \n",
    "где $\\varepsilon$ — относительная погрешность вычисления $ f(x) $ (например, машинный эпсилон $\\sim 10^{-16}$).  \n",
    "- **Свойства**:  \n",
    "  - $\\delta_r \\propto \\frac{1}{h}$.  \n",
    "  - Растёт при уменьшении $ h $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Суммарная погрешность**  \n",
    "$\n",
    "\\delta_{\\text{сум}} = |\\delta_t| + |\\delta_r|.\n",
    "$  \n",
    "- **График зависимости** (см. рис. в лекции):  \n",
    "  - При уменьшении $ h $: $\\delta_t \\downarrow$, но $\\delta_r \\uparrow$.  \n",
    "  - При увеличении $ h $: $\\delta_t \\uparrow$, но $\\delta_r \\downarrow$.  \n",
    "- **Оптимальный шаг $ h_{\\text{opt}} $** находится в точке минимума $\\delta_{\\text{сум}}$ (где $\\delta_t \\approx \\delta_r$).  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Как найти $ h_{\\text{opt}} $?**  \n",
    "Из условия $|\\delta_t| \\approx |\\delta_r|$:  \n",
    "$\n",
    "\\left| \\frac{h}{2} f''(x) \\right| \\approx \\frac{2 \\varepsilon |f(x)|}{h}.\n",
    "$  \n",
    "Решение относительно $ h $:  \n",
    "$\n",
    "h_{\\text{opt}} \\approx 2 \\sqrt{\\varepsilon \\cdot \\left| \\frac{f(x)}{f''(x)} \\right|}.\n",
    "$  \n",
    "**Проблема**: Для расчёта $ h_{\\text{opt}} $ нужно знать $ f''(x) $, которая обычно неизвестна.  \n",
    "**Практика**: Шаг выбирают экспериментально, например:  \n",
    "- Начинают с $ h \\sim 10^{-6} $,  \n",
    "- Уменьшают $ h $ в 10 раз, пока погрешность не начнёт расти.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Как улучшить точность?**  \n",
    "Используют формулы **высокого порядка точности**, например:  \n",
    "$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} \\quad \\text{(порядок точности 2)}.\n",
    "$  \n",
    "- **Преимущества**:  \n",
    "  - Ошибка усечения $\\delta_t \\propto h^2$ (убывает быстрее!).  \n",
    "  - Можно брать больший шаг $ h $, уменьшая $\\delta_r$.  \n",
    "  - Оптимальная погрешность $\\delta_{\\text{min}}$ ниже, чем у простейшего метода (см. рис. 3 в лекции).  \n",
    "\n",
    "---\n",
    "\n",
    "### Примеры расчета  \n",
    "#### Пример 1: Функция $ f(x) = x^2 $ в точке $ x = 1 $  \n",
    "- Точная производная: $ f'(1) = 2 $.  \n",
    "- Простейшая формула:  \n",
    "  $\n",
    "  f'(1) \\approx \\frac{(1+h)^2 - 1^2}{h} = \\frac{1 + 2h + h^2 - 1}{h} = 2 + h.\n",
    "  $  \n",
    "  - Ошибка усечения: $\\delta_t = h$.  \n",
    "- Симметричная формула:  \n",
    "  $\n",
    "  f'(1) \\approx \\frac{(1+h)^2 - (1-h)^2}{2h} = \\frac{(1 + 2h + h^2) - (1 - 2h + h^2)}{2h} = \\frac{4h}{2h} = 2.\n",
    "  $  \n",
    "  - Ошибка усечения: $\\delta_t = 0$ (для квадратичной функции).  \n",
    "\n",
    "#### Пример 2: Функция $ f(x) = e^x $ в точке $ x = 0 $  \n",
    "- Точная производная: $ f'(0) = 1 $.  \n",
    "- Пусть $\\varepsilon = 10^{-16}$ (машинный эпсилон).  \n",
    "- Оптимальный шаг для простейшей формулы:  \n",
    "  $\n",
    "  h_{\\text{opt}} \\approx 2 \\sqrt{\\frac{\\varepsilon \\cdot |e^0|}{|e^0|}} = 2 \\sqrt{\\varepsilon} \\approx 2 \\times 10^{-8}.\n",
    "  $  \n",
    "- При $ h = 10^{-8} $:  \n",
    "  - $\\delta_t \\approx \\frac{h}{2} \\cdot 1 = 5 \\times 10^{-9}$,  \n",
    "  - $\\delta_r \\approx \\frac{2 \\varepsilon}{h} = \\frac{2 \\times 10^{-16}}{10^{-8}} = 2 \\times 10^{-8}$.  \n",
    "  - Суммарная погрешность: $\\delta_{\\text{сум}} \\approx 2.5 \\times 10^{-8}$.  \n",
    "- При $ h = 10^{-10} $:  \n",
    "  - $\\delta_t \\approx 5 \\times 10^{-11}$,  \n",
    "  - $\\delta_r \\approx \\frac{2 \\times 10^{-16}}{10^{-10}} = 2 \\times 10^{-6}$ (доминирует!).  \n",
    "\n",
    "---\n",
    "\n",
    "### Итог:  \n",
    "1. **Суммарная погрешность** — компромисс между ошибкой усечения и округления.  \n",
    "2. **Оптимальный шаг** $ h_{\\text{opt}} $ минимизирует $\\delta_{\\text{сум}}$.  \n",
    "3. **Формулы высокого порядка** (например, симметричная) дают лучшую точность при бóльших $ h $.  \n",
    "4. **Не используйте слишком малые $ h \\!$** — это увеличивает ошибку округления!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92413e24",
   "metadata": {},
   "source": [
    "<h1>11 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b621a32",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:  \n",
    "**Суммарная погрешность численного дифференцирования** складывается из двух компонент:  \n",
    "1. **Ошибка усечения (дискретизации)** $\\delta_t$:  \n",
    "   - Возникает из-за замены производной разностным отношением.  \n",
    "   - Для формулы $f'(x) \\approx \\frac{f(x+h)-f(x)}{h}$: $\\delta_t \\propto h$.  \n",
    "   - Уменьшается при увеличении $h$.  \n",
    "\n",
    "2. **Ошибка округления** $\\delta_r$:  \n",
    "   - Связана с потерей точности при вычитании близких значений $f(x)$ и $f(x+h)$.  \n",
    "   - $\\delta_r \\propto \\frac{\\varepsilon}{h}$, где $\\varepsilon$ — погрешность вычисления $f(x)$.  \n",
    "   - Увеличивается при уменьшении $h$.  \n",
    "\n",
    "**Суммарная погрешность**:  \n",
    "$\n",
    "\\delta_{\\text{сум}} = |\\delta_t| + |\\delta_r|.\n",
    "$  \n",
    "График $\\delta_{\\text{сум}}(h)$ имеет минимум в точке **оптимального шага $h_{\\text{opt}}$**, где $\\delta_t \\approx \\delta_r$.  \n",
    "\n",
    "**Оптимальный шаг**:  \n",
    "$\n",
    "h_{\\text{opt}} \\approx 2 \\sqrt{\\varepsilon \\cdot \\left| \\frac{f(x)}{f''(x)} \\right|}.\n",
    "$  \n",
    "**Практический выбор**:  \n",
    "- Начать с $h \\sim 10^{-6}$,  \n",
    "- Уменьшать $h$ в 10 раз, пока погрешность не начнёт расти.  \n",
    "\n",
    "**Советы**:  \n",
    "- Используйте формулы высокого порядка (например, $\\frac{f(x+h)-f(x-h)}{2h}$), где $\\delta_t \\propto h^2$.  \n",
    "- Избегайте слишком малых $h$ (усиливается $\\delta_r$).  \n",
    "\n",
    "---\n",
    "\n",
    "### Примеры для иллюстрации:  \n",
    "1. **Для $f(x) = x^2$ в $x=1$**:  \n",
    "   - Симметричная формула: $\\frac{(1+h)^2 - (1-h)^2}{2h} = 2$ (точное значение).  \n",
    "\n",
    "2. **Для $f(x) = e^x$ в $x=0$**:  \n",
    "   - При $h = 10^{-8}$: $\\delta_{\\text{сум}} \\approx 2.5 \\times 10^{-8}$,  \n",
    "   - При $h = 10^{-10}$: $\\delta_{\\text{сум}} \\approx 2 \\times 10^{-6}$ (доминирует $\\delta_r$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04ae0a",
   "metadata": {},
   "source": [
    "<h1>12 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bfa52",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Численное дифференцирование\n",
    "\n",
    "#### 1. **Уточненное дифференцирование. Порядок точности формул дифференцирования**  \n",
    "Численное дифференцирование — это приближенное вычисление производной функции, заданной таблично или аналитически, с помощью конечных разностей.  \n",
    "\n",
    "- **Порядок точности** — максимальная степень многочлена, для которого формула дает точное значение производной.  \n",
    "  - *Пример*: Формула $ f'(x) \\approx \\frac{f(x+h) - f(x)}{h} $ имеет **1-й порядок точности**. Она точна для линейных функций ($ f(x) = ax + b $), но не для квадратичных.  \n",
    "  - Формула $ f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} $ имеет **2-й порядок точности**. Она точна для квадратичных функций ($ f(x) = x^2 $).  \n",
    "\n",
    "**Почему порядок важен?**  \n",
    "Чем выше порядок, тем быстрее уменьшается погрешность при уменьшении шага $ h $. Например, для 2-го порядка ошибка убывает как $ h^2 $, а для 1-го — как $ h $.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Способы уменьшения суммарной погрешности**  \n",
    "Суммарная погрешность $ \\delta $ складывается из:  \n",
    "- **Ошибки усечения (дискретизации)** $ \\delta_t $:  \n",
    "  Возникает из-за отбрасывания старших членов ряда Тейлора.  \n",
    "  - Для формулы (1): $ \\delta_t \\approx \\frac{h}{2} f''(x) $.  \n",
    "  - Для формулы (6): $ \\delta_t \\approx \\frac{h^2}{6} f'''(x) $.  \n",
    "\n",
    "- **Ошибки округления** $ \\delta_r $:  \n",
    "  Обусловлена неточностью вычисления $ f(x) $ (машинная арифметика).  \n",
    "  $$\n",
    "  \\delta_r \\approx \\frac{2 \\varepsilon |f(x)|}{h}, \\quad \\text{где } \\varepsilon \\text{ — относительная погрешность}.\n",
    "  $$  \n",
    "\n",
    "**Как уменьшить суммарную погрешность?**  \n",
    "1. **Увеличить точность вычислений** (уменьшить $ \\varepsilon $):  \n",
    "   Использовать числа с двойной точностью (`double` вместо `float`).  \n",
    "2. **Выбрать оптимальный шаг $ h $** из условия $ |\\delta_t| \\approx |\\delta_r| $:  \n",
    "   $$\n",
    "   \\frac{2 \\varepsilon |f(x)|}{h} \\approx \\left| \\frac{h}{2} f''(x) \\right| \\implies h \\approx \\sqrt[3]{\\frac{4 \\varepsilon |f(x)|}{|f''(x)|}}.\n",
    "   $$  \n",
    "3. **Использовать формулы высшего порядка точности**:  \n",
    "   Например, формула (6) вместо (1) позволяет увеличить шаг $ h $, уменьшив $ \\delta_r $ без роста $ \\delta_t $.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Вывод формулы произвольного порядка точности**  \n",
    "Формулы строятся на основе **интерполяционного многочлена Лагранжа**.  \n",
    "1. Выбирают узлы $ x_k $ вблизи точки $ x $.  \n",
    "2. Строят многочлен $ P_n(x) $, аппроксимирующий $ f(x) $.  \n",
    "3. Производная $ f'(x) $ заменяется на $ P_n'(x) $:  \n",
    "   $$\n",
    "   f'(x) \\approx \\sum_{k=0}^{n} w_k f(x_k), \\quad w_k = \\frac{dL_k^{(n)}(x)}{dx},\n",
    "   $$  \n",
    "   где $ L_k^{(n)}(x) $ — базисные полиномы Лагранжа.  \n",
    "\n",
    "**Пример для 2-го порядка** (узлы: $ x-h, x, x+h $):  \n",
    "$$\n",
    "w_0 = -\\frac{1}{2h}, \\quad w_1 = 0, \\quad w_2 = \\frac{1}{2h} \\implies f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Формулы для высших производных**  \n",
    "- **Вторая производная** (2-й порядок точности):  \n",
    "  $$\n",
    "  f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}.\n",
    "  $$  \n",
    "- **Четвертый порядок точности**:  \n",
    "  $$\n",
    "  f''(x) \\approx \\frac{1}{12h^2} \\left[ -f(x-2h) + 16f(x-h) - 30f(x) + 16f(x+h) - f(x+2h) \\right].\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### Примеры расчета  \n",
    "#### Пример 1: Линейная функция $ f(x) = 2x + 1 $  \n",
    "- Точная производная: $ f'(x) = 2 $.  \n",
    "- Формула (1) при $ h = 0.1 $:  \n",
    "  $$\n",
    "  \\frac{f(1.1) - f(1)}{0.1} = \\frac{(2.2 + 1) - (2 + 1)}{0.1} = 2 \\quad (\\text{точный результат}).\n",
    "  $$  \n",
    "- **Почему точно?** Формула 1-го порядка точна для линейных функций.  \n",
    "\n",
    "#### Пример 2: Квадратичная функция $ f(x) = x^2 $  \n",
    "- Точная производная: $ f'(x) = 2x $. В точке $ x = 1 $: $ f'(1) = 2 $.  \n",
    "- Формула (1) при $ h = 0.1 $:  \n",
    "  $$\n",
    "  \\frac{f(1.1) - f(1)}{0.1} = \\frac{1.21 - 1}{0.1} = 2.1 \\quad (\\text{ошибка } 5\\%).\n",
    "  $$  \n",
    "- Формула (6) при $ h = 0.1 $:  \n",
    "  $$\n",
    "  \\frac{f(1.1) - f(0.9)}{0.2} = \\frac{1.21 - 0.81}{0.2} = 2 \\quad (\\text{точный результат}).\n",
    "  $$  \n",
    "- **Вывод:** Формула 2-го порядка точна для квадратичных функций.  \n",
    "\n",
    "#### Пример 3: Влияние ошибок округления  \n",
    "Для $ f(x) = e^x $ при $ x = 0 $:  \n",
    "- Точная производная: $ f'(0) = 1 $.  \n",
    "- Формула (1) при маленьком $ h = 10^{-10} $:  \n",
    "  $$\n",
    "  \\frac{e^{10^{-10}} - e^0}{10^{-10}} \\approx \\frac{(1 + 10^{-10}) - 1}{10^{-10}} = 1,\n",
    "  $$  \n",
    "  но из-за округления $ e^{10^{-10}} \\approx 1 $ (машина считает $ 1.0000000001 - 1 = 0.0000000001 $), что дает:  \n",
    "  $$\n",
    "  \\frac{0.0000000001}{10^{-10}} = 1 \\quad (\\text{ошибка округления незаметна}).\n",
    "  $$  \n",
    "- Если $ h = 10^{-15} $:  \n",
    "  $$\n",
    "  \\frac{e^{10^{-15}} - 1}{10^{-15}} \\approx \\frac{1.000000000000001 - 1}{10^{-15}} = 1,\n",
    "  $$  \n",
    "  но при вычислениях в Python:  \n",
    "  ```python\n",
    "  (math.exp(1e-15) - 1) / 1e-15  # Результат ≈ 1.1102230246251565 (ошибка 11%!)\n",
    "  ```  \n",
    "**Почему?** При $ h \\to 0 $ ошибка округления $ \\delta_r \\sim \\frac{\\varepsilon}{h} $ растет.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449d8d8",
   "metadata": {},
   "source": [
    "<h1>12 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc41ca2",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:  \n",
    "**Численное дифференцирование**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Порядок точности формул дифференцирования**  \n",
    "Порядок точности — максимальная степень многочлена, для которого формула даёт точную производную.  \n",
    "- **Формула 1-го порядка**:  \n",
    "  $$\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
    "  $$  \n",
    "  Точна для линейных функций ($f(x) = ax + b$).  \n",
    "- **Формула 2-го порядка**:  \n",
    "  $$\n",
    "  f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "  $$  \n",
    "  Точна для квадратичных функций ($f(x) = x^2$).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Способы уменьшения суммарной погрешности**  \n",
    "Суммарная погрешность $\\delta = \\delta_t + \\delta_r$:  \n",
    "- $\\delta_t$ (ошибка усечения): уменьшается при $h \\to 0$ или использовании формул высшего порядка.  \n",
    "- $\\delta_r$ (ошибка округления): уменьшается при увеличении $h$ или повышении точности вычислений.  \n",
    "**Оптимальный шаг $h$**: выбирается из условия $|\\delta_t| \\approx |\\delta_r|$, что минимизирует $\\delta$.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Вывод формулы произвольного порядка точности**  \n",
    "1. Выбрать $n+1$ узлов $x_k$ вокруг точки $x$.  \n",
    "2. Построить интерполяционный многочлен Лагранжа $P_n(x)$.  \n",
    "3. Продифференцировать $P_n(x)$:  \n",
    "   $$\n",
    "   f'(x) \\approx \\sum_{k=0}^{n} w_k f(x_k), \\quad w_k = \\frac{dL_k}{dx}(x).\n",
    "   $$  \n",
    "**Пример для 2-го порядка** (узлы $x-h, x, x+h$):  \n",
    "$$\n",
    "w_0 = -\\frac{1}{2h}, \\quad w_1 = 0, \\quad w_2 = \\frac{1}{2h}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Формулы для высших производных**  \n",
    "- **Вторая производная (2-й порядок)**:  \n",
    "  $$\n",
    "  f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}.\n",
    "  $$  \n",
    "- **Вторая производная (4-й порядок)**:  \n",
    "  $$\n",
    "  f''(x) \\approx \\frac{-f(x-2h) + 16f(x-h) - 30f(x) + 16f(x+h) - f(x+2h)}{12h^2}.\n",
    "  $$  \n",
    "\n",
    "---\n",
    "\n",
    "### Примеры  \n",
    "1. **Для $f(x) = x^2$ в точке $x=1$**:  \n",
    "   - Формула 1-го порядка ($h=0.1$):  \n",
    "     $$\n",
    "     \\frac{(1.1)^2 - 1^2}{0.1} = 2.1 \\quad (\\text{ошибка } 5\\%).\n",
    "     $$  \n",
    "   - Формула 2-го порядка ($h=0.1$):  \n",
    "     $$\n",
    "     \\frac{(1.1)^2 - (0.9)^2}{0.2} = 2 \\quad (\\text{точно}).\n",
    "     $$  \n",
    "\n",
    "2. **Оптимальный шаг $h$**:  \n",
    "   Для $f(x) = e^x$ при малых $h$ ($h < 10^{-10}$) ошибка округления доминирует. Решение: взять $h \\sim \\sqrt[3]{\\varepsilon}$ (например, $h = 10^{-5}$ для $\\varepsilon = 10^{-15}$).  \n",
    "\n",
    "---\n",
    "\n",
    "**Итог**:  \n",
    "- Порядок точности определяет скорость сходимости.  \n",
    "- Оптимальный $h$ балансирует $\\delta_t$ и $\\delta_r$.  \n",
    "- Формулы высших порядков уменьшают погрешность без уменьшения $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296a463",
   "metadata": {},
   "source": [
    "<h1>13 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3935d",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Численное интегрирование  \n",
    "**Тема:** Построение квадратурных формул, формулы Ньютона-Котеса (трапеций, Симпсона, Ньютона), веса узлов, погрешность.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Общий принцип построения квадратурных формул  \n",
    "**Цель:** Приближенно вычислить интеграл $\\int_a^b f(x)  dx$.  \n",
    "**Алгоритм:**  \n",
    "1. **Выбрать узлы** на $[a, b]$: $x_0, x_1, \\ldots, x_n$ (часто равноотстоящие).  \n",
    "2. **Построить интерполяционный многочлен $P_n(x)$** степени $n$, проходящий через значения $f(x_k)$ в узлах.  \n",
    "3. **Проинтегрировать $P_n(x)$** вместо $f(x)$:  \n",
    "   $$\n",
    "   \\int_a^b f(x)  dx \\approx \\int_a^b P_n(x)  dx = \\sum_{k=0}^n w_k f(x_k),\n",
    "   $$  \n",
    "   где **веса $w_k$** вычисляются как:  \n",
    "   $$\n",
    "   w_k = \\int_a^b L_k(x)  dx, \\quad L_k(x) = \\prod_{\\substack{i=0 \\\\ i \\neq k}}^n \\frac{x - x_i}{x_k - x_i} \\quad (\\text{базисный многочлен Лагранжа}).\n",
    "   $$  \n",
    "**Итог:** Веса зависят только от расположения узлов, а не от $f(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Формулы Ньютона-Котеса  \n",
    "**Особенности:**  \n",
    "- Узлы **равноотстоящие**, включают концы отрезка: $x_k = a + kh$, $h = \\frac{b-a}{n}$.  \n",
    "- Веса **симметричны** (для $x_k$ и $x_{n-k}$).  \n",
    "\n",
    "#### Формула трапеций ($n=1$)  \n",
    "- **Узлы:** $x_0 = a$, $x_1 = b$.  \n",
    "- **Веса:**  \n",
    "  $$\n",
    "  w_0 = w_1 = \\frac{h}{2}, \\quad h = b - a.\n",
    "  $$  \n",
    "- **Формула:**  \n",
    "  $$\n",
    "  \\int_a^b f(x)  dx \\approx \\frac{h}{2} \\left[ f(a) + f(b) \\right].\n",
    "  $$  \n",
    "- **Погрешность:** $-\\frac{h^3}{12} f''(\\xi)$, $\\xi \\in [a, b]$.\n",
    "\n",
    "#### Формула Симпсона ($n=2$)  \n",
    "- **Узлы:** $x_0 = a$, $x_1 = a + h$, $x_2 = b$, $h = \\frac{b-a}{2}$.  \n",
    "- **Веса:**  \n",
    "  $$\n",
    "  w_0 = w_2 = \\frac{h}{3}, \\quad w_1 = \\frac{4h}{3}.\n",
    "  $$  \n",
    "- **Формула:**  \n",
    "  $$\n",
    "  \\int_a^b f(x)  dx \\approx \\frac{h}{3} \\left[ f(a) + 4f(a+h) + f(b) \\right].\n",
    "  $$  \n",
    "- **Погрешность:** $-\\frac{h^5}{90} f^{(4)}(\\xi)$.\n",
    "\n",
    "#### Формула Ньютона («правило 3/8», $n=3$)  \n",
    "- **Узлы:** $x_0 = a$, $x_1 = a + h$, $x_2 = a + 2h$, $x_3 = b$, $h = \\frac{b-a}{3}$.  \n",
    "- **Веса:**  \n",
    "  $$\n",
    "  w_0 = w_3 = \\frac{3h}{8}, \\quad w_1 = w_2 = \\frac{9h}{8}.\n",
    "  $$  \n",
    "- **Формула:**  \n",
    "  $$\n",
    "  \\int_a^b f(x)  dx \\approx \\frac{3h}{8} \\left[ f(a) + 3f(a+h) + 3f(a+2h) + f(b) \\right].\n",
    "  $$  \n",
    "- **Погрешность:** $-\\frac{3h^5}{80} f^{(4)}(\\xi)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Порядок точности  \n",
    "**Определение:** Максимальная степень многочлена, интегрируемого **точно**.  \n",
    "- **Трапеции:** точна для линейных функций (порядок 1).  \n",
    "- **Симпсона:** точна для кубических функций (порядок 3).  \n",
    "- **Ньютона ($n=3$):** точна для кубических функций (порядок 3).  \n",
    "\n",
    "**Важно!**  \n",
    "Формулы с нечётным $n$ имеют тот же порядок точности, что и формулы с $n+1$ узлами. Например, трапеции ($n=1$) и Симпсон ($n=2$) оба имеют порядок 1 для линейных функций, но Симпсон точнее за счёт более высокого порядка погрешности.\n",
    "\n",
    "---\n",
    "\n",
    "### Примеры расчета  \n",
    "#### Пример 1: Формула трапеций для $f(x) = x^2$ на $[0, 1]$  \n",
    "- Точное значение: $\\int_0^1 x^2  dx = \\frac{1}{3} \\approx 0.3333$.  \n",
    "- Шаг $h = 1$:  \n",
    "  $$\n",
    "  \\frac{1}{2} \\left[ f(0) + f(1) \\right] = \\frac{1}{2} \\left[ 0 + 1 \\right] = 0.5.\n",
    "  $$  \n",
    "- Погрешность: $|0.5 - 0.3333| = 0.1667$.  \n",
    "**Объяснение:** Формула трапеций заменяет кривую $x^2$ на прямую, соединяющую $(0,0)$ и $(1,1)$, что даёт завышенный результат.\n",
    "\n",
    "---\n",
    "\n",
    "#### Пример 2: Формула Симпсона для $f(x) = x^3$ на $[0, 2]$  \n",
    "- Точное значение: $\\int_0^2 x^3  dx = 4$.  \n",
    "- Узлы: $x_0=0$, $x_1=1$, $x_2=2$, $h=1$.  \n",
    "- Расчет:  \n",
    "  $$\n",
    "  \\frac{1}{3} \\left[ f(0) + 4f(1) + f(2) \\right] = \\frac{1}{3} \\left[ 0 + 4 \\cdot 1 + 8 \\right] = \\frac{12}{3} = 4.\n",
    "  $$  \n",
    "**Итог:** Точное значение, так как формула Симпсона интегрирует кубические функции точно.\n",
    "\n",
    "---\n",
    "\n",
    "#### Пример 3: Формула Ньютона ($n=3$) для $f(x) = \\sin x$ на $[0, \\pi]$  \n",
    "- Точное значение: $\\int_0^\\pi \\sin x  dx = 2$.  \n",
    "- Шаг $h = \\pi/3$:  \n",
    "  Узлы: $x_0=0$, $x_1=\\pi/3$, $x_2=2\\pi/3$, $x_3=\\pi$.  \n",
    "- Расчет:  \n",
    "  $$\n",
    "  \\frac{3h}{8} \\left[ \\sin 0 + 3 \\sin(\\pi/3) + 3 \\sin(2\\pi/3) + \\sin \\pi \\right] = \\frac{3 \\cdot \\pi/3}{8} \\left[ 0 + 3 \\cdot \\frac{\\sqrt{3}}{2} + 3 \\cdot \\frac{\\sqrt{3}}{2} + 0 \\right] = \\frac{\\pi}{8} \\cdot 3\\sqrt{3} \\approx 2.0405.\n",
    "  $$  \n",
    "- Погрешность: $|2.0405 - 2| = 0.0405$.  \n",
    "**Вывод:** Для гладких функций увеличение числа узлов уменьшает погрешность.\n",
    "\n",
    "---\n",
    "\n",
    "### Почему веса такие?  \n",
    "- **Трапеции:** Среднее значений на концах (площадь трапеции).  \n",
    "- **Симпсона:** Учет «веса» средней точки для аппроксимации параболы.  \n",
    "- **Ньютона ($n=3$):** Балансировка вкладов точек для кубической интерполяции.  \n",
    "\n",
    "**Ключевая идея:** Чем выше степень многочлена, используемого в формуле, тем точнее результат для гладких функций, но тем сложнее вычисления. Для больших $n$ ($>9$) формулы Ньютона-Котеса становятся неустойчивыми из-за отрицательных весов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d471fef",
   "metadata": {},
   "source": [
    "<h1>13 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9410cf5",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет: Численное интегрирование  \n",
    "**(Краткая шпаргалка для устного ответа)**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Общий принцип квадратурных формул**  \n",
    "- **Цель:** Приближённое вычисление $\\int_a^b f(x)  dx$.  \n",
    "- **Алгоритм:**  \n",
    "  1. Выбрать узлы $x_0 < x_1 < \\cdots < x_n$ на $[a, b]$.  \n",
    "  2. Построить интерполяционный многочлен $P_n(x)$ степени $n$ через $f(x_k)$.  \n",
    "  3. Интегрировать $P_n(x)$:  \n",
    "  $$\n",
    "  \\int_a^b f(x)  dx \\approx \\sum_{k=0}^n w_k f(x_k), \\quad w_k = \\int_a^b L_k(x)  dx,\n",
    "  $$  \n",
    "  где $L_k(x)$ — базис Лагранжа.  \n",
    "- **Ключевое:** Веса $w_k$ зависят только от узлов, не от $f(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Формулы Ньютона-Котеса**  \n",
    "**(Равноотстоящие узлы, включая концы отрезка: $x_k = a + kh$, $h = \\frac{b-a}{n}$)**  \n",
    "\n",
    "| Формула       | Узлы                           | Веса                                    | Формула вычисления                                  | Погрешность                     |\n",
    "|---------------|--------------------------------|-----------------------------------------|-----------------------------------------------------|----------------------------------|\n",
    "| **Трапеций**  | $a, b$                       | $w_0 = w_1 = \\frac{h}{2}$             | $\\frac{h}{2} \\left[ f(a) + f(b) \\right]$          | $-\\frac{h^3}{12} f''(\\xi)$     |\n",
    "| ($n=1$)     |                                |                                         |                                                     |                                  |\n",
    "| **Симпсона**  | $a, a+h, b$                  | $w_0 = w_2 = \\frac{h}{3}$, $w_1 = \\frac{4h}{3}$ | $\\frac{h}{3} \\left[ f(a) + 4f(a+h) + f(b) \\right]$ | $-\\frac{h^5}{90} f^{(4)}(\\xi)$ |\n",
    "| ($n=2$)     |                                |                                         |                                                     |                                  |\n",
    "| **Ньютона**   | $a, a+h, a+2h, b$            | $w_0 = w_3 = \\frac{3h}{8}$, $w_1 = w_2 = \\frac{9h}{8}$ | $\\frac{3h}{8} \\left[ f(a) + 3f(a+h) + 3f(a+2h) + f(b) \\right]$ | $-\\frac{3h^5}{80} f^{(4)}(\\xi)$ |\n",
    "| ($n=3$)     |                                |                                         |                                                     |                                  |\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Порядок точности**  \n",
    "- Максимальная степень многочлена, интегрируемого **точно**:  \n",
    "  - Трапеций: 1 (линейные функции).  \n",
    "  - Симпсона: 3 (кубические функции).  \n",
    "  - Ньютона ($n=3$): 3 (кубические функции).  \n",
    "- **Важно!** Формулы с нечётным $n$ имеют тот же порядок, что формулы с $n+1$ узлами.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Примеры для иллюстрации**  \n",
    "1. **Трапеций для $f(x) = x^2$ на $[0, 1]$**:  \n",
    "   - Точное значение: $\\frac{1}{3} \\approx 0.3333$.  \n",
    "   - Расчёт: $\\frac{1}{2} \\left[ 0 + 1 \\right] = 0.5$.  \n",
    "   - Погрешность: $0.1667$ (замена кривой прямой даёт завышение).  \n",
    "\n",
    "2. **Симпсона для $f(x) = x^3$ на $[0, 2]$**:  \n",
    "   - Точное значение: $4$.  \n",
    "   - Расчёт: $\\frac{1}{3} \\left[ 0 + 4 \\cdot 1 + 8 \\right] = 4$ (кубики интегрируются точно).  \n",
    "\n",
    "3. **Ньютона ($n=3$) для $f(x) = \\sin x$ на $[0, \\pi]$**:  \n",
    "   - Точное значение: $2$.  \n",
    "   - Расчёт: $\\frac{\\pi}{8} \\cdot 3\\sqrt{3} \\approx 2.0405$.  \n",
    "   - Погрешность: $0.0405$ (увеличение узлов повышает точность).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Выводы**  \n",
    "- **Простота vs. Точность:** Трапеции — грубее, Симпсон — оптимален для гладких функций.  \n",
    "- **Ограничения:** Формулы Ньютона-Котеса с $n > 9$ неустойчивы (отрицательные веса).  \n",
    "- **Применение:** Для быстроосциллирующих функций уменьшайте шаг $h$ или используйте составные формулы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e017b1",
   "metadata": {},
   "source": [
    "<h1>14 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c695b",
   "metadata": {},
   "source": [
    "### Подробный разбор экзаменационного билета\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Формулы наивысшего порядка точности (Гаусса)\n",
    "**Суть:**  \n",
    "Формулы Гаусса достигают максимально возможного порядка точности **2n+1** для n+1 узлов (в отличие от формул Ньютона-Котеса, где порядок ≤ n+1). Узлы выбираются как корни многочленов, ортогональных на отрезке интегрирования.\n",
    "\n",
    "**Как это работает:**\n",
    "- Узлы не равноотстоящие, а расположены в корнях ортогональных многочленов (например, Лежандра для отрезка [-1, 1]).\n",
    "- Веса рассчитываются так, чтобы формула была точна для многочленов степени до 2n+1.\n",
    "- **Преимущество:** Для гладких функций точность выше, чем у Ньютона-Котеса при том же числе узлов.\n",
    "\n",
    "**Пример:**  \n",
    "Вычислим интеграл ∫₁⁻¹ x⁴ dx точным значением 2/5 = 0.4.  \n",
    "- **Формула Гаусса с 2 узлами** (корни многочлена Лежандра P₂(x): ±1/√3 ≈ ±0.577):  \n",
    "  Веса: w₁ = w₂ = 1.  \n",
    "  Приближение: (0.577)⁴ + (-0.577)⁴ ≈ 0.111 + 0.111 = 0.222 ❌ (погрешность 45%).  \n",
    "- **Формула Гаусса с 3 узлами** (корни P₃(x): 0, ±√(3/5) ≈ ±0.774):  \n",
    "  Веса: w₁ = 8/9 ≈ 0.889, w₂ = w₃ = 5/9 ≈ 0.556.  \n",
    "  Приближение:  \n",
    "  0.889·(0)⁴ + 0.556·(0.774)⁴ + 0.556·(-0.774)⁴ ≈ 0 + 0.556·0.359 + 0.556·0.359 ≈ 0.4 ✅ (точное значение).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Интегрирование с весовой функцией\n",
    "**Суть:**  \n",
    "Если подынтегральная функция имеет вид f(x) = g(x)·p(x), где p(x) — \"вес\" (например, p(x) = e⁻ˣ, √(1-x²)), то строят специализированные квадратурные формулы, точно учитывающие p(x).\n",
    "\n",
    "**Как это работает:**\n",
    "- p(x) выделяют в весовую функцию.\n",
    "- Узлы выбирают как корни многочленов, ортогональных с весом p(x) (например, Чебышева для p(x)=1/√(1-x²)).\n",
    "- Ошибка зависит только от аппроксимации g(x) многочленом.\n",
    "\n",
    "**Пример:**  \n",
    "Интеграл ∫₁⁻¹ √(1-x²) dx (площадь полукруга, точное значение π/2 ≈ 1.571).  \n",
    "- **Обычная формула Гаусса (без веса):**  \n",
    "  При 5 узлах погрешность ~10⁻² (требует много узлов из-за особенностей на концах).  \n",
    "- **Формула Гаусса-Чебышева 2-го рода** (вес p(x)=√(1-x²), узлы — корни Uₙ(x)):  \n",
    "  Для n=2: узлы ±√(2)/2 ≈ ±0.707, веса w₁=w₂=π/4 ≈ 0.785.  \n",
    "  Приближение: g(x)=1 ⇒ ∑wₖ·g(xₖ) = 0.785·1 + 0.785·1 = 1.57 ✅ (погрешность < 0.01%).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Интегрирование с контролем точности (алгоритм Рунге)\n",
    "**Суть:**  \n",
    "Автоматическое адаптивное вычисление интеграла с оценкой погрешности. Отрезок дробится, пока разность между двумя приближениями не станет меньше заданной точности ε.\n",
    "\n",
    "**Алгоритм для формулы Симпсона:**\n",
    "1. **Инициализация:**  \n",
    "   - Разбить [a, b] на 1 интервал (шаг h = b-a).  \n",
    "   - Вычислить S₁ = f(a) + f(b).  \n",
    "2. **Итерация:**  \n",
    "   - Добавить узлы в серединах новых подынтервалов.  \n",
    "   - Вычислить S₂ (значения в новых узлах).  \n",
    "   - Обновить S₃ = S₃ + S₂ (старые внутренние узлы).  \n",
    "   - Новое приближение: I = (h/3)·(S₁ + 4S₂ + 2S₃).  \n",
    "3. **Оценка погрешности:**  \n",
    "   - ΔI = |Iₜₕᵢₛ − Iₚᵣₑᵥ| ≈ |R| (по формуле Рунге).  \n",
    "   - Если ΔI < ε → остановка, иначе h = h/2, повтор шага 2.\n",
    "\n",
    "**Пример:**  \n",
    "∫₀¹ e⁻ˣ² dx (точное значение ≈ 0.7468).  \n",
    "- **Шаг 1:** h=1, S₁ = f(0)+f(1)=1+0.3679=1.3679 → I₁ = (1/3)·1.3679 ≈ 0.456 (погрешность 39%).  \n",
    "- **Шаг 2:** h=0.5, добавить узлы x=0.5, x=1.5 (не входит в [0,1]).  \n",
    "  S₂ = f(0.5)=0.7788 → I₂ = (0.5/3)·[1.3679 + 4·0.7788] ≈ 0.708 (ΔI=|0.708-0.456|=0.252 > ε).  \n",
    "- **Шаг 3:** h=0.25, добавить узлы x=0.25, x=0.75.  \n",
    "  S₂ = f(0.25)+f(0.75)≈0.9394+0.5698=1.5092 → I₃=0.746 (ΔI=|0.746-0.708|=0.038 < ε для ε=0.05).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Вычисление несобственных интегралов\n",
    "**Подходы:**  \n",
    "1. **Формулы Гаусса для бесконечных интервалов:**  \n",
    "   - Лагерра: для [0, ∞) с весом e⁻ˣ.  \n",
    "   - Эрмита: для (-∞, ∞) с весом e⁻ˣ².  \n",
    "2. **Замена переменных:** Например, x = t/(1-t) для [a, ∞) → [0, 1].\n",
    "\n",
    "**Пример:**  \n",
    "∫₀^∞ e⁻ˣ·sin(x) dx (точное значение = 0.5).  \n",
    "- **Формула Гаусса-Лагерра** (n=3):  \n",
    "  Узлы: 0.4158, 1.2943, 3.6353; веса: 0.711, 0.278, 0.011.  \n",
    "  Приближение: ∑wₖ·sin(xₖ) ≈ 0.711·0.401 + 0.278·0.963 + 0.011·(-0.475) ≈ 0.5 ✅.\n",
    "\n",
    "---\n",
    "\n",
    "### Итог\n",
    "- **Формулы Гаусса** дают высокую точность для гладких функций на конечных интервалах.  \n",
    "- **Весовые функции** устраняют особенности (например, бесконечные производные на концах).  \n",
    "- **Алгоритм Рунге** автоматически адаптирует шаг под требуемую точность.  \n",
    "- **Несобственные интегралы** вычисляются через специализированные квадратуры или замены переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82696b",
   "metadata": {},
   "source": [
    "<h1>14 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855e2a1",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Формулы наивысшего порядка точности (Гаусса)\n",
    "- **Суть:** Максимальный порядок точности **2n+1** для n+1 узлов достигается выбором узлов в корнях ортогональных многочленов (например, Лежандра для [-1, 1]).  \n",
    "- **Преимущество:** Точнее формул Ньютона-Котеса при том же числе узлов.  \n",
    "- **Пример:**  \n",
    "  ∫₁⁻¹ x⁴ dx ≈ 0.4. Формула Гаусса с 3 узлами (корни P₃(x): 0, ±√(3/5)) даёт точное значение.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Интегрирование с весовой функцией\n",
    "- **Суть:** Для f(x) = g(x)·p(x) (особенно при особенностях) строят формулы с узлами — корнями многочленов, ортогональных с весом p(x).  \n",
    "- **Метод:** p(x) включается в веса, ошибка зависит только от g(x).  \n",
    "- **Пример:**  \n",
    "  ∫₁⁻¹ √(1-x²) dx ≈ 1.571. Формула Гаусса-Чебышёва 2-го рода (вес p(x)=√(1-x²)) при 2 узлах даёт 1.57.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Алгоритм интегрирования с контролем точности (Рунге)\n",
    "- **Шаги:**  \n",
    "  1. Инициализация: S₁ = f(a) + f(b), h = b-a.  \n",
    "  2. Цикл:  \n",
    "     - Добавить узлы в серединах новых подынтервалов → вычислить S₂.  \n",
    "     - Обновить S₃ = S₃ + S₂.  \n",
    "     - I = (h/3)·(S₁ + 4S₂ + 2S₃).  \n",
    "     - Если |I - I_пред| < ε → выход, иначе h = h/2.  \n",
    "- **Пример:**  \n",
    "  ∫₀¹ e⁻ˣ² dx ≈ 0.7468. При ε=0.05: после 3 итераций I=0.746.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Вычисление несобственных интегралов\n",
    "- **Методы:**  \n",
    "  - Гаусса-Лагерра: для [0, ∞) с весом e⁻ˣ.  \n",
    "  - Гаусса-Эрмита: для (-∞, ∞) с весом e⁻ˣ².  \n",
    "  - Замена переменных (например, x = t/(1-t) для [a, ∞)).  \n",
    "- **Пример:**  \n",
    "  ∫₀^∞ e⁻ˣ·sin(x) dx = 0.5. Формула Гаусса-Лагерра (n=3) даёт точное значение.\n",
    "\n",
    "---\n",
    "\n",
    "### Ключевые выводы\n",
    "- **Гаусс:** Ортогональные многочлены → высокая точность.  \n",
    "- **Весовая функция:** Учёт особенностей через p(x).  \n",
    "- **Рунге:** Адаптивное разбиение отрезка по оценке погрешности.  \n",
    "- **Несобственные интегралы:** Специализированные квадратуры или замена переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7db824",
   "metadata": {},
   "source": [
    "<h1>15 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757c08d",
   "metadata": {},
   "source": [
    "### Подробный разбор экзаменационного билета  \n",
    "**Тема:** Численные методы решения задачи Коши для обыкновенных дифференциальных уравнений (ОДУ), анализ ошибок и устойчивости, сравнение явных и неявных методов.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Задача Коши**  \n",
    "**Определение:**  \n",
    "Задача Коши требует найти решение ОДУ $ y' = F(x, y) $ с начальным условием $ y(x_0) = y_0 $. Численное решение строится в точках $ x_0, x_1, \\ldots, x_m $ с шагом $ h $.  \n",
    "\n",
    "**Пример:**  \n",
    "Уравнение радиоактивного распада:  \n",
    "$$ \\frac{dA}{dt} = -kA, \\quad A(0) = A_0. $$  \n",
    "Аналитическое решение: $ A(t) = A_0 e^{-kt} $.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Метод Эйлера (явный)**  \n",
    "**Формула:**  \n",
    "$$ y_{n+1} = y_n + h \\cdot F(x_n, y_n) $$  \n",
    "**Суть:**  \n",
    "Аппроксимация решения прямой линией с наклоном касательной в начальной точке шага.  \n",
    "**Недостаток:** Низкая точность (погрешность $ O(h^2) $ на шаге).  \n",
    "\n",
    "**Пример:**  \n",
    "Для уравнения $ y' = -y $, $ y(0) = 1 $, $ h = 0.1 $:  \n",
    "- Точное решение: $ y(1) = e^{-1} \\approx 0.3679 $.  \n",
    "- Численное решение:  \n",
    "  $$ y_1 = 1 + 0.1 \\cdot (-1) = 0.9, \\quad y_2 = 0.9 + 0.1 \\cdot (-0.9) = 0.81, \\ldots, \\quad y_{10} \\approx 0.3487. $$  \n",
    "Ошибка накопления: $ |0.3487 - 0.3679| \\approx 0.0192 $.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Локальная и глобальная ошибки**  \n",
    "- **Локальная ошибка:** Отклонение на одном шаге, если предыдущая точка точна. Для метода Эйлера: $ O(h^2) $.  \n",
    "- **Глобальная ошибка:** Суммарное отклонение после $ n $ шагов. Для метода Эйлера: $ O(h) $.  \n",
    "\n",
    "**Анализ:**  \n",
    "Глобальная ошибка зависит от поведения решения:  \n",
    "- Если $ \\lambda = \\frac{\\partial F}{\\partial y} < 0 $ (решение убывает), ошибка может затухать.  \n",
    "- Если $ \\lambda > 0 $ (решение растет), ошибка растет экспоненциально.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Устойчивость**  \n",
    "**Определение:**  \n",
    "Решение **устойчиво**, если малые ошибки не накапливаются лавинообразно.  \n",
    "\n",
    "**Критерий для явного метода Эйлера:**  \n",
    "- При $ \\lambda < 0 $: устойчиво, если $ h < \\frac{2}{|\\lambda|} $.  \n",
    "- При $ \\lambda > 0 $: неустойчиво всегда.  \n",
    "\n",
    "**Пример:**  \n",
    "Для $ y' = -1000y $ (быстрое убывание):  \n",
    "- Максимальный шаг для устойчивости: $ h < \\frac{2}{1000} = 0.002 $.  \n",
    "- При $ h = 0.003 $: решение \"раскачивается\", значения колеблются и растут.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Неявный метод Эйлера**  \n",
    "**Формула:**  \n",
    "$$ y_{n+1} = y_n + h \\cdot F(x_{n+1}, y_{n+1}) $$  \n",
    "**Суть:**  \n",
    "Касательная берется в **конечной** точке шага. Требует решения уравнения (итерационно).  \n",
    "\n",
    "**Преимущества:**  \n",
    "- **A-устойчивость** при $ \\lambda < 0 $: устойчив для любого $ h $.  \n",
    "- Лучшая точность для жестких систем.  \n",
    "\n",
    "**Пример:**  \n",
    "Для $ y' = -1000y $, $ y(0) = 1 $:  \n",
    "- При $ h = 0.1 $:  \n",
    "  $$ y_{n+1} = \\frac{y_n}{1 + 1000 \\cdot 0.1} = \\frac{y_n}{101} \\implies y_1 \\approx 0.0099, \\quad y_2 \\approx 9.8 \\cdot 10^{-5}. $$  \n",
    "Решение устойчиво даже при большом шаге.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Жесткие системы ОДУ**  \n",
    "**Определение:**  \n",
    "Системы, где компоненты решения меняются с резко разными скоростями (например, $ |\\lambda_{\\text{max}}| \\gg |\\lambda_{\\text{min}}| $).  \n",
    "\n",
    "**Проблема явных методов:**  \n",
    "Шаг ограничен самой быстрой компонентой, что ведет к огромному числу шагов.  \n",
    "\n",
    "**Пример из химии:**  \n",
    "Система кинетических уравнений радиолиза метанола:  \n",
    "- Константы скоростей: от $ 10^3 $ до $ 10^{11} $ с⁻¹.  \n",
    "- Для метода Рунге-Кутты (явный): шаг $ h < 10^{-9} $ с.  \n",
    "- Интегрирование до $ t = 100 $ с потребовало бы $ 10^{11} $ шагов и ≈20 лет расчета.  \n",
    "- **Метод Гира** (неявный, A-устойчивый): решил задачу за 806 шагов за 15 секунд.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Сравнение явных и неявных методов**  \n",
    "| **Критерий**       | **Явный метод**       | **Неявный метод**     |  \n",
    "|---------------------|------------------------|------------------------|  \n",
    "| **Формула**         | $ y_{n+1} = y_n + h F(x_n, y_n) $ | $ y_{n+1} = y_n + h F(x_{n+1}, y_{n+1}) $ |  \n",
    "| **Устойчивость**    | Требует малого $ h $ | A-устойчив при $ \\lambda < 0 $ |  \n",
    "| **Сложность**       | Прост в реализации    | Требует решения уравнения |  \n",
    "| **Применение**      | Нежесткие системы     | Жесткие системы (химическая кинетика) |  \n",
    "\n",
    "---\n",
    "\n",
    "### Резюме  \n",
    "- **Локальная ошибка** — погрешность на одном шаге, **глобальная** — накопленная.  \n",
    "- **Устойчивость** критична для жестких систем: неявные методы (например, Эйлера, Гира) позволяют брать большие шаги.  \n",
    "- **Жесткие системы** требуют A-устойчивых методов: в противном случае шаг ограничен быстро меняющимися компонентами, что делает расчеты нереализуемыми.  \n",
    "\n",
    "Для успешной сдачи экзамена важно понимать, как выбор метода влияет на устойчивость и точность решения, особенно при моделировании реальных физико-химических процессов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2702d1",
   "metadata": {},
   "source": [
    "<h1>15 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bdf10",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:  \n",
    "**1. Задача Коши:**  \n",
    "Решение ОДУ $ y' = F(x, y) $ с начальным условием $ y(x_0) = y_0 $. Численный ответ — таблица значений $ y_i $ в точках $ x_i $.  \n",
    "\n",
    "**2. Метод Эйлера (явный):**  \n",
    "$$ y_{n+1} = y_n + h \\cdot F(x_n, y_n) $$  \n",
    "- **Порядок точности:** 1 (локальная ошибка $ O(h^2) $).  \n",
    "- **Недостатки:** Низкая точность, ограниченная устойчивость.  \n",
    "\n",
    "**3. Ошибки:**  \n",
    "- **Локальная:** Погрешность на одном шаге ($ O(h^2) $ для Эйлера).  \n",
    "- **Глобальная:** Накопленное отклонение после $ n $ шагов ($ O(h) $ для Эйлера).  \n",
    "\n",
    "**4. Устойчивость:**  \n",
    "- **Явный метод Эйлера:**  \n",
    "  - Устойчив при $ \\lambda = \\frac{\\partial F}{\\partial y} < 0 $ и $ h < \\frac{2}{|\\lambda|} $.  \n",
    "  - Неустойчив при $ \\lambda > 0 $ или больших $ h $.  \n",
    "- **Неявный метод Эйлера:**  \n",
    "  $$ y_{n+1} = y_n + h \\cdot F(x_{n+1}, y_{n+1}) $$  \n",
    "  - **A-устойчивость** при $ \\lambda < 0 $ (устойчив для любого $ h $!).  \n",
    "\n",
    "**5. Жесткие системы:**  \n",
    "- Компоненты решения меняются с резко разными скоростями ($ |\\lambda_{\\text{max}}| \\gg |\\lambda_{\\text{min}}| $).  \n",
    "- **Проблема явных методов:** Шаг $ h $ ограничен быстрой компонентой → нереализуемо много шагов.  \n",
    "- **Решение:** Неявные методы (Гира, A-устойчивые).  \n",
    "\n",
    "**6. Сравнение методов:**  \n",
    "| **Критерий**   | **Явный метод**       | **Неявный метод**     |  \n",
    "|----------------|------------------------|------------------------|  \n",
    "| **Устойчивость** | Ограничена $ h $      | A-устойчивость         |  \n",
    "| **Точность**   | Низкая (1-й порядок)   | Выше (1-й порядок)      |  \n",
    "| **Применение** | Нежесткие системы      | Жесткие системы        |  \n",
    "\n",
    "---\n",
    "\n",
    "### Выводы:  \n",
    "1. Для **нежестких систем** подходят явные методы (Рунге-Кутты 4-го порядка).  \n",
    "2. Для **жестких систем** (химкинетика) обязательны **неявные A-устойчивые методы** (Эйлера неявный, Гира).  \n",
    "3. **Устойчивость > Точность:** Если решение \"раскачивается\", уменьшите $ h $ или выберите неявный метод."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1624aef",
   "metadata": {},
   "source": [
    "<h1>16 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab5c7c",
   "metadata": {},
   "source": [
    "### Подробный разбор экзаменационного билета:  \n",
    "**Тема:** Численные методы решения обыкновенных дифференциальных уравнений (ОДУ).  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Порядок решения ОДУ**  \n",
    "**Суть:** Решение задачи Коши $ y' = F(x, y) $, $ y(x_0) = y_0 $ выполняется последовательно, шаг за шагом, от начальной точки $ x_0 $ до конечной $ x_m $.  \n",
    "**Этапы:**  \n",
    "1. **Задание шага $ h $:**  \n",
    "   - Фиксированный шаг: $ h = \\text{const} $ (просто, но может быть неэффективно).  \n",
    "   - Переменный шаг: $ h $ адаптируется под точность (сложнее, но экономичнее).  \n",
    "2. **Выбор метода:** Определяет, как вычислять $ y_{n+1} $ по известным $ y_n $.  \n",
    "3. **Итерации:** На каждом шаге применяется выбранная схема.  \n",
    "4. **Контроль ошибки:** Оценка погрешности и корректировка $ h $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Модифицированный метод Эйлера**  \n",
    "**Суть:** Улучшенная версия явного метода Эйлера с повышением точности до 2-го порядка.  \n",
    "**Алгоритм (два этапа):**  \n",
    "1. **Прогноз (явный шаг):**  \n",
    "   $$\n",
    "   \\hat{y}_{n+1} = y_n + h \\cdot F(x_n, y_n).\n",
    "   $$  \n",
    "2. **Коррекция (усреднение производных):**  \n",
    "   $$\n",
    "   y_{n+1} = y_n + h \\cdot \\frac{F(x_n, y_n) + F(x_{n+1}, \\hat{y}_{n+1})}{2}.\n",
    "   $$  \n",
    "**Почему лучше?**  \n",
    "- Локальная ошибка: $ O(h^3) $ (против $ O(h^2) $ у стандартного Эйлера).  \n",
    "- Геометрическая интерпретация: использует средний наклон касательных в точках $ x_n $ и $ x_{n+1} $.  \n",
    "\n",
    "**Пример:**  \n",
    "Решим $ y' = -y $, $ y(0) = 1 $ на $[0, 1]$ с $ h = 0.5 $:  \n",
    "- **Шаг 1 ($ x_0 = 0 $):**  \n",
    "  Прогноз: $ \\hat{y}_1 = 1 + 0.5 \\cdot (-1) = 0.5 $.  \n",
    "  Коррекция: $ y_1 = 1 + 0.5 \\cdot \\frac{-1 + (-0.5)}{2} = 1 + 0.5 \\cdot (-0.75) = 0.625 $.  \n",
    "- **Шаг 2 ($ x_1 = 0.5 $):**  \n",
    "  Прогноз: $ \\hat{y}_2 = 0.625 + 0.5 \\cdot (-0.625) = 0.3125 $.  \n",
    "  Коррекция: $ y_2 = 0.625 + 0.5 \\cdot \\frac{-0.625 + (-0.3125)}{2} = 0.625 - 0.234375 = 0.390625 $.  \n",
    "Точное решение: $ y = e^{-1} \\approx 0.367 $. Погрешность: $ |0.391 - 0.367| \\approx 0.024 $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Метод Рунге-Кутты 4-го порядка**  \n",
    "**Суть:** Вычисляет производные в 4 вспомогательных точках для повышения точности до $ O(h^5) $.  \n",
    "**Формулы:**  \n",
    "$$\n",
    "\\begin{align*}\n",
    "k_1 &= F(x_n, y_n), \\\\\n",
    "k_2 &= F\\left(x_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right), \\\\\n",
    "k_3 &= F\\left(x_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_2\\right), \\\\\n",
    "k_4 &= F(x_n + h, y_n + h k_3), \\\\\n",
    "y_{n+1} &= y_n + \\frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4).\n",
    "\\end{align*}\n",
    "$$  \n",
    "**Преимущества:**  \n",
    "- Высокая точность (погрешность шага $ \\sim h^5 $).  \n",
    "- Универсальность (не требует вычисления старших производных).  \n",
    "\n",
    "**Пример:**  \n",
    "Для $ y' = -y $, $ y(0) = 1 $, $ h = 1 $:  \n",
    "- $ k_1 = -1 $,  \n",
    "- $ k_2 = F(0.5, 1 + 0.5 \\cdot (-1)) = F(0.5, 0.5) = -0.5 $,  \n",
    "- $ k_3 = F(0.5, 1 + 0.5 \\cdot (-0.5)) = F(0.5, 0.75) = -0.75 $,  \n",
    "- $ k_4 = F(1, 1 + 1 \\cdot (-0.75)) = F(1, 0.25) = -0.25 $,  \n",
    "- $ y_1 = 1 + \\frac{1}{6}(-1 + 2 \\cdot (-0.5) + 2 \\cdot (-0.75) + (-0.25)) = 1 - 0.75 = 0.25 $.  \n",
    "Точное решение: $ e^{-1} \\approx 0.367 $. Погрешность: $ |0.25 - 0.367| = 0.117 $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Многошаговые методы (Методы прогноза-коррекции)**  \n",
    "**Суть:** Используют значения $ y $ и $ y' $ в нескольких предыдущих точках.  \n",
    "**Примеры:**  \n",
    "- **Метод Адамса-Башфорта (4-й порядок):**  \n",
    "  **Прогноз (явный):**  \n",
    "  $$\n",
    "  y_{n+1} = y_n + \\frac{h}{24} (55y_n' - 59y_{n-1}' + 37y_{n-2}' - 9y_{n-3}').\n",
    "  $$  \n",
    "  **Коррекция (неявный):**  \n",
    "  $$\n",
    "  y_{n+1} = y_n + \\frac{h}{24} (9y_{n+1}' + 19y_n' - 5y_{n-1}' + y_{n-2}').\n",
    "  $$  \n",
    "- **Особенности:**  \n",
    "  - Требуют стартовых точек (вычисленных, например, методом Рунге-Кутты).  \n",
    "  - Экономичны (меньше вычислений $ F(x, y) $ на шаг).  \n",
    "  - Локальная ошибка коррекции в 20–30 раз меньше, чем у прогноза.  \n",
    "\n",
    "**Пример (упрощенный):**  \n",
    "Для системы:  \n",
    "$$\n",
    "\\begin{cases}\n",
    "y_1' = -0.7y_1 + 0.25y_2 \\\\\n",
    "y_2' = 0.25y_1 - 1.9y_2\n",
    "\\end{cases}\n",
    "$$  \n",
    "с начальными условиями $ y_1(0) = 1.2 $, $ y_2(0) = 0.8 $.  \n",
    "- Первые 4 точки вычисляем методом Рунге-Кутты.  \n",
    "- Далее используем метод Адамса-Башфорта.  \n",
    "Решение покажет два масштаба времени: быстрый спад $ y_2 $ и медленное изменение $ y_1 $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Контроль точности и выбор шага**  \n",
    "**Стратегии:**  \n",
    "1. **Оценка локальной ошибки:**  \n",
    "   - Сравнение прогноза и коррекции (в методах Адамса/Милна).  \n",
    "   - Разница решений для разных $ h $ (метод Рунге).  \n",
    "2. **Адаптивный шаг:**  \n",
    "   - Если ошибка $ \\delta > \\delta_{\\text{доп}} $, уменьшают $ h $ (например, в 2 раза).  \n",
    "   - Если $ \\delta < \\delta_{\\text{доп}}/10 $, увеличивают $ h $.  \n",
    "**Пример:** В Python-библиотеке `scipy.integrate.solve_ivp` используется автоматический выбор шага на основе оценки погрешности.  \n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Алгоритмы с фиксированным шагом**  \n",
    "**Плюсы:** Простота реализации.  \n",
    "**Минусы:** Риск неустойчивости или избыточных вычислений.  \n",
    "**Пример:** Метод Эйлера для $ y' = -1000y $, $ y(0) = 1 $:  \n",
    "- Требует $ h < 0.002 $ для устойчивости, хотя решение $ y = e^{-1000x} $ быстро стремится к 0.  \n",
    "- При $ h = 0.001 $ нужно 1000 шагов для $ x \\in [0, 1] $, при этом на большей части интервала $ y \\approx 0 $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Решение жестких систем**  \n",
    "**Проблема:** В системах вида $ \\mathbf{y}' = A\\mathbf{y} $ (например, химическая кинетика) собственные значения $ \\lambda_i $ матрицы $ A $ сильно различаются (например, $ \\lambda_1 = -1 $, $ \\lambda_2 = -1000 $).  \n",
    "**Чем опасно:**  \n",
    "- Явные методы требуют $ h < 2 / |\\lambda_{\\max}| $ (здесь $ h < 0.002 $), но интерес представляет интервал, где доминирует медленная компонента ($ e^{-x} $).  \n",
    "**Решение:**  \n",
    "- **Неявные методы** (например, неявный Эйлер, метод Гира).  \n",
    "- **Метод Гира (Gear):**  \n",
    "  - Неявный, переменный порядок (1–6), автоматический выбор шага.  \n",
    "  - Жестко устойчив.  \n",
    "**Пример из лекции:** Модель радиолиза метанола (37 реакций).  \n",
    "- Метод Рунге-Кутты: шаг $ h = 10^{-9} $ с, время расчета до $ t = 4 \\cdot 10^{-5} $ с — 5 минут.  \n",
    "- Метод Гира: до $ t = 100 $ с — 806 шагов за 15 секунд.  \n",
    "\n",
    "---\n",
    "\n",
    "### Итоговая таблица сравнения методов  \n",
    "| **Метод**               | **Порядок** | **Тип**       | **Устойчивость**       | **Применение**         |  \n",
    "|--------------------------|-------------|---------------|------------------------|------------------------|  \n",
    "| Явный Эйлер             | 1           | Явный         | Условная ($ h < 2/\\|\\lambda\\| $) | Простые ОДУ          |  \n",
    "| Модифицированный Эйлер  | 2           | Явный         | Условная               | Умеренно сложные ОДУ |  \n",
    "| Рунге-Кутты 4           | 4           | Явный         | Условная               | Универсальный         |  \n",
    "| Адамс-Башфорт           | 4           | Прогноз-коррекция | Условная          | Экономичные расчеты   |  \n",
    "| Неявный Эйлер           | 1           | Неявный       | A-устойчивость         | Жесткие системы       |  \n",
    "| Метод Гира              | 1–6         | Неявный       | Жесткая устойчивость   | Сверхжесткие системы  |  \n",
    "\n",
    "Для успешной сдачи экзамена важно понимать:  \n",
    "- Разницу между явными/неявными схемами,  \n",
    "- Связь порядка метода с точностью,  \n",
    "- Причины жесткости и способы ее преодоления."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f14f68",
   "metadata": {},
   "source": [
    "<h1>16 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf26b28",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:  \n",
    "**1. Порядок решения ОДУ**  \n",
    "- Решаем задачу Коши: $ y' = F(x, y) $, $ y(x_0) = y_0 $.  \n",
    "- Последовательные шаги: $ x_n = x_0 + n \\cdot h $.  \n",
    "- **Фиксированный шаг $ h = \\text{const} $** (просто, но не всегда эффективно).  \n",
    "- **Переменный шаг $ h $** (адаптируется под точность, сложнее).  \n",
    "\n",
    "---\n",
    "\n",
    "**2. Модифицированный метод Эйлера (2-й порядок)**  \n",
    "**Формулы:**  \n",
    "1. **Прогноз:** $ \\hat{y}_{n+1} = y_n + h \\cdot F(x_n, y_n) $  \n",
    "2. **Коррекция:** $ y_{n+1} = y_n + h \\cdot \\frac{F(x_n, y_n) + F(x_{n+1}, \\hat{y}_{n+1})}{2} $  \n",
    "**Точность:** Локальная ошибка $ O(h^3) $.  \n",
    "\n",
    "**Пример для $ y' = -y $, $ y(0)=1 $, $ h=0.5 $:**  \n",
    "- Шаг 1: $ \\hat{y}_1 = 0.5 $, $ y_1 = 0.625 $ (точное: $ e^{-0.5} \\approx 0.606 $).  \n",
    "\n",
    "---\n",
    "\n",
    "**3. Метод Рунге-Кутты 4-го порядка**  \n",
    "**Формулы:**  \n",
    "$$\n",
    "\\begin{align*}\n",
    "k_1 &= F(x_n, y_n), \\\\\n",
    "k_2 &= F\\left(x_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right), \\\\\n",
    "k_3 &= F\\left(x_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_2\\right), \\\\\n",
    "k_4 &= F(x_n + h, y_n + h k_3), \\\\\n",
    "y_{n+1} &= y_n + \\frac{h}{6} (k_1 + 2k_2 + 2k_3 + k_4).\n",
    "\\end{align*}\n",
    "$$  \n",
    "**Точность:** Локальная ошибка $ O(h^5) $.  \n",
    "\n",
    "**Пример для $ y' = -y $, $ y(0)=1 $, $ h=1 $:**  \n",
    "- $ y_1 = 0.25 $ (точное: $ e^{-1} \\approx 0.367 $).  \n",
    "\n",
    "---\n",
    "\n",
    "**4. Многошаговые методы (прогноз-коррекция)**  \n",
    "- Используют $ y $ и $ y' $ в нескольких предыдущих точках.  \n",
    "- **Метод Адамса-Башфорта (4-й порядок):**  \n",
    "  **Прогноз:** $ y_{n+1} = y_n + \\frac{h}{24} (55y_n' - 59y_{n-1}' + 37y_{n-2}' - 9y_{n-3}') $  \n",
    "  **Коррекция:** $ y_{n+1} = y_n + \\frac{h}{24} (9y_{n+1}' + 19y_n' - 5y_{n-1}' + y_{n-2}') $  \n",
    "- **Особенности:** Требует стартовых точек (например, Рунге-Кутты), экономичен.  \n",
    "\n",
    "---\n",
    "\n",
    "**5. Контроль точности**  \n",
    "- **Оценка ошибки:**  \n",
    "  - Разница между прогнозом и коррекцией (в методах Адамса/Милна).  \n",
    "  - Сравнение решений для $ h $ и $ h/2 $ (метод Рунге).  \n",
    "- **Адаптивный шаг:**  \n",
    "  - Если $ \\delta > \\delta_{\\text{доп}} $, уменьшают $ h $ (например, в 2 раза).  \n",
    "  - Если $ \\delta < \\delta_{\\text{доп}}/10 $, увеличивают $ h $.  \n",
    "\n",
    "---\n",
    "\n",
    "**6. Алгоритмы с фиксированным шагом**  \n",
    "- **Плюс:** Простота.  \n",
    "- **Минус:** Риск неустойчивости (например, для $ y' = -1000y $ требуется $ h < 0.002 $).  \n",
    "\n",
    "---\n",
    "\n",
    "**7. Жесткие системы**  \n",
    "- **Проблема:** Сильно различающиеся собственные значения ($ \\lambda_{\\min} \\ll \\lambda_{\\max} $).  \n",
    "- **Решение:**  \n",
    "  - **Неявные методы** (неявный Эйлер: $ y_{n+1} = y_n + h \\cdot F(x_{n+1}, y_{n+1}) $).  \n",
    "  - **Метод Гира:** Неявный, переменный порядок (1–6), жесткая устойчивость.  \n",
    "\n",
    "---\n",
    "\n",
    "**Ключевые понятия:**  \n",
    "- **Устойчивость:**  \n",
    "  - Явные методы: условная ($ h < 2/|\\lambda| $).  \n",
    "  - Неявные: A-устойчивость (для $ \\lambda < 0 $ любой $ h $).  \n",
    "- **Порядок метода:** Определяет точность (чем выше, тем меньше ошибка).  \n",
    "- **Жесткость:** Шаг лимитируется быстрой компонентой, хотя доминирует медленная.()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6e8a3",
   "metadata": {},
   "source": [
    "<h1>17 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb3634",
   "metadata": {},
   "source": [
    "### Экзаменационный билет:  \n",
    "**1. Жесткие системы ОДУ.  \n",
    "2. Методы интегрирования жестких систем.**  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Жесткие системы ОДУ  \n",
    "**Определение:**  \n",
    "Система ОДУ называется **жесткой**, если её решение содержит компоненты, изменяющиеся с *сильно разными скоростями*. Например, одна компонента убывает медленно (например, $ e^{-t} $), а другая — очень быстро (например, $ e^{-1000t} $).  \n",
    "\n",
    "**Проблема для численных методов:**  \n",
    "- Для явных методов (как метод Эйлера или Рунге-Кутты) шаг интегрирования $ h $ ограничен **наиболее быстрой компонентой** системы (условие устойчивости $ h < 2 / |\\lambda_{\\text{max}}| $, где $ \\lambda_{\\text{max}} $ — максимальное по модулю собственное значение матрицы Якоби).  \n",
    "- Однако практический интерес часто представляет поведение решения на интервале, определяемом **медленной компонентой**. Это приводит к необходимости делать огромное число шагов, что вычислительно неэффективно или невозможно.  \n",
    "\n",
    "**Пример:**  \n",
    "Рассмотрим систему:  \n",
    "$$\n",
    "\\begin{cases}\n",
    "y_1' = -0.01y_1, \\\\\n",
    "y_2' = -500y_2,\n",
    "\\end{cases}\n",
    "$$  \n",
    "с начальными условиями $ y_1(0) = 1 $, $ y_2(0) = 1 $.  \n",
    "- **Быстрая компонента:** $ y_2(t) = e^{-500t} $ (практически исчезает при $ t > 0.01 $).  \n",
    "- **Медленная компонента:** $ y_1(t) = e^{-0.01t} $ (значительно меняется на интервале $ t \\in [0, 100] $).  \n",
    "Для явного метода Эйлера максимальный шаг $ h < 2 / 500 = 0.004 $. Чтобы достичь $ t = 100 $, нужно сделать 25,000 шагов. При этом после $ t = 0.01 $ компонента $ y_2 $ уже не влияет на решение!  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Методы интегрирования жестких систем  \n",
    "**Почему стандартные методы не подходят?**  \n",
    "- Явные методы (Рунге-Кутты, Адамса) требуют малого шага из-за быстрых компонент.  \n",
    "- Неявные методы (например, неявный Эйлер) устойчивы при любом $ h $, но имеют низкий порядок точности ($ O(h) $), что вынуждает использовать мелкие шаги для точности.  \n",
    "\n",
    "**Решение: Метод Гира (Gear)**  \n",
    "**Особенности:**  \n",
    "1. **Неявная схема:** Уравнение вида $ y_{n+1} = F(y_{n+1}, \\dots) $ решается итерационно (устойчиво при любом $ h $).  \n",
    "2. **Переменный порядок:** Автоматически выбирает порядок метода от 1 до 6 в зависимости от поведения решения.  \n",
    "3. **Автоматический выбор шага:** Оценивает локальную ошибку и регулирует $ h $.  \n",
    "4. **Жесткая устойчивость:** Решение остается устойчивым даже для больших $ h $ после затухания быстрых компонент.  \n",
    "\n",
    "**Пример из лекции:**  \n",
    "Модель радиолиза метанола (37 реакций, константы скоростей от $ 10^3 $ до $ 10^{11} $ М⁻¹с⁻¹):  \n",
    "- **Метод Рунге-Кутты:** Требует $ h < 10^{-9} $ с для устойчивости. Для расчета до $ t = 100 $ с нужно $ \\sim 10^{11} $ шагов (20 лет вычислений).  \n",
    "- **Метод Гира:** До $ t = 100 $ с потребовалось **806 шагов** (15 секунд вычислений).  \n",
    "\n",
    "---\n",
    "\n",
    "### Итог  \n",
    "- **Жесткость** возникает, когда система содержит компоненты с сильно различающимися масштабами времени.  \n",
    "- **Метод Гира** — золотой стандарт для жестких систем благодаря сочетанию устойчивости, переменного порядка и адаптивного шага.  \n",
    "\n",
    "> Источники: Лекции МГУ (Фирсов Д.А., Абраменков А.В.), стр. 22–25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5e936",
   "metadata": {},
   "source": [
    "<h1>17 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a3ec1",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:\n",
    "\n",
    "**1. Жесткие системы ОДУ**  \n",
    "— Системы, где решение содержит компоненты с **сильно различающимися скоростями изменения** (например, $ e^{-t} $ и $ e^{-1000t} $).  \n",
    "— **Проблема:** Для явных методов (Рунге-Кутты, Адамса) шаг $ h $ ограничен **быстрой компонентой** (условие устойчивости: $ h < \\frac{2}{|\\lambda_{\\text{max}}|} $). Это требует огромного числа шагов для изучения медленной компоненты.  \n",
    "— **Пример:**  \n",
    "  $$\n",
    "  \\begin{cases}\n",
    "  y_1' = -0.01y_1, \\\\\n",
    "  y_2' = -500y_2\n",
    "  \\end{cases}\n",
    "  $$  \n",
    "  Для $ t \\in [0, 100] $ явный метод Эйлера требует $ h < 0.004 $ → 25 000 шагов, хотя после $ t > 0.01 $ быстрая компонента $ y_2 $ уже несущественна.  \n",
    "\n",
    "**2. Методы интегрирования жестких систем**  \n",
    "— **Стандартные методы неэффективны:**  \n",
    "  - Явные методы неустойчивы при больших $ h $.  \n",
    "  - Неявные методы (например, неявный Эйлер) устойчивы, но низкого порядка ($ O(h) $).  \n",
    "— **Решение: Метод Гира (Gear):**  \n",
    "  - **Неявная схема** (решение устойчиво при любом $ h $).  \n",
    "  - **Переменный порядок** (1–6) + **автоматический выбор шага** (контроль локальной ошибки).  \n",
    "  - **Жесткая устойчивость** (разрешает большие шаги после затухания быстрых компонент).  \n",
    "— **Пример:** Модель радиолиза метанола (37 реакций):  \n",
    "  - *Метод Рунге-Кутты:* $ h < 10^{-9} $ с → для $ t = 100 $ с нужно $ 10^{11} $ шагов (годы расчета).  \n",
    "  - *Метод Гира:* всего **806 шагов** до $ t = 100 $ с (15 секунд расчета).  \n",
    "\n",
    "---\n",
    "\n",
    "### Ключевые выводы:\n",
    "1. Жесткость возникает при **разных временных масштабах** компонент решения.  \n",
    "2. **Метод Гира** — эталон для жестких систем благодаря сочетанию устойчивости, переменного порядка и адаптивности.  \n",
    "\n",
    "*Источники: Лекции МГУ (стр. 22–25), определение жесткости через матрицу Якоби (стр. 23), метод Гира (стр. 24–25).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687ca79",
   "metadata": {},
   "source": [
    "<h1>18 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c7544",
   "metadata": {},
   "source": [
    "### Разбор экзаменационного билета: \"Проблема собственных значений\"\n",
    "\n",
    "#### 1. **Собственные значения и векторы (Определение и значимость)**\n",
    "- **Определение**:  \n",
    "  Для матрицы $ A $ размерности $ n \\times n $ число $ \\lambda $ называется *собственным значением*, если существует ненулевой вектор $ v $ (собственный вектор), такой что:  \n",
    "  $$\n",
    "  A v = \\lambda v.\n",
    "  $$  \n",
    "  Вектор $ v $ определяется с точностью до умножения на константу. Обычно его нормируют: $ v^*v = 1 $.\n",
    "\n",
    "- **Значимость в химии/физике**:  \n",
    "  - В квантовой химии: Гамильтониан $ \\hat{H} $ (оператор энергии) имеет собственные значения $ E_n $ (энергии состояний) и собственные функции $ \\psi_n $ (волновые функции).  \n",
    "  - В химической кинетике: Решение систем ОДУ с постоянными коэффициентами выражается через $ \\lambda $ и $ v $ матрицы коэффициентов.  \n",
    "  - Пример: Топологические индексы молекул (хемоинформатика) строятся на основе $ \\lambda $ матрицы связей.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Ортогональные и подобные матрицы**\n",
    "- **Ортогональные матрицы**:  \n",
    "  Матрица $ Q $ называется ортогональной, если $ Q' = Q^{-1} $ (для вещественных матриц). Свойства:  \n",
    "  - Сохраняют длины векторов и углы: $ (Qx)'(Qy) = x'y $.  \n",
    "  - Примеры:  \n",
    "    - Матрица поворота на угол $ \\varphi $:  \n",
    "    $$\n",
    "    R = \\begin{bmatrix}\n",
    "    \\cos \\varphi & -\\sin \\varphi \\\\\n",
    "    \\sin \\varphi & \\cos \\varphi\n",
    "    \\end{bmatrix}.\n",
    "    $$  \n",
    "    - Матрица отражения: $ Q = E - 2vv' $, где $ v $ — единичный вектор нормали к плоскости.\n",
    "\n",
    "- **Подобные матрицы**:  \n",
    "  Матрицы $ A $ и $ \\tilde{A} $ подобны, если $ \\tilde{A} = P^{-1} A P $ для невырожденной $ P $.  \n",
    "  - **Свойства**:  \n",
    "    - Имеют одинаковые собственные значения, определитель и след.  \n",
    "    - Пример: Если $ A $ диагонализуема, то $ \\Lambda = V^{-1} A V $, где $ \\Lambda $ — диагональная матрица с $ \\lambda_i $ на диагонали.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Метод Якоби для симметричных матриц**\n",
    "- **Идея метода**:  \n",
    "  Последовательность преобразований подобия ортогональными матрицами вращения (матрицы Гивенса), чтобы обнулить недиагональные элементы.  \n",
    "  - На каждом шаге выбирается наибольший недиагональный элемент $ a_{ij} $.  \n",
    "  - Матрица вращения $ R_{ij} $ обнуляет $ a_{ij} $ и $ a_{ji} $ при помощи угла $ \\varphi $:  \n",
    "  $$\n",
    "  \\operatorname{ctg} 2\\varphi = \\frac{a_{ii} - a_{jj}}{2a_{ij}}, \\quad |\\varphi| \\leq \\frac{\\pi}{4}.\n",
    "  $$  \n",
    "  - Сумма квадратов недиагональных элементов монотонно убывает.\n",
    "\n",
    "- **Сходимость**:  \n",
    "  После нескольких проходов матрица становится диагональной с $ \\lambda_i $ на диагонали. Собственные векторы — столбцы произведения всех матриц вращения.\n",
    "\n",
    "- **Пример**:  \n",
    "  Для матрицы $ A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $:  \n",
    "  - Максимальный недиагональный элемент $ a_{12} = 1 $.  \n",
    "  - Угол $ \\varphi $: $ \\operatorname{ctg} 2\\varphi = \\frac{2-2}{2 \\cdot 1} = 0 \\Rightarrow \\varphi = \\pi/4 $.  \n",
    "  - Матрица вращения: $ R = \\begin{bmatrix} \\cos \\pi/4 & -\\sin \\pi/4 \\\\ \\sin \\pi/4 & \\cos \\pi/4 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} $.  \n",
    "  - Результат: $ R' A R = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix} $. Собственные значения: $ \\lambda_1 = 3 $, $ \\lambda_2 = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **QR- и QL-алгоритмы**\n",
    "- **QR-алгоритм**:  \n",
    "  1. Разложить $ A_k = Q_k R_k $ ($ Q_k $ — ортогональная, $ R_k $ — верхняя треугольная).  \n",
    "  2. Перемножить в обратном порядке: $ A_{k+1} = R_k Q_k $.  \n",
    "  - Матрицы $ A_k $ и $ A_{k+1} $ подобны: $ A_{k+1} = Q_k^T A_k Q_k $.  \n",
    "  - Для симметричных матриц $ A_k \\to \\Lambda $ (диагональная).\n",
    "\n",
    "- **Ускорение сдвигом**:  \n",
    "  - Используется $ A_k - \\sigma_k E = Q_k L_k $ (для QL), затем $ A_{k+1} = L_k Q_k + \\sigma_k E $.  \n",
    "  - Сдвиг $ \\sigma_k $ (например, $ \\sigma_k = (A_k)_{11} $) ускоряет сходимость.  \n",
    "  - **Неявный сдвиг**: Повышает устойчивость без явного вычитания $ \\sigma_k $.\n",
    "\n",
    "- **Пример**:  \n",
    "  Для трёхдиагональной матрицы $ A = \\begin{bmatrix} d_1 & e_1 \\\\ e_1 & d_2 \\end{bmatrix} $:  \n",
    "  - QL-алгоритм со сдвигом $ \\sigma = d_1 $ быстро обнуляет $ e_1 $.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Обратные итерации для нахождения векторов**\n",
    "- **Идея**:  \n",
    "  Решение системы $ (A - \\sigma E) u_k = \\alpha_k u_{k-1} $ для нахождения $ v_i $, ближайшего к $ \\sigma $.  \n",
    "  - Если $ \\sigma \\approx \\lambda_i $, то $ u_k \\to v_i $ за 1-2 итерации.  \n",
    "  - Вектор нормируется на каждом шаге.\n",
    "\n",
    "- **Алгоритм**:  \n",
    "  1. Задать $ \\sigma = \\lambda_i + \\epsilon $ (малое $ \\epsilon $).  \n",
    "  2. Решить систему: $ (A - \\sigma E) u_1 = w_0 $ (начальный вектор).  \n",
    "  3. Нормировать: $ w_1 = u_1 / \\|u_1\\| $.  \n",
    "  4. Повторять, пока $ \\|w_k - w_{k-1}\\| $ не станет малой.\n",
    "\n",
    "- **Пример**:  \n",
    "  Для матрицы $ A = \\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix} $ и $ \\sigma = 2.1 $:  \n",
    "  - Система: $ \\begin{bmatrix} 1.9 & 0 \\\\ 0 & -0.1 \\end{bmatrix} u_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\Rightarrow u_1 = \\begin{bmatrix} 0.526 \\\\ -10 \\end{bmatrix} $.  \n",
    "  - Нормировка: $ w_1 = \\begin{bmatrix} 0.0525 \\\\ -0.998 \\end{bmatrix} $ (близко к $ v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $).\n",
    "\n",
    "---\n",
    "\n",
    "### Примеры применения методов\n",
    "1. **Метод Якоби**:  \n",
    "   - Матрица: $ A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} $.  \n",
    "   - Шаг 1: Обнулить $ a_{12} = 1 $. Угол $ \\varphi = \\pi/4 $.  \n",
    "   - $ R = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix} $,  \n",
    "   - $ \\tilde{A} = R' A R = \\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix} $.  \n",
    "   - Результат: $ \\lambda_1 = 4 $, $ \\lambda_2 = 2 $.\n",
    "\n",
    "2. **Обратные итерации**:  \n",
    "   - Матрица: $ A = \\begin{bmatrix} 5 & 0 \\\\ 0 & 1 \\end{bmatrix} $, известно $ \\lambda_2 = 1 $.  \n",
    "   - Задаем $ \\sigma = 1.01 $, начальный вектор $ w_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $.  \n",
    "   - Система: $ \\begin{bmatrix} 3.99 & 0 \\\\ 0 & -0.01 \\end{bmatrix} u_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\Rightarrow u_1 = \\begin{bmatrix} 0.2506 \\\\ -100 \\end{bmatrix} $.  \n",
    "   - Нормировка: $ w_1 = \\begin{bmatrix} 0.0025 \\\\ -0.999 \\end{bmatrix} \\approx v_2 $.\n",
    "\n",
    "---\n",
    "\n",
    "**Заключение**:  \n",
    "Методы решения проблемы собственных значений зависят от структуры матрицы (симметричная, ленточная) и требуемой полноты решения (все или часть $ \\lambda_i $. Для плотных симметричных матриц оптимален метод Хаусхолдера + QL-алгоритм, для частичной проблемы — обратные итерации или рациональный QR-алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31d045",
   "metadata": {},
   "source": [
    "<h1>18 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f4b1e",
   "metadata": {},
   "source": [
    "### **Билет: Проблема собственных значений**\n",
    "#### **1. Основные понятия**\n",
    "- **Собственное значение (λ)**: Число λ, для которого существует ненулевой вектор **v** такой, что:  \n",
    "  $$\n",
    "  A\\mathbf{v} = \\lambda\\mathbf{v}\n",
    "  $$\n",
    "- **Собственный вектор (v)**: Ненулевой вектор, удовлетворяющий уравнению выше. Определяется с точностью до множителя. Нормировка: $\\mathbf{v}^*\\mathbf{v} = 1$.\n",
    "- **Характеристическое уравнение**:  \n",
    "  $$\n",
    "  \\det(A - \\lambda E) = 0\n",
    "  $$\n",
    "  Корни этого уравнения — собственные значения. Для матрицы $n \\times n$ существует $n$ значений (с учётом кратности).\n",
    "\n",
    "#### **2. Свойства матриц**\n",
    "- **Подобие матриц**: $A$ и $\\tilde{A}$ подобны, если $\\tilde{A} = P^{-1}AP$. Подобные матрицы имеют:\n",
    "  - Одинаковые собственные значения.\n",
    "  - Одинаковые след ($\\operatorname{tr}(A) = \\sum \\lambda_i$) и определитель ($\\det(A) = \\prod \\lambda_i$).\n",
    "- **Специальные матрицы**:\n",
    "  - **Симметричная**: $A = A'$ (вещественная).\n",
    "  - **Эрмитова**: $A = A^*$ (комплексно-сопряжённая). Собственные значения вещественные.\n",
    "  - **Ортогональная**: $A' = A^{-1}$. Сохраняет длины и углы.\n",
    "  - **Унитарная**: $A^* = A^{-1}$ (обобщение ортогональной на комплексные матрицы).\n",
    "\n",
    "#### **3. Численные методы (полная проблема)**\n",
    "- **Метод Якоби**:\n",
    "  - Для симметричных матриц.\n",
    "  - Последовательность поворотов (матрицы Гивенса $R_{ij}$), обнуляющих недиагональные элементы.\n",
    "  - Угол поворота: $\\operatorname{ctg} 2\\varphi = \\frac{a_{ii} - a_{jj}}{2a_{ij}}$.\n",
    "  - Сходится к диагональной матрице. Собственные векторы — произведение матриц поворотов.\n",
    "- **QR/QL-алгоритмы**:\n",
    "  - Разложение $A = QR$ (Q — ортогональная, R — верхняя треугольная).\n",
    "  - Итерации: $A_{k+1} = R_k Q_k$.\n",
    "  - Со сдвигом: $A_k - \\sigma_k E = Q_k R_k$, $A_{k+1} = R_k Q_k + \\sigma_k E$ (ускоряет сходимость).\n",
    "  - Для симметричных матриц → диагонализация.\n",
    "\n",
    "#### **4. Приведение к трёхдиагональной форме**\n",
    "- **Метод Хаусхолдера**:\n",
    "  - Для симметричных матриц.\n",
    "  - Преобразование подобия с матрицей отражения: $Q = E - 2\\mathbf{v}\\mathbf{v}'$.\n",
    "  - За $n-2$ шагов получаем трёхдиагональную матрицу.\n",
    "  - Эффективен для плотных матриц.\n",
    "\n",
    "#### **5. Частичная проблема**\n",
    "- **Прямые итерации**:\n",
    "  - Ищет наибольшее по модулю $\\lambda$:  \n",
    "    $$\n",
    "    \\mathbf{u}_k = \\alpha_k A \\mathbf{u}_{k-1}\n",
    "    $$\n",
    "  - Сходится к $\\mathbf{v}_1$, $\\lambda_1 = \\lim_{k \\to \\infty} \\frac{\\mathbf{u}_{k+1}}{\\mathbf{u}_k}$.\n",
    "- **Обратные итерации**:\n",
    "  - Ищет ближайшее к $\\sigma$ значение:  \n",
    "    $$\n",
    "    (A - \\sigma E) \\mathbf{u}_k = \\alpha_k \\mathbf{u}_{k-1}\n",
    "    $$\n",
    "  - Эффективен для нахождения собственных векторов по известным $\\lambda_i$.\n",
    "- **Метод бисекции (последовательность Штурма)**:\n",
    "  - Для трёхдиагональных матриц.\n",
    "  - Рекуррентное вычисление $Q_k(\\sigma) = (d_k - \\sigma) - \\frac{e_{k-1}^2}{Q_{k-1}(\\sigma)}$.\n",
    "  - Число отрицательных $Q_k(\\sigma)$ равно числу $\\lambda_i < \\sigma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d9b6a",
   "metadata": {},
   "source": [
    "<h1>19 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836e87c",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Методы решения частичной проблемы собственных значений  \n",
    "**Частичная проблема** — нахождение *некоторых* выбранных собственных значений (λ) и/или собственных векторов (v) матрицы, без вычисления всех. Актуально для больших матриц, когда нужны только крайние или близкие к заданному числу λ. Методы:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Прямые итерации**  \n",
    "**Цель:** Найти наибольшее по модулю собственное значение λ₁ и соответствующий вектор v₁.  \n",
    "**Алгоритм:**  \n",
    "1. Выбрать произвольный нормированный вектор $ u_0 $ (например, $ u_0 = [1, 0, ..., 0]^T $).  \n",
    "2. Итеративно умножать матрицу A на вектор:  \n",
    "   $$\n",
    "   u_k = A \\cdot u_{k-1}, \\quad \\text{затем нормировать:} \\quad u_k \\leftarrow \\frac{u_k}{\\|u_k\\|}.\n",
    "   $$  \n",
    "3. При $ k \\to \\infty $ вектор $ u_k $ стремится к v₁, а отношение Рэлея:  \n",
    "   $$\n",
    "   \\lambda^{(k)} = \\frac{u_k^T A u_k}{u_k^T u_k} \\to \\lambda_1.\n",
    "   $$  \n",
    "\n",
    "**Сходимость:** Быстрая, если $ |\\lambda_1| \\gg |\\lambda_2| $. Если $ \\lambda_1 $ кратно, сходится к вектору из подпространства.  \n",
    "\n",
    "**Пример:**  \n",
    "Матрица $ A = \\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix} $.  \n",
    "- Начальный вектор $ u_0 = [1, 0]^T $.  \n",
    "- Шаг 1: $ u_1 = A u_0 = [1, 3]^T $, нормируем: $ u_1 = [0.316, 0.949]^T $.  \n",
    "- Шаг 2: $ u_2 = A u_1 = [3.163, 3.163]^T $, нормируем: $ u_2 = [0.707, 0.707]^T $.  \n",
    "- Оценка λ: $ \\lambda^{(2)} = u_2^T A u_2 / u_2^T u_2 = 4 $ (истинное λ₁ = 4).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Обратные итерации**  \n",
    "**Цель:** Найти собственное значение, ближайшее к заданному числу σ, и его вектор. Особенно полезно для нахождения v по известному λ.  \n",
    "**Алгоритм:**  \n",
    "1. Выбрать σ (например, приближение к искомому λᵢ).  \n",
    "2. Решать систему на каждом шаге:  \n",
    "   $$\n",
    "   (A - \\sigma E) \\cdot u_k = u_{k-1}, \\quad \\text{нормировать:} \\quad u_k \\leftarrow \\frac{u_k}{\\|u_k\\|}.\n",
    "   $$  \n",
    "3. При $ k \\to \\infty $ вектор $ u_k $ стремится к vᵢ, а оценка λ:  \n",
    "   $$\n",
    "   \\lambda^{(k)} = \\sigma + \\frac{u_{k-1}^T u_k}{u_k^T u_k}.\n",
    "   $$  \n",
    "\n",
    "**Сходимость:** Тем быстрее, чем ближе σ к λᵢ. Если σ = λᵢ, система вырождена, поэтому используют $ \\sigma = \\lambda_i + \\epsilon $.  \n",
    "\n",
    "**Пример:**  \n",
    "Матрица $ A = \\begin{bmatrix} 1 & 3 \\\\ 3 & 1 \\end{bmatrix} $, известно λ₂ ≈ -2.  \n",
    "- Положим $ \\sigma = -2 $, начальный вектор $ u_0 = [1, 0]^T $.  \n",
    "- Решаем $ (A + 2E) u_1 = u_0 $:  \n",
    "  $$\n",
    "  \\begin{bmatrix} 3 & 3 \\\\ 3 & 3 \\end{bmatrix} u_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\implies u_1 = [0.5, -0.5]^T \\text{ (нормируем: } [-0.707, 0.707]^T).\n",
    "  $$  \n",
    "- Истинный v₂ = [-0.707, 0.707]ᵀ.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Последовательность Штурма + метод бисекции**  \n",
    "**Цель:** Найти все λ в заданном интервале [a, b] для симметричных матриц.  \n",
    "**Условие:** Матрица должна быть приведена к **трёхдиагональной форме** (например, преобразованиями Хаусхолдера).  \n",
    "\n",
    "**Алгоритм:**  \n",
    "1. Для трёхдиагональной матрицы T вычислить последовательность многочленов:  \n",
    "   $$\n",
    "   P_0(\\sigma) = 1, \\quad P_1(\\sigma) = d_1 - \\sigma,\n",
    "   $$  \n",
    "   $$\n",
    "   P_k(\\sigma) = (d_k - \\sigma) P_{k-1}(\\sigma) - e_{k-1}^2 P_{k-2}(\\sigma), \\quad k = 2, ..., n,\n",
    "   $$  \n",
    "   где $ d_i $ — диагональ, $ e_i $ — внедиагональ.  \n",
    "2. Для произвольного σ посчитать число перемен знака в последовательности $ \\{P_0(\\sigma), P_1(\\sigma), ..., P_n(\\sigma)\\} $. Это число $ s(\\sigma) $ равно количеству λ < σ.  \n",
    "3. Метод бисекции:  \n",
    "   - Выбрать интервал [a, b], содержащий искомые λ.  \n",
    "   - Вычислить $ s(a) $, $ s(b) $.  \n",
    "   - Пока длина интервала > ε:  \n",
    "     - $ c = (a + b)/2 $,  \n",
    "     - Если $ s(c) - s(a) > 0 $, то в [a, c] есть λ: полагаем b = c,  \n",
    "     - Иначе a = c.  \n",
    "\n",
    "**Пример:**  \n",
    "Трёхдиагональная матрица $ T = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $.  \n",
    "- Характеристический многочлен: $ \\det(T - \\lambda E) = (2-\\lambda)^2 - 1 = 0 \\implies \\lambda_1=1, \\lambda_2=3 $.  \n",
    "- Последовательность Штурма для σ = 2:  \n",
    "  $$\n",
    "  P_0(2) = 1 (>0), \\quad P_1(2) = 2-2 = 0 \\to \\text{заменяем на } \\varepsilon, \\quad P_2(2) = (2-2)(0) - 1^2 \\cdot 1 = -1 (<0).\n",
    "  $$  \n",
    "  Число перемен знака: $ P_0 \\to P_1 $ (1→0), $ P_1 \\to P_2 $ (0→-1) → $ s(2) = 2 $.  \n",
    "  Так как $ s(0) = 0 $ (все Pᵢ >0), то $ s(2) - s(0) = 2 $ → в [0, 2] два λ (оба λ = 1 и 3 > 2? Ошибка в замене P₁(2)=0. На практике используют $ Q_k = P_k / P_{k-1} $).  \n",
    "\n",
    "---\n",
    "\n",
    "### Итоговая таблица сравнения методов:\n",
    "| **Метод**               | **Когда применять**                          | **Точность**       | **Сложность**         |\n",
    "|--------------------------|---------------------------------------------|--------------------|-----------------------|\n",
    "| Прямые итерации         | Нужно λ_max и v_max                         | Средняя            | O(n²) на итерацию    |\n",
    "| Обратные итерации       | Нужно v для известного λ или λ ≈ σ         | Высокая            | O(n³) (решение СЛАУ) |\n",
    "| Последовательность Штурма | Поиск всех λ в интервале для симметричных | Высокая            | O(n) на точку σ      |\n",
    "\n",
    "**Примечание:** Для несимметричных матриц используют QR-методы, но это уже полная проблема."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca2602",
   "metadata": {},
   "source": [
    "<h1>19 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9859752",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет:  \n",
    "**Методы решения частичной проблемы собственных значений**  \n",
    "\n",
    "#### 1. **Прямые итерации**  \n",
    "- **Цель:** Найти наибольшее по модулю $\\lambda_1$ и его вектор $v_1$.  \n",
    "- **Алгоритм:**  \n",
    "  1. Нормированный стартовый вектор $u_0$.  \n",
    "  2. Итерации: $u_k = A \\cdot u_{k-1}$ + нормировка.  \n",
    "  3. Оценка $\\lambda_1$: отношение Рэлея $\\lambda^{(k)} = \\frac{u_k^T A u_k}{u_k^T u_k}$.  \n",
    "- **Условие сходимости:** $|\\lambda_1| > |\\lambda_2|$.  \n",
    "\n",
    "#### 2. **Обратные итерации**  \n",
    "- **Цель:** Найти $\\lambda_i$ и $v_i$ ближайшие к заданному $\\sigma$ (особенно для векторов по известному $\\lambda$).  \n",
    "- **Алгоритм:**  \n",
    "  1. Решать систему $(A - \\sigma E) \\cdot u_k = u_{k-1}$ + нормировка.  \n",
    "  2. Сдвиг $\\sigma = \\lambda_i + \\epsilon$ (избегает вырождения).  \n",
    "- **Плюсы:** Высокая точность для векторов.  \n",
    "\n",
    "#### 3. **Последовательность Штурма + бисекция**  \n",
    "- **Цель:** Найти все $\\lambda$ в интервале $[a, b]$ для симметричных матриц.  \n",
    "- **Условие:** Матрица трёхдиагональная (приведение Хаусхолдером).  \n",
    "- **Алгоритм:**  \n",
    "  1. Рекуррентные многочлены:  \n",
    "     $$\n",
    "     Q_k(\\sigma) = (d_k - \\sigma) - \\frac{e_{k-1}^2}{Q_{k-1}(\\sigma)}, \\quad Q_1 = d_1 - \\sigma.\n",
    "     $$  \n",
    "  2. $s(\\sigma)$ = число отрицательных $Q_k(\\sigma)$.  \n",
    "  3. Биссекция: если $s(c) - s(a) > 0$, то $\\lambda \\in [a, c]$.  \n",
    "\n",
    "---\n",
    "\n",
    "**Сравнение методов:**  \n",
    "| Метод               | Применение                          | Сложность          |  \n",
    "|----------------------|-------------------------------------|--------------------|  \n",
    "| Прямые итерации      | $\\lambda_{\\max}$, $v_{\\max}$     | $O(n^2)$/шаг    |  \n",
    "| Обратные итерации    | $v_i$ по известному $\\lambda_i$ | $O(n^3)$/шаг    |  \n",
    "| Штурм + бисекция     | Все $\\lambda \\in [a, b]$          | $O(n)$/точка    |  \n",
    "\n",
    "**Замечания:**  \n",
    "- Для несимметричных матриц методы неприменимы (используется полный QR/QL).  \n",
    "- Обратные итерации устойчивы при $\\sigma \\approx \\lambda_i$.  \n",
    "- Бисекция требует трёхдиагональной формы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66fb5a",
   "metadata": {},
   "source": [
    "<h1>20 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096f3e9",
   "metadata": {},
   "source": [
    "### Разбор экзаменационного билета\n",
    "\n",
    "**Тема:** Методы оптимизации функций одной переменной.  \n",
    "**Ключевые элементы:**  \n",
    "1. Локализация минимума.  \n",
    "2. Метод золотого сечения.  \n",
    "3. Кубическая интерполяция (метод Дэвидона).  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Локализация минимума\n",
    "**Цель:** Найти отрезок $[a, b]$, содержащий минимум функции $f(x)$ (причем единственный).  \n",
    "**Алгоритм:**  \n",
    "- Задаем начальную точку $x_0$ и шаг $h$.  \n",
    "- Вычисляем $f(x_0)$ и $f(x_1)$, где $x_1 = x_0 + h$.  \n",
    "- Если $f(x_1) < f(x_0)$, двигаемся в этом направлении:  \n",
    "  - Переносим $x_0 \\to x_1$,  \n",
    "  - Увеличиваем шаг (обычно $h \\to 2h$).  \n",
    "- Если $f(x_1) \\geq f(x_0)$, меняем направление ($h \\to -h$).  \n",
    "- Процесс продолжается, пока $f(x_{n+1}) > f(x_n)$. Тогда минимум лежит между $x_{n-1}$ и $x_{n+1}$.  \n",
    "\n",
    "**Простой пример:**  \n",
    "Для $f(x) = x^2$ и $x_0 = 3$, $h = 1$:  \n",
    "- $f(3) = 9$, $f(4) = 16 > 9$ → меняем знак шага: $h = -1$.  \n",
    "- $f(2) = 4 < 9$ → новый $x_0 = 2$, увеличиваем шаг: $h = -2$.  \n",
    "- $f(0) = 0 < 4$ → новый $x_0 = 0$, $h = -4$.  \n",
    "- $f(-4) = 16 > 0$ → минимум локализован на отрезке $[-4, 0]$.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Метод золотого сечения\n",
    "**Цель:** Уточнить положение минимума на локализованном отрезке $[x_1, x_3]$.  \n",
    "**Принцип:**  \n",
    "- Отрезок делится в соотношении **золотого сечения** $\\tau = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$.  \n",
    "- Точки $x_2$ и $x_4$ выбираются так:  \n",
    "  $$\n",
    "  x_2 = x_1 + \\frac{x_3 - x_1}{\\tau}, \\quad x_4 = x_1 + x_3 - x_2.\n",
    "  $$  \n",
    "- Сравниваем $f(x_2)$ и $f(x_4)$:  \n",
    "  - Если $f(x_2) < f(x_4)$, новый отрезок $[x_1, x_4]$.  \n",
    "  - Иначе — $[x_2, x_3]$.  \n",
    "- Процесс повторяется, пока длина отрезка не станет меньше заданной точности.  \n",
    "\n",
    "**Особенности:**  \n",
    "- Линейная сходимость (погрешность уменьшается в $\\tau \\approx 1.618$ раз за шаг).  \n",
    "- Гарантированная скорость сходимости для любых унимодальных функций.  \n",
    "\n",
    "**Пример для $f(x) = x^2$ на $[-4, 0]$:**  \n",
    "- Первые точки: $x_1 = -4$, $x_3 = 0$, $x_2 \\approx -2.472$, $x_4 \\approx -1.528$.  \n",
    "- $f(x_2) \\approx 6.11$, $f(x_4) \\approx 2.33$ → т.к. $f(x_4) < f(x_2)$, новый отрезок $[-4, -1.528]$.  \n",
    "- Повторяем, пока не приблизимся к $x = 0$.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Метод кубической интерполяции (Дэвидона)\n",
    "**Цель:** Быстро найти минимум с использованием производных.  \n",
    "**Алгоритм:**  \n",
    "- Задаем начальную точку $x_1 = 0$ (для удобства) и вторую точку $x_2 = q > 0$.  \n",
    "- Используем значения функции и производной: $f_1, f_2, g_1 = f'(0), g_2 = f'(q)$.  \n",
    "- Строим кубический многочлен:  \n",
    "  $$\n",
    "  P_3(x) = a + bx + cx^2 + dx^3,\n",
    "  $$  \n",
    "  где коэффициенты:  \n",
    "  $$\n",
    "  a = f_1, \\ b = g_1, \\ z = \\frac{3(f_1 - f_2)}{q} + g_1 + g_2, \\ c = -\\frac{g_1 + z}{q}, \\ d = \\frac{g_1 + g_2 + 2z}{3q^2}.\n",
    "  $$  \n",
    "- Положение минимума:  \n",
    "  $$\n",
    "  r = \\frac{g_1 + z + \\sqrt{z^2 - g_1 g_2}}{g_1 + g_2 + 2z}, \\quad x_{\\text{min}} = r \\cdot q.\n",
    "  $$  \n",
    "\n",
    "**Важно:**  \n",
    "- Точка $q$ выбирается так, чтобы минимум лежал между $0$ и $q$ (если $g_2 < 0$, $q$ удваивают).  \n",
    "- Требует оценки $f_{\\min}$ для выбора $q$ (не обязательно точной).  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Сравнение методов\n",
    "| **Метод**             | **Требует производных?** | **Сходимость**       | **Применение**                     |  \n",
    "|------------------------|--------------------------|----------------------|-----------------------------------|  \n",
    "| Золотое сечение       | Нет                     | Линейная            | Надёжен, но медленный             |  \n",
    "| Кубическая интерполяция | Да (первые)             | Кубическая          | Быстрый, но нужна оценка $f_{\\min}$ |  \n",
    "\n",
    "**Итог:**  \n",
    "- Для функций без производных или с шумом используйте **золотое сечение**.  \n",
    "- Если производные известны и функция гладкая — **метод Дэвидона** эффективнее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b370290",
   "metadata": {},
   "source": [
    "<h1>20 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542362c",
   "metadata": {},
   "source": [
    "### Ответ на экзаменационный билет  \n",
    "**1. Условия достаточности минимума:**  \n",
    "- **Необходимое условие:** $ f'(x_0) = 0 $ (стационарная точка).  \n",
    "- **Достаточное условие:** $ f''(x_0) > 0 $ (локальный минимум).  \n",
    "\n",
    "**2. Локализация минимума:**  \n",
    "- **Цель:** Найти отрезок $[a, b]$, где минимум единственный.  \n",
    "- **Алгоритм:**  \n",
    "  1. Задать $x_0$ и шаг $h$.  \n",
    "  2. Если $f(x_0 + h) < f(x_0)$, двигаться в этом направлении, удваивая $h$.  \n",
    "  3. Иначе сменить знак $h$ и повторить шаг 2.  \n",
    "  4. Остановиться, когда $f(x_{n+1}) > f(x_n)$. Минимум в $[x_{n-1}, x_{n+1}]$.  \n",
    "\n",
    "**3. Метод золотого сечения:**  \n",
    "- **Цель:** Уточнить минимум на отрезке $[x_1, x_3]$.  \n",
    "- **Шаги:**  \n",
    "  1. Вычислить точки:  \n",
    "     $$\n",
    "     x_2 = x_1 + \\frac{x_3 - x_1}{\\tau}, \\quad x_4 = x_1 + x_3 - x_2 \\quad (\\tau \\approx 1.618).\n",
    "     $$  \n",
    "  2. Сравнить $f(x_2)$ и $f(x_4)$:  \n",
    "     - Если $f(x_2) < f(x_4)$, новый отрезок $[x_1, x_4]$.  \n",
    "     - Иначе $[x_2, x_3]$.  \n",
    "  3. Повторять, пока длина отрезка не станет меньше точности.  \n",
    "\n",
    "**4. Кубическая интерполяция (Дэвидона):**  \n",
    "- **Применение:** Если известны первая производная и оценка $f_{\\min}$.  \n",
    "- **Алгоритм:**  \n",
    "  1. Задать $x_1 = 0$, $x_2 = q$ (выбор $q$: $ q = \\min\\left\\{-2 \\frac{f_1 - f_{\\min}}{g_1}, L \\right\\} $).  \n",
    "  2. Вычислить $f_1, f_2, g_1 = f'(0), g_2 = f'(q)$.  \n",
    "  3. Найти минимум кубического многочлена:  \n",
    "     $$\n",
    "     r = \\frac{g_1 + z + \\sqrt{z^2 - g_1 g_2}}{g_1 + g_2 + 2z}, \\quad \\text{где} \\quad z = \\frac{3(f_1 - f_2)}{q} + g_1 + g_2.\n",
    "     $$  \n",
    "  4. Новое приближение: $x_{\\text{min}} = r \\cdot q$.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ключевые особенности методов  \n",
    "| **Метод**       | **Сходимость** | **Требования**         |  \n",
    "|------------------|----------------|------------------------|  \n",
    "| Золотое сечение  | Линейная       | Только значения $f(x)$ |  \n",
    "| Кубическая интерполяция | Кубическая | Значения $f(x)$ и $f'(x)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98a372",
   "metadata": {},
   "source": [
    "<h1>21 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa205c",
   "metadata": {},
   "source": [
    "### Билет: Кубическая интерполяция (метод Дэвидона) для одномерной минимизации\n",
    "\n",
    "#### Суть метода\n",
    "Метод Дэвидона — это алгоритм поиска минимума функции одной переменной $ f(x) $ с использованием **кубической интерполяции**. В отличие от квадратичной аппроксимации, кубический многочлен учитывает асимметрию функции вокруг минимума, что ускоряет сходимость. Для построения многочлена $ P_3(x) = a + bx + cx^2 + dx^3 $ используются:\n",
    "- Значения функции $ f $ и её первой производной $ f' $ в двух точках: $ x_1 = 0 $ и $ x_2 = q $.\n",
    "\n",
    "#### Шаги алгоритма\n",
    "1. **Задание начальных данных**:\n",
    "   - Начальная точка $ x_1 = 0 $ (для упрощения).\n",
    "   - Значения в точке $ x_1 $: $ f_1 = f(0) $, $ g_1 = f'(0) $.\n",
    "   - Оценка $ f_{\\text{min}} $ — предполагаемое значение функции в минимуме (ориентировочно).\n",
    "   - Ограничитель шага $ L $ (защита от нереалистичных оценок $ f_{\\text{min}} $).\n",
    "\n",
    "2. **Выбор второй точки $ q $**:\n",
    "   $$\n",
    "   q = \\min \\left\\{ -2 \\frac{f_1 - f_{\\text{min}}}{g_1},  L \\right\\}.\n",
    "   $$\n",
    "   - Если $ g_1 > 0 $, направление оси $ x $ меняется на противоположное.\n",
    "   - Если $ f'(q) < 0 $, точка $ q $ слишком близка к $ x_1 $. Тогда $ q $ удваивают, пока $ f'(q) > 0 $.\n",
    "\n",
    "3. **Расчёт коэффициентов кубического многочлена**:\n",
    "   - По значениям $ f_1, f_2 = f(q), g_1, g_2 = f'(q) $:\n",
    "     $$\n",
    "     z = \\frac{3(f_1 - f_2)}{q} + g_1 + g_2,\n",
    "     $$\n",
    "     $$\n",
    "     c = -\\frac{g_1 + z}{q}, \\quad d = \\frac{g_1 + g_2 + 2z}{3q^2}, \\quad a = f_1, \\quad b = g_1.\n",
    "     $$\n",
    "\n",
    "4. **Поиск минимума $ P_3(x) $**:\n",
    "   - Решение уравнения $ P_3'(x) = 0 $:\n",
    "     $$\n",
    "     w = \\sqrt{z^2 - g_1 g_2}, \\quad r = \\frac{g_1 + z + w}{g_1 + g_2 + 2z}.\n",
    "     $$\n",
    "   - Положение минимума: $ x_{\\text{min}} = r \\cdot q $ (в отрезке $[0, q]$).\n",
    "\n",
    "5. **Итеративное уточнение**:\n",
    "   - Если $ f'(x_{\\text{min}}) > 0 $, заменяем $ q \\to x_{\\text{min}} $.\n",
    "   - Если $ f'(x_{\\text{min}}) < 0 $, заменяем начальную точку $ x_1 \\to x_{\\text{min}} $.\n",
    "   - Процесс повторяется, пока производная или длина интервала не станут малыми.\n",
    "\n",
    "#### Преимущества\n",
    "- **Кубическая сходимость** вблизи минимума (быстрее квадратичной интерполяции).\n",
    "- Не требует предварительной локализации минимума (в отличие от метода Брента).\n",
    "- Учитывает асимметрию функции.\n",
    "\n",
    "\n",
    "\n",
    "#### Итог\n",
    "Метод Дэвидона эффективен, когда доступны первые производные, и особенно полезен для функций с асимметричным поведением вокруг минимума. Требует лишь ориентировочной оценки $ f_{\\text{min}} $, а ограничитель $ L $ предотвращает расходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8e1c1",
   "metadata": {},
   "source": [
    "<h1>21 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846b095",
   "metadata": {},
   "source": [
    "### Билет: Методы одномерной оптимизации  \n",
    "**1. Метод золотого сечения**  \n",
    "- **Суть**: Поиск минимума на отрезке $[a, b]$ путём деления в отношении золотого сечения $\\tau = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618$.  \n",
    "- **Формула**:  \n",
    "  $$\n",
    "  x_2 = a + \\frac{b - a}{\\tau}, \\quad x_4 = a + b - x_2\n",
    "  $$  \n",
    "- **Сходимость**: Линейная. Коэффициент сокращения отрезка: $\\tau - 1 \\approx 0.618$.  \n",
    "- **Пример**: Минимизация $f(x) = x^2 + 5$ на $[-5, 5]$.  \n",
    "  - Шаг 1: $a = -5$, $b = 5$, $x_2 = -0.528$, $x_4 = 0.528$.  \n",
    "  - $f(x_2) < f(x_4)$ → новый отрезок $[-5, 0.528]$.  \n",
    "\n",
    "**2. Квадратичная интерполяция (Пауэлла)**  \n",
    "- **Суть**: Аппроксимация функции параболой по трём точкам $(x_1, x_2, x_3)$ и поиск минимума параболы.  \n",
    "- **Формула**:  \n",
    "  $$\n",
    "  x_4 = \\frac{1}{2} \\cdot \\frac{(x_2^2 - x_3^2)f_1 + (x_3^2 - x_1^2)f_2 + (x_1^2 - x_2^2)f_3}{(x_2 - x_3)f_1 + (x_3 - x_1)f_2 + (x_1 - x_2)f_3}\n",
    "  $$  \n",
    "- **Сходимость**: Квадратичная вблизи минимума.  \n",
    "- **Пример**: Минимизация $f(x) = \\sin(x)$ на $[0, \\pi]$ с точками $x_1=0$, $x_2=1$, $x_3=3$.  \n",
    "  - Вычисляем $x_4 \\approx 1.57$ (близко к истинному минимуму $\\pi/2$).  \n",
    "\n",
    "**3. Метод Брента**  \n",
    "- **Суть**: Комбинация золотого сечения (на больших интервалах) и квадратичной интерполяции (на малых).  \n",
    "- **Алгоритм**:  \n",
    "  1. Сжать отрезок методом золотого сечения.  \n",
    "  2. При малом отрезке ($< \\varepsilon$) применить квадратичную интерполяцию.  \n",
    "- **Преимущество**: Гарантированная сходимость золотого сечения + скорость квадратичной интерполяции.  \n",
    "- **Пример**: Минимизация $f(x) = x^4 - 3x^2 + x$ на $[-2, 2]$.  \n",
    "  - Золотое сечение: локализует минимум на $[-0.5, 1]$.  \n",
    "  - Квадратичная интерполяция: уточняет минимум в $x \\approx 0.25$.  \n",
    "\n",
    "**4. Кубическая интерполяция (Дэвидона)**  \n",
    "- **Суть**: Построение кубического полинома по двум точкам с использованием $f(x)$ и $f'(x)$.  \n",
    "- **Формула**:  \n",
    "  $$\n",
    "  r = \\frac{g_1 + z + w}{g_1 + g_2 + 2z}, \\quad \\text{где} \\quad z = \\frac{3(f_1 - f_2)}{q} + g_1 + g_2, \\quad w = \\sqrt{z^2 - g_1 g_2}\n",
    "  $$  \n",
    "  ($q$ — шаг, $g_1, g_2$ — производные).  \n",
    "- **Особенность**: Не требует локализации минимума.  \n",
    "- **Пример**: Минимизация $f(x) = e^{-(x-3)^2}$ с $x_1 = 0$, $g_1 = f'(0)$, оценкой $f_{\\min} = 0$, $L = 10$.  \n",
    "  - Вычисляем $q = \\min\\left\\{-2 \\frac{f(0) - 0}{g_1}, 10\\right\\}$, затем $r \\approx 3$.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ключевые выводы:  \n",
    "- **Золотое сечение**: Надёжен, но медленен.  \n",
    "- **Квадратичная интерполяция**: Быстрая сходимость, но требует близости к минимуму.  \n",
    "- **Брент**: Универсален (сочетает надёжность и скорость).  \n",
    "- **Дэвидон**: Эффективен при известных производных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37466062",
   "metadata": {},
   "source": [
    "<h1>22 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b6ba8",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Методы многомерной минимизации  \n",
    "**Рассмотренные методы:**  \n",
    "1. **Метод покоординатного спуска**  \n",
    "2. **Метод Пауэлла**  \n",
    "3. **Метод Хука-Дживса**  \n",
    "4. **Метод деформируемого многогранника Нелдера-Мида**  \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Метод покоординатного спуска**  \n",
    "**Описание:**  \n",
    "Поиск минимума проводится поочередно вдоль каждой координатной оси. Начинается с начальной точки $ x_0 $. Фиксируются все координаты, кроме одной (например, $ x_1 $), и находится минимум вдоль этой оси. Затем процесс повторяется для следующей координаты ($ x_2 $, $ x_3 $, и т.д.). После прохождения всех осей цикл повторяется.  \n",
    "**Критерий остановки:**  \n",
    "Значение функции перестает уменьшаться или смещение точки становится меньше заданного предела.  \n",
    "**Недостатки:**  \n",
    "- Медленная сходимость для функций с вытянутыми \"оврагами\" (например, функция Розенброка).  \n",
    "- Направления поиска ортогональны и не учитывают топографию поверхности.  \n",
    "\n",
    "**Пример:**  \n",
    "Для функции Розенброка $ f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2 $ при старте из точки $(-1, 1)$ метод требует множества мелких шагов вдоль осей, так как минимум лежит в $(1, 1)$, а направление к нему постоянно меняется.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Метод Пауэлла (метод сопряженных направлений)**  \n",
    "**Описание:**  \n",
    "1. **Инициализация:** Задаются $ n $ линейно независимых направлений (обычно координатные оси).  \n",
    "2. **Цикл минимизаций:**  \n",
    "   - Последовательно проводится одномерная минимизация вдоль каждого направления.  \n",
    "   - Определяется направление, давшее наибольшее снижение функции.  \n",
    "   - Вычисляется вектор суммарного смещения за цикл.  \n",
    "3. **Обновление направлений:**  \n",
    "   \"Наименее полезное\" направление заменяется вектором суммарного смещения.  \n",
    "**Критерий остановки:**  \n",
    "Смещение точки или уменьшение значения функции меньше заданного порога.  \n",
    "**Преимущество:**  \n",
    "Формирует сопряженные направления, что для квадратичных функций гарантирует сходимость за $ \\leq n $ шагов.  \n",
    "\n",
    "**Пример:**  \n",
    "Для эллиптических линий уровня (например, $ f(x_1, x_2) = x_1^2 + 5x_2^2 $) метод быстро сходится к минимуму $(0, 0)$, так как новые направления адаптируются к геометрии функции.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Метод Хука-Дживса**  \n",
    "**Описание:**  \n",
    "Состоит из двух этапов:  \n",
    "1. **Исследующий поиск:**  \n",
    "   - Из текущей точки $ x_k $ делаются пробные шаги вдоль координатных осей.  \n",
    "   - Если шаг успешен (функция уменьшилась), точка перемещается.  \n",
    "   - Если нет, шаг повторяется в противоположном направлении.  \n",
    "2. **Шаг по образцу:**  \n",
    "   - После успешного исследующего поиска делается шаг от исходной точки $ x_k $ в направлении $ x_{k+1} - x_k $.  \n",
    "   - От новой точки повторяется исследующий поиск.  \n",
    "**Критерий остановки:**  \n",
    "Величина шагов $ h_i $ становится меньше заданной точности.  \n",
    "\n",
    "**Пример:**  \n",
    "Для функции с \"пологим склоном\" (например, $ f(x_1, x_2) = x_1^2 + x_2^2 $) метод быстро движется к минимуму, комбинируя мелкие исследующие шаги и крупные шаги по образцу.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Метод деформируемого многогранника Нелдера-Мида**  \n",
    "**Описание:**  \n",
    "Использует симплекс ($ n $-мерный многогранник с $ n+1 $ вершиной). На каждом шаге:  \n",
    "1. **Выбор вершин:**  \n",
    "   - $ x_h $: вершина с наибольшим значением функции (\"худшая\").  \n",
    "   - $ x_l $: вершина с наименьшим значением (\"лучшая\").  \n",
    "   - $ x_g $: \"вторая по худости\".  \n",
    "2. **Операции над симплексом:**  \n",
    "   - **Отражение:** $ x_r = x_c + \\alpha (x_c - x_h) $, где $ x_c $ — центр тяжести всех вершин, кроме $ x_h $.  \n",
    "   - **Растяжение:** Если $ f(x_r) < f(x_l) $, $ x_e = x_c + \\gamma (x_r - x_c) $.  \n",
    "   - **Сжатие:** Если $ f(x_r) > f(x_h) $, $ x_s = x_c + \\beta (x_h - x_c) $.  \n",
    "   - **Сокращение:** Все вершины сдвигаются к $ x_l $.  \n",
    "**Критерий остановки:**  \n",
    "Размеры многогранника становятся меньше заданной точности.  \n",
    "\n",
    "**Пример:**  \n",
    "Для функции с \"оврагом\" (Розенброка) симплекс вытягивается вдоль дна \"оврага\", быстро приближаясь к минимуму $(1, 1)$, избегая движения по крутым склонам.  \n",
    "\n",
    "---\n",
    "\n",
    "**Сравнение методов:**  \n",
    "- **Без производных:** Пауэлл и Нелдер-Мид эффективны для негладких функций.  \n",
    "- **Сходимость:** Пауэлл быстрее для квадратичных функций, Нелдер-Мид устойчивее в \"оврагах\".  \n",
    "- **Простота:** Покоординатный спус и Хука-Дживс легче реализовать, но медленнее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a07fc",
   "metadata": {},
   "source": [
    "<h1>22 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a33909",
   "metadata": {},
   "source": [
    "### Методы многомерной минимизации  \n",
    "**1. Метод покоординатного спуска**  \n",
    "- **Идея**: Последовательная минимизация вдоль координатных осей.  \n",
    "- **Алгоритм**:  \n",
    "  1. Фиксируют все переменные, кроме одной, и ищут минимум вдоль её оси.  \n",
    "  2. Цикл повторяется для всех переменных, пока не будет достигнута заданная точность.  \n",
    "- **Недостатки**: Медленная сходимость для \"овражных\" функций (например, Розенброка).  \n",
    "\n",
    "**2. Метод Пауэлла**  \n",
    "- **Идея**: Поиск вдоль сопряженных направлений.  \n",
    "- **Алгоритм**:  \n",
    "  1. Начальные направления — координатные оси.  \n",
    "  2. Цикл из $n$ одномерных минимизаций.  \n",
    "  3. Замена \"наилучшего\" направления на вектор суммарного смещения.  \n",
    "- **Особенности**:  \n",
    "  - Не требует производных.  \n",
    "  - Для квадратичных функций сходится за $n$ шагов.  \n",
    "\n",
    "**3. Метод Хука-Дживса**  \n",
    "- **Идея**: Комбинация исследующего поиска и шага по образцу.  \n",
    "- **Алгоритм**:  \n",
    "  1. **Исследующий поиск**: Локальный поиск вдоль осей с заданным шагом.  \n",
    "  2. **Шаг по образцу**: Смещение в направлении успешного смещения.  \n",
    "- **Условия остановки**: Шаги становятся меньше заданного предела.  \n",
    "\n",
    "**4. Метод Нелдера-Мида (деформируемого многогранника)**  \n",
    "- **Идея**: Работа с симплексом ($n+1$ вершин в $n$-мерном пространстве).  \n",
    "- **Операции**:  \n",
    "  - **Отражение** худшей вершины через центр противоположной грани.  \n",
    "  - **Растяжение** при успешном отражении.  \n",
    "  - **Сжатие** при неудаче.  \n",
    "- **Преимущества**: Устойчивость, не требует производных.  \n",
    "- **Недостатки**: Медленнее градиентных методов.  \n",
    "\n",
    "---  \n",
    "**Ключевые характеристики методов**:  \n",
    "- **Без производных**: Пауэлла, Хука-Дживса, Нелдера-Мида.  \n",
    "- **Сходимость**: Пауэлла и градиентные методы эффективны для квадратичных функций.  \n",
    "- **Устойчивость**: Нелдера-Мида подходит для \"шумных\" функций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a85f2",
   "metadata": {},
   "source": [
    "<h1>23 билет. <b style=\"color:red;\">К прочтению</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56461781",
   "metadata": {},
   "source": [
    "### Билет: Методы оптимизации  \n",
    "**1. Метод скорейшего спуска (градиентный спуск)**  \n",
    "**Идея:** Движение в направлении антиградиента (-∇F), где функция убывает быстрее всего.  \n",
    "**Алгоритм:**  \n",
    "1. Вычислить антиградиент в точке $x_k$: $g_k = -\\nabla F(x_k)$.  \n",
    "2. Найти минимум вдоль $g_k$ (одномерная минимизация), получить $x_{k+1}$.  \n",
    "**Проблема:** Последовательные направления ортогональны → зигзагообразная траектория (особенно в \"оврагах\", например, функции Розенброка).  \n",
    "**Демпфирование:** Укороченный шаг: $x_{k+1} = x_k + \\alpha (x_{\\text{min}} - x_k)$, где $\\alpha = 0.6–0.8$. Устраняет ортогональность, ускоряет сходимость.  \n",
    "\n",
    "**Пример:**  \n",
    "- Функция: $f(x, y) = 100(y - x^2)^2 + (1 - x)^2$ (Розенброка).  \n",
    "- Проблема: В точке $(-1, 1)$ градиентный спуск делает много мелких шагов вдоль \"оврага\".  \n",
    "- Решение: Демпфирование направляет к минимуму $(1, 1)$ за меньшее число шагов.  \n",
    "\n",
    "---\n",
    "\n",
    "**2. Метод Ньютона**  \n",
    "**Идея:** Решение системы $\\nabla F = 0$ итерационно. Использует гессиан $H$ (матрицу вторых производных).  \n",
    "**Формула:**  \n",
    "$$\n",
    "\\Delta x = -H^{-1} \\cdot \\nabla F \\quad \\Rightarrow \\quad x_{k+1} = x_k - H^{-1} \\nabla F\n",
    "$$  \n",
    "**Преимущества:**  \n",
    "- Квадратичная сходимость вблизи минимума.  \n",
    "- Для квадратичных функций: точный минимум за 1 шаг.  \n",
    "**Недостатки:** Требует вычисления и обращения гессиана → сложно, если вторые производные неизвестны.  \n",
    "\n",
    "**Пример:**  \n",
    "- Квадратичная функция: $f(x, y) = x^2 + 5y^2$.  \n",
    "- Гессиан: $H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 10 \\end{pmatrix}$.  \n",
    "- Из точки $(3, 2)$: один шаг даёт минимум $(0, 0)$.  \n",
    "\n",
    "---\n",
    "\n",
    "**3. Метод сопряжённых градиентов (Флетчера-Ривса)**  \n",
    "**Идея:** Построение направлений, сопряжённых относительно $H$ (для квадратичных функций). Не требует явного вычисления $H$.  \n",
    "**Алгоритм:**  \n",
    "1. Начать с антиградиента: $p_0 = -\\nabla F(x_0)$.  \n",
    "2. Новое направление: $p_{k} = -\\nabla F(x_k) + \\beta_k p_{k-1}$,  \n",
    "   где $\\beta_k = \\frac{(\\nabla F(x_k))' \\nabla F(x_k)}{(\\nabla F(x_{k-1}))' \\nabla F(x_{k-1})}$.  \n",
    "**Преимущества:**  \n",
    "- Для $n$-мерной квадратичной функции: минимум за $\\leq n$ шагов.  \n",
    "- Не требует вторых производных.  \n",
    "\n",
    "**Пример:**  \n",
    "- Функция Розенброка.  \n",
    "- Старт с $(-1, 1)$: направления адаптируются к \"оврагу\", сходится быстрее градиентного спуска.  \n",
    "\n",
    "---\n",
    "\n",
    "**4. Квазиньютоновские методы (BFGS, ДФП)**  \n",
    "**Идея:** Аппроксимация обратного гессиана $H^{-1}$ матрицей $B_k$, обновляемой на каждом шаге.  \n",
    "**Формула (BFGS):**  \n",
    "$$\n",
    "B_{k+1}^{-1} = \\left( I - \\frac{s_k y_k'}{y_k' s_k} \\right) B_k^{-1} \\left( I - \\frac{y_k s_k'}{y_k' s_k} \\right) + \\frac{s_k s_k'}{y_k' s_k},\n",
    "$$  \n",
    "где $s_k = x_{k+1} - x_k$, $y_k = \\nabla F(x_{k+1}) - \\nabla F(x_k)$.  \n",
    "**Преимущества:**  \n",
    "- Сверхлинейная сходимость.  \n",
    "- Не требует вторых производных.  \n",
    "- Матрица $B_k$ остаётся положительно определённой.  \n",
    "\n",
    "**Пример:**  \n",
    "- Минимизация $f(x, y) = \\sin(x) + \\cos(y)$:  \n",
    "  - BFGS использует прошлые градиенты для аппроксимации $H^{-1}$ → меньше итераций, чем метод Ньютона.  \n",
    "\n",
    "---\n",
    "\n",
    "### Сравнение методов  \n",
    "| **Метод**               | **Сходимость**       | **Требует**                     | **Применимость**               |  \n",
    "|--------------------------|----------------------|----------------------------------|--------------------------------|  \n",
    "| Скорейший спуск          | Линейная            | Градиент                        | Недорогие вычисления           |  \n",
    "| Ньютона                  | Квадратичная        | Гессиан                         | Аналитические производные      |  \n",
    "| Сопряжённых градиентов   | Конечная (для квад.)| Градиент                        | Большие $n$                 |  \n",
    "| BFGS/ДФП                 | Сверхлинейная       | Градиент                        | Универсальность                |  \n",
    "\n",
    "**Ключевая разница:**  \n",
    "- **Градиентные методы** (скорейший спуск, Флетчера-Ривса): используют только первые производные.  \n",
    "- **Ньютон и квазиньютоновские:** моделируют кривизну функции (через $H$ или $B_k$), что ускоряет сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea789c43",
   "metadata": {},
   "source": [
    "<h1>23 билет. <b style=\"color:red;\">Записать в тетрадь</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5f41e",
   "metadata": {},
   "source": [
    "### Экзаменационный билет: Методы оптимизации  \n",
    "**1. Метод Ньютона**  \n",
    "**Суть:** Решение системы уравнений ∇F(x) = 0 для поиска экстремума. Использует матрицу Гессе (H) и градиент:  \n",
    "$$\\Delta x = -H^{-1} \\cdot \\nabla F(x_k)$$  \n",
    "**Особенности:**  \n",
    "- Квадратичная сходимость вблизи минимума.  \n",
    "- Для квадратичной функции находит минимум за 1 шаг.  \n",
    "- Требует вычисления вторых производных (гессиана), что сложно для неаналитических функций.  \n",
    "**Пример:** Для $F(x) = x_1^2 + 5x_2^2$ начальное приближение $(1, 1)$:  \n",
    "- Гессиан $H = \\begin{bmatrix} 2 & 0 \\\\ 0 & 10 \\end{bmatrix}$, градиент $\\nabla F(1,1) = (2, 10)$.  \n",
    "- $\\Delta x = -H^{-1} \\nabla F = -\\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 10 \\end{bmatrix} = (-1, -1)$, минимум в $(0,0)$.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Метод сопряжённых градиентов (Флетчера–Ривса)**  \n",
    "**Суть:** Построение взаимно сопряжённых направлений поиска относительно матрицы Гессе. Направление обновляется по правилу:  \n",
    "$$p_k = -\\nabla F(x_k) + \\beta_k p_{k-1}, \\quad \\beta_k = \\frac{\\|\\nabla F(x_k)\\|^2}{\\|\\nabla F(x_{k-1})\\|^2}$$  \n",
    "**Особенности:**  \n",
    "- Для квадратичной функции сходится за $n$ шагов (размерность пространства).  \n",
    "- Не требует явного вычисления гессиана (только градиент).  \n",
    "- Направления ортогональны в смысле $p_i^T H p_j = 0$ при $i \\neq j$.  \n",
    "**Пример:** Минимизация функции Розенброка $F(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$:  \n",
    "- Начало: $x_0 = (-1.5, 2)$, $\\nabla F(x_0) = (-215, 200)$.  \n",
    "- Первое направление $p_0 = -\\nabla F(x_0)$. После минимизации вдоль $p_0$: $x_1 \\approx (0.8, 0.6)$.  \n",
    "- Новое направление $p_1 = -\\nabla F(x_1) + \\beta_1 p_0$, $\\beta_1 = \\|\\nabla F(x_1)\\|^2 / \\|\\nabla F(x_0)\\|^2 \\approx 0.1$. Сходится к $(1, 1)$ за 20 итераций.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Квазиньютоновские методы (Дэвидона–Флетчера–Пауэлла, BFGS)**  \n",
    "**Суть:** Аппроксимация обратного гессиана ($H^{-1}$) матрицей $B_k$, обновляемой на каждом шаге:  \n",
    "- **ДФП:**  \n",
    "  $$B_{k+1} = B_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T \\Delta g_k} - \\frac{B_k \\Delta g_k \\Delta g_k^T B_k}{\\Delta g_k^T B_k \\Delta g_k},$$  \n",
    "  где $\\Delta x_k = x_{k+1} - x_k$, $\\Delta g_k = \\nabla F(x_{k+1}) - \\nabla F(x_k)$.  \n",
    "- **BFGS:**  \n",
    "  $$B_{k+1}^{-1} = \\left( I - \\frac{\\Delta x_k \\Delta g_k^T}{\\Delta g_k^T \\Delta x_k} \\right) B_k^{-1} \\left( I - \\frac{\\Delta g_k \\Delta x_k^T}{\\Delta g_k^T \\Delta x_k} \\right) + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta g_k^T \\Delta x_k}.$$  \n",
    "**Особенности:**  \n",
    "- Сходится как метод Ньютона, но без вычисления вторых производных.  \n",
    "- Начальная матрица $B_0 = I$ (единичная).  \n",
    "- BFGS устойчивее ДФП при неточном одномерном поиске.  \n",
    "**Пример:** Минимизация $F(x) = x_1^4 + x_2^2 - x_1 x_2$:  \n",
    "- Начало: $x_0 = (5, 5)$, $B_0 = I$.  \n",
    "- Первый шаг: $\\Delta x_0 = -B_0 \\nabla F(x_0) \\approx (-500, -45)$, минимум вдоль направления: $x_1 \\approx (0.2, 0.1)$.  \n",
    "- После обновления $B_1$ (BFGS) следующий шаг направлен к $(0, 0)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Заключение:**  \n",
    "- **Метод Ньютона** точен, но требует гессиана.  \n",
    "- **Сопряжённые градиенты** эффективны для задач с разреженными матрицами.  \n",
    "- **BFGS/ДФП** — компромисс между скоростью и вычислительными затратами."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
